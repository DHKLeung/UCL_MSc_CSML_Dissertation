{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ultimate CLP Experiment Framework colab.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMeQMubqsPn+LpGTYV+/d85"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"wybNB3V5Y_sB"},"source":["# Configuration"]},{"cell_type":"code","metadata":{"id":"SjFLLO7ipr76"},"source":["# Configuration #\n","config = {\n","    'validation_times': None,\n","    'data_name': None,  # {'breast_cancer', 'cifar10', 'MNIST', 'FashionMNIST', 'spambase', 'abalone', 'iris', 'wine'}\n","    'perturb_type': None,  # {None, 'mixup', 'mixup sc', 'mixup nb', 'gauss VRM', 'downup', 'whitenoise'}; 'downup' is only valid for image data\n","    'augWPL': None,  # {None, 0.25, 0.5, 0.75, 0.9, 0.99} or others\n","    'geometric_param': None,  # {None, 0.25, 0.5, 0.75, 1.} or others\n","    'gauss_vicinal_std': None,  # {None, 0.25, 0.5, 0.75, 1.} or others\n","    'batch_size': None,\n","    'step_size': None,  # {0.1, 0.01, 0.001, 0.0005} or others\n","    'epochs': None,\n","    'L2_decay': None,  # {0., 1e-4} or others\n","    'alpha': None,  # {None, 0.25, 0.5, 0.75, 1.} or others\n","    'downupsize': None,  # {None, (8, 8)} or others\n","    'CLP': None,  # {None, [(1, 1.0), (40, 0.0)]} or others\n","    'data_model_option': None  # {None, '4hl_tanh_sigmoid', '3hl_tanh_sigmoid', '2hl_relu_sigmoid', '2hl_relu_softmax', 'resnet18_external', 'resnet18_pytorch'}; Note that not all applicable, each dataset has its own pre-hardcoded model, please refer to the model section for details\n","}\n","if config['data_model_option'] in ['4hl_tanh_sigmoid', '3hl_tanh_sigmoid', '2hl_relu_sigmoid']:\n","    config['criterion_type'] = 'BCE'\n","elif config['data_model_option'] in ['2hl_relu_softmax', 'resnet18_external', 'resnet18_pytorch']:\n","    config['criterion_type'] = 'CE'\n","\n","# Folders and Files #\n","gdrive_dir = '/content/gdrive'\n","experiment_folder_dir = gdrive_dir + '/My Drive/colab'\n","save_folder_dir = experiment_folder_dir + '/{}-{}-{}'.format(str(config['data_name']), str(config['perturb_type']), str(config['data_model_option']))\n","file_prefix = '{}-{}-{}-{}-{}-{}-'.format(str(config['augWPL']), str(config['alpha']), str(config['geometric_param']), str(config['gauss_vicinal_std']), str(config['downupsize']), str('CLP' if config['CLP'] is not None else None))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NpP4X8dP4cGj"},"source":["# Libraries"]},{"cell_type":"code","metadata":{"id":"Cvxw76jyn_lp"},"source":["import torch\n","from torchvision import transforms, datasets, models\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount(gdrive_dir)\n","import sys\n","sys.path.append(experiment_folder_dir)\n","import time\n","import datetime\n","import os\n","import json\n","import resnet\n","import load_external_data\n","import pandas as pd\n","\n","if not os.path.exists(save_folder_dir):\n","    os.mkdir(save_folder_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kKQa_Key4eEZ"},"source":["# Data Augmentation / Perturbation related functions"]},{"cell_type":"code","metadata":{"id":"3Hi7aPfE4XFC"},"source":["def mixup(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    batch_size = labels.size(0)\n","    idx = torch.randperm(batch_size).to('cuda')\n","    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n","    labels_b = labels[idx]\n","    return mixup_inputs, labels, labels_b, lmbda"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D6VcCy1IwgP9"},"source":["def mixup_sc(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_uc_list = list()\n","    labels_uc_list = list()\n","    unique_classes = torch.unique(labels)\n","    for uc in unique_classes:\n","        mask_uc = (labels == uc).flatten()  # flatten to avoid the labels in column vector\n","        inputs_uc = inputs[mask_uc]\n","        labels_uc = labels[mask_uc]\n","        batch_size_uc = labels_uc.size(0)\n","        idx = torch.randperm(batch_size_uc).to('cuda')\n","        mixup_inputs_uc = lmbda * inputs_uc + (1 - lmbda) * inputs_uc[idx]\n","        mixup_inputs_uc_list.append(mixup_inputs_uc)\n","        labels_uc_list.append(labels_uc)\n","    mixup_inputs_sc = torch.vstack(mixup_inputs_uc_list)\n","    mixup_labels_sc = torch.cat(labels_uc_list, dim=0)  # use cat not hstack to avoid labels in column vector\n","    return mixup_inputs_sc, mixup_labels_sc, lmbda"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H3615DIf3poh"},"source":["def mixup_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)  # !!! BUG may exist if the inner_batch_size = 1 -> selectable neighbours = 0; please change the batch_size a little bit so that the remainder is > 1 !!!\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LI_ziZaZ3y_C"},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iBOl0OTV9xpd"},"source":["def gauss_vicinal(inputs, gauss_vicinal_std):\n","    inputs_gauss = torch.normal(inputs, gauss_vicinal_std)\n","    return inputs_gauss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQKd7Zu3Y1JW"},"source":["def downup(inputs, downupsize):\n","    original_size = inputs.size()[2:]\n","    inputs_downup = F.interpolate(F.interpolate(inputs, size=downupsize, mode='bilinear'), size=original_size, mode='bilinear')\n","    return inputs_downup"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CHJIBSbEoDHX"},"source":["def white_noise(inputs):\n","    original_size = inputs.size()\n","    inputs_whitenoise_unstandard = torch.distributions.uniform.Uniform(0, 1).sample(sample_shape=original_size).to('cuda')\n","    inputs_whitenoise = (inputs_whitenoise_unstandard - 0.5) / 0.2887\n","    return inputs_whitenoise"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vIMr7ruWDIdt"},"source":["# Critical Learning Period"]},{"cell_type":"code","metadata":{"id":"N7I89gWWDIER"},"source":["def decode_CLP(config):\n","    CLP_details = np.ones(config['epochs']) * -1\n","    for epoch, proportion in config['CLP']:\n","        CLP_details[epoch - 1] = proportion\n","    current = -1\n","    for i in range(config['epochs']):\n","        if CLP_details[i] != -1:\n","            current = CLP_details[i]\n","        else:\n","            CLP_details[i] = current\n","    return CLP_details"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ohoxJvKUDOFI"},"source":["def CLP_deficit_proportion(deficit_inputs, inputs, deficit_proportion):\n","    current_batch_size = deficit_inputs.size(0)\n","    deficit_num = int(current_batch_size * deficit_proportion)\n","    normal_num = current_batch_size - deficit_num\n","    original_rnd_idx = torch.randperm(current_batch_size).to('cuda')\n","    deficit_rnd_idx = torch.randperm(current_batch_size).to('cuda')\n","    deficit_inputs_proportioned = torch.vstack((inputs[original_rnd_idx][:normal_num], deficit_inputs[deficit_rnd_idx][:deficit_num]))\n","    return deficit_inputs_proportioned, original_rnd_idx, deficit_rnd_idx, normal_num, deficit_num"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ixum4Iss48sJ"},"source":["# Data Loader"]},{"cell_type":"code","metadata":{"id":"3QLo4lDG4_KN"},"source":["def get_data_loader(config):\n","    if config['data_name'] == 'breast_cancer':\n","        train_data, train_labels, val_data, val_labels, test_data, test_labels = load_external_data.load_skl_data('breast_cancer')\n","        test_data = np.vstack((val_data, test_data))\n","        test_labels = np.hstack((val_labels, test_labels))\n","        train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","        train_labels = torch.from_numpy(train_labels)\n","        test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","        test_labels = torch.from_numpy(test_labels)\n","        train_mean = torch.mean(train_data, 0)\n","        train_std = torch.std(train_data, 0)\n","        train_data = (train_data - train_mean) / train_std\n","        test_data = (test_data - train_mean) / train_std\n","        train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","        test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n","        test_loader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n","    elif config['data_name'] == 'cifar10':\n","        transform_train = transforms.Compose([\n","            transforms.RandomCrop(32, padding=4),  # can omit\n","            transforms.RandomHorizontalFlip(),  # can omit\n","            transforms.ToTensor(),\n","            transforms.Normalize(\n","                (0.4914, 0.4822, 0.4465),\n","                (0.2023, 0.1994, 0.2010)\n","            )\n","        ])\n","        transform_test = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(\n","                (0.4914, 0.4822, 0.4465),\n","                (0.2023, 0.1994, 0.2010)\n","            )\n","        ])\n","        train_set = datasets.CIFAR10(root=experiment_folder_dir, train=True, download=True, transform=transform_train)\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n","        test_set = datasets.CIFAR10(root=experiment_folder_dir, train=False, download=True, transform=transform_test)\n","        test_loader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n","    elif config['data_name'] == 'MNIST':\n","        transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.1307,), (0.3015,))\n","        ])\n","        train_set = datasets.MNIST(root=experiment_folder_dir, train=True, download=True, transform=transform)\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n","        test_set = datasets.MNIST(root=experiment_folder_dir, train=False, download=True, transform=transform)\n","        test_loader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n","    elif config['data_name'] == 'FashionMNIST':\n","        transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.2860,), (0.3205,))\n","        ])\n","        train_set = datasets.FashionMNIST(root=experiment_folder_dir, train=True, download=True, transform=transform)\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n","        test_set = datasets.FashionMNIST(root=experiment_folder_dir, train=False, download=True, transform=transform)\n","        test_loader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n","    elif config['data_name'] == 'spambase':\n","        spambase = pd.read_csv(experiment_folder_dir + '/dataset_44_spambase.csv').to_numpy()\n","        data, labels = spambase[:, :-1], spambase[:, -1]\n","        num_data = labels.shape[0]\n","        idx = np.random.permutation(num_data)\n","        data = data[idx]\n","        labels = labels[idx]\n","        splitpoint = int(num_data * 0.6)\n","        train_data = data[:splitpoint]\n","        train_labels = labels[:splitpoint]\n","        test_data = data[splitpoint:]\n","        test_labels = labels[splitpoint:]\n","        train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","        train_labels = torch.from_numpy(train_labels)\n","        test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","        test_labels = torch.from_numpy(test_labels)\n","        train_mean = torch.mean(train_data, 0)\n","        train_std = torch.std(train_data, 0)\n","        train_data = (train_data - train_mean) / train_std\n","        test_data = (test_data - train_mean) / train_std\n","        train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","        test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n","        test_loader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n","    elif config['data_name'] == 'abalone':\n","        column_names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\", \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"]\n","        abalone = pd.read_csv(experiment_folder_dir + '/' + 'abalone.data', names=column_names)\n","        for label in \"MFI\":\n","            abalone[label] = abalone[\"sex\"] == label\n","        del abalone[\"sex\"]\n","        labels = abalone.rings.values\n","        del abalone[\"rings\"]\n","        data = abalone.values.astype(np.float32)\n","        num_data = labels.shape[0]\n","        idx = np.random.permutation(num_data)\n","        data = data[idx]\n","        labels = labels[idx]\n","        labels[labels == 29] = 28\n","        labels = labels - 1\n","        splitpoint = int(num_data * 0.6)\n","        train_data = data[:splitpoint]\n","        train_labels = labels[:splitpoint]\n","        test_data = data[splitpoint:]\n","        test_labels = labels[splitpoint:]\n","        train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","        train_labels = torch.from_numpy(train_labels)\n","        test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","        test_labels = torch.from_numpy(test_labels)\n","        train_mean = torch.mean(train_data, 0)\n","        train_std = torch.std(train_data, 0)\n","        train_data = (train_data - train_mean) / train_std\n","        test_data = (test_data - train_mean) / train_std\n","        train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","        test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n","        test_loader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n","    elif config['data_name'] == 'iris':\n","        train_data, train_labels, val_data, val_labels, test_data, test_labels = load_external_data.load_skl_data('iris')\n","        test_data = np.vstack((val_data, test_data))\n","        test_labels = np.hstack((val_labels, test_labels))\n","        train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","        train_labels = torch.from_numpy(train_labels)\n","        test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","        test_labels = torch.from_numpy(test_labels)\n","        train_mean = torch.mean(train_data, 0)\n","        train_std = torch.std(train_data, 0)\n","        train_data = (train_data - train_mean) / train_std\n","        test_data = (test_data - train_mean) / train_std\n","        train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","        test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n","        test_loader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n","    elif config['data_name'] == 'wine':\n","        train_data, train_labels, val_data, val_labels, test_data, test_labels = load_external_data.load_skl_data('wine')\n","        test_data = np.vstack((val_data, test_data))\n","        test_labels = np.hstack((val_labels, test_labels))\n","        train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","        train_labels = torch.from_numpy(train_labels)\n","        test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","        test_labels = torch.from_numpy(test_labels)\n","        train_mean = torch.mean(train_data, 0)\n","        train_std = torch.std(train_data, 0)\n","        train_data = (train_data - train_mean) / train_std\n","        test_data = (test_data - train_mean) / train_std\n","        train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","        test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n","        test_loader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n","    return train_loader, test_loader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UPZqcYGh65-k"},"source":["# Models"]},{"cell_type":"code","metadata":{"id":"jv3jxDXi7FxA"},"source":["def get_model(config):\n","    if config['data_name'] == 'breast_cancer':\n","        if config['data_model_option'] == '3hl_tanh_sigmoid':\n","            class fc_model(nn.Module):\n","                def __init__(self):\n","                    super(fc_model, self).__init__()\n","                    self.fc1 = nn.Linear(30, 128)\n","                    self.fc2 = nn.Linear(128, 64)\n","                    self.fc3 = nn.Linear(64, 32)\n","                    self.fc4 = nn.Linear(32, 1)\n","                def forward(self, inputs):\n","                    fc1_out = F.tanh(self.fc1(inputs))\n","                    fc2_out = F.tanh(self.fc2(fc1_out))\n","                    fc3_out = F.tanh(self.fc3(fc2_out))\n","                    fc4_out = self.fc4(fc3_out)\n","                    return fc4_out\n","            model = fc_model()\n","        elif config['data_model_option'] == '2hl_relu_sigmoid':\n","            class fc_model(nn.Module):\n","                def __init__(self):\n","                    super(fc_model, self).__init__()\n","                    self.fc1 = nn.Linear(30, 128)\n","                    self.fc2 = nn.Linear(128, 128)\n","                    self.fc3 = nn.Linear(128, 1)\n","                def forward(self, inputs):\n","                    fc1_out = F.relu(self.fc1(inputs))\n","                    fc2_out = F.relu(self.fc2(fc1_out))\n","                    fc3_out = self.fc3(fc2_out)\n","                    return fc3_out\n","            model = fc_model()\n","        elif config['data_model_option'] == '2hl_relu_softmax':\n","            class fc_model(nn.Module):\n","                def __init__(self):\n","                    super(fc_model, self).__init__()\n","                    self.fc1 = nn.Linear(30, 128)\n","                    self.fc2 = nn.Linear(128, 128)\n","                    self.fc3 = nn.Linear(128, 2)\n","                def forward(self, inputs):\n","                    fc1_out = F.relu(self.fc1(inputs))\n","                    fc2_out = F.relu(self.fc2(fc1_out))\n","                    fc3_out = self.fc3(fc2_out)\n","                    return fc3_out \n","            model = fc_model()\n","        model.cuda()\n","    elif config['data_name'] == 'cifar10':\n","        if config['data_model_option'] == 'resnet18_external':\n","            model = resnet.ResNet18()\n","        model.cuda()\n","    elif config['data_name'] == 'MNIST':\n","        if config['data_model_option'] == 'resnet18_pytorch':\n","            model = models.resnet18(pretrained=False)\n","            for param in model.parameters():\n","                param.requires_grad = True\n","            model.conv1 = torch.nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n","            model.fc = torch.nn.Linear(512, 10)\n","        model.cuda()\n","    elif config['data_name'] == 'FashionMNIST':\n","        if config['data_model_option'] == 'resnet18_pytorch':\n","            model = models.resnet18(pretrained=False)\n","            for param in model.parameters():\n","                param.requires_grad = True\n","            model.conv1 = torch.nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n","            model.fc = torch.nn.Linear(512, 10)\n","        model.cuda()\n","    elif config['data_name'] == 'spambase':\n","        if config['data_model_option'] == '4hl_tanh_sigmoid':\n","            class fc_model(nn.Module):\n","                def __init__(self):\n","                    super(fc_model, self).__init__()\n","                    self.fc1 = nn.Linear(57, 256)\n","                    self.fc2 = nn.Linear(256, 128)\n","                    self.fc3 = nn.Linear(128, 64)\n","                    self.fc4 = nn.Linear(64, 32)\n","                    self.fc5 = nn.Linear(32, 1)\n","                def forward(self, inputs):\n","                    fc1_out = F.tanh(self.fc1(inputs))\n","                    fc2_out = F.tanh(self.fc2(fc1_out))\n","                    fc3_out = F.tanh(self.fc3(fc2_out))\n","                    fc4_out = F.tanh(self.fc4(fc3_out))\n","                    fc5_out = self.fc5(fc4_out)\n","                    return fc5_out\n","            model = fc_model()\n","        elif config['data_model_option'] == '2hl_relu_sigmoid':\n","            class fc_model(nn.Module):\n","                def __init__(self):\n","                    super(fc_model, self).__init__()\n","                    self.fc1 = nn.Linear(57, 128)\n","                    self.fc2 = nn.Linear(128, 128)\n","                    self.fc3 = nn.Linear(128, 1)\n","                def forward(self, inputs):\n","                    fc1_out = F.relu(self.fc1(inputs))\n","                    fc2_out = F.relu(self.fc2(fc1_out))\n","                    fc3_out = self.fc3(fc2_out)\n","                    return fc3_out\n","            model = fc_model()\n","        elif config['data_model_option'] == '2hl_relu_softmax':\n","            class fc_model(nn.Module):\n","                def __init__(self):\n","                    super(fc_model, self).__init__()\n","                    self.fc1 = nn.Linear(57, 128)\n","                    self.fc2 = nn.Linear(128, 128)\n","                    self.fc3 = nn.Linear(128, 2)\n","                def forward(self, inputs):\n","                    fc1_out = F.relu(self.fc1(inputs))\n","                    fc2_out = F.relu(self.fc2(fc1_out))\n","                    fc3_out = self.fc3(fc2_out)\n","                    return fc3_out \n","            model = fc_model()\n","        model.cuda()\n","    elif config['data_name'] == 'abalone':\n","        if config['data_model_option'] == '2hl_relu_softmax':\n","            class fc_model(nn.Module):\n","                def __init__(self):\n","                    super(fc_model, self).__init__()\n","                    self.fc1 = nn.Linear(10, 128)\n","                    self.fc2 = nn.Linear(128, 128)\n","                    self.fc3 = nn.Linear(128, 28)\n","                def forward(self, inputs):\n","                    fc1_out = F.relu(self.fc1(inputs))\n","                    fc2_out = F.relu(self.fc2(fc1_out))\n","                    fc3_out = self.fc3(fc2_out)\n","                    return fc3_out\n","            model = fc_model()\n","        model.cuda()\n","    elif config['data_name'] == 'iris':\n","        if config['data_model_option'] == '2hl_relu_softmax':\n","            class fc_model(nn.Module):\n","                def __init__(self):\n","                    super(fc_model, self).__init__()\n","                    self.fc1 = nn.Linear(4, 128)\n","                    self.fc2 = nn.Linear(128, 128)\n","                    self.fc3 = nn.Linear(128, 3)\n","                def forward(self, inputs):\n","                    fc1_out = F.relu(self.fc1(inputs))\n","                    fc2_out = F.relu(self.fc2(fc1_out))\n","                    fc3_out = self.fc3(fc2_out)\n","                    return fc3_out\n","            model = fc_model()\n","        model.cuda()\n","    elif config['data_name'] == 'wine':\n","        if config['data_model_option'] == '2hl_relu_softmax':\n","            class fc_model(nn.Module):\n","                def __init__(self):\n","                    super(fc_model, self).__init__()\n","                    self.fc1 = nn.Linear(13, 128)\n","                    self.fc2 = nn.Linear(128, 128)\n","                    self.fc3 = nn.Linear(128, 3)\n","                def forward(self, inputs):\n","                    fc1_out = F.relu(self.fc1(inputs))\n","                    fc2_out = F.relu(self.fc2(fc1_out))\n","                    fc3_out = self.fc3(fc2_out)\n","                    return fc3_out\n","            model = fc_model()\n","        model.cuda()\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IARNLbNzBOe0"},"source":["# Testing"]},{"cell_type":"code","metadata":{"id":"XWi9rDWNBYOx"},"source":["def testing(data_loader, criterion, model, config):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    loss = 0.\n","\n","    # Store perturbation loss if required #\n","    if config['perturb_type'] is not None:\n","        perturb_loss = 0\n","    \n","    # Start testing #\n","    with torch.no_grad():\n","        for data in data_loader:\n","\n","            # Get the loss of the batch #\n","            inputs, labels = data\n","            inputs = inputs.to('cuda')\n","            if config['criterion_type'] == 'BCE':\n","                labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","            elif config['criterion_type'] == 'CE':\n","                labels = labels.type(torch.LongTensor).to('cuda')\n","            outputs = model(inputs)\n","            batch_loss = criterion(outputs, labels)\n","            loss += batch_loss.item()\n","\n","            # Get the loss of the perturb batch if required #\n","            if config['perturb_type'] == 'mixup':\n","                mixup_inputs, mixup_labels_a, mixup_labels_b, lmbda = mixup(inputs, labels, config['alpha'])\n","                mixup_outputs = model(mixup_inputs)\n","                batch_mixup_loss = mixup_criterion(criterion, mixup_outputs, mixup_labels_a, mixup_labels_b, lmbda)\n","                perturb_loss += batch_mixup_loss.item()\n","            elif config['perturb_type'] == 'mixup sc':\n","                mixup_inputs_sc, mixup_labels_sc, lmbda = mixup_sc(inputs, labels, config['alpha'])\n","                mixup_outputs_sc = model(mixup_inputs_sc)\n","                batch_mixup_loss_sc = criterion(mixup_outputs_sc, mixup_labels_sc)\n","                perturb_loss += batch_mixup_loss_sc.item()\n","            elif config['perturb_type'] == 'mixup nb':\n","                mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_nb(inputs, labels, config['geometric_param'], config['alpha'])\n","                mixup_outputs_nb = model(mixup_inputs_nb)\n","                batch_mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","                perturb_loss += batch_mixup_loss_nb.item()\n","            elif config['perturb_type'] == 'gauss VRM':\n","                gauss_inputs = gauss_vicinal(inputs, config['gauss_vicinal_std'])\n","                gauss_outputs = model(gauss_inputs)\n","                batch_gauss_loss = criterion(gauss_outputs, labels)\n","                perturb_loss += batch_gauss_loss.item()\n","            elif config['perturb_type'] == 'downup':\n","                downup_inputs = downup(inputs, config['downupsize'])\n","                downup_outputs = model(downup_inputs)\n","                batch_downup_loss = criterion(downup_outputs, labels)\n","                perturb_loss += batch_downup_loss.item()\n","            elif config['perturb_type'] == 'whitenoise':\n","                whitenoise_inputs = white_noise(inputs)\n","                whitenoise_outputs = model(whitenoise_inputs)\n","                batch_whitenoise_loss = criterion(whitenoise_outputs, labels)\n","                perturb_loss += batch_whitenoise_loss.item()\n","\n","            # Compute predictions #\n","            if config['criterion_type'] == 'BCE':\n","                predicts = (torch.sign(outputs) + 1) / 2\n","            elif config['criterion_type'] == 'CE':\n","                _, predicts = torch.max(outputs, 1)\n","\n","            # Accumulation #\n","            total += labels.size(0)\n","            correct += (predicts == labels).sum().item()\n","    \n","    # Compute accuracy #\n","    accuracy = correct / total\n","\n","    # Compute mean losses #\n","    mean_loss = loss / total\n","    if config['perturb_type'] is not None:\n","        mean_perturb_loss = perturb_loss / total\n","\n","    # Return according to required #\n","    model.train()\n","    if config['perturb_type'] is not None:\n","        return mean_loss, accuracy, mean_perturb_loss\n","    else:\n","        return mean_loss, accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"REzrKKutSlQp"},"source":["# Plot"]},{"cell_type":"code","metadata":{"id":"He-pD9w0SjMJ"},"source":["def plot_lines(history, config, save_folder_dir, file_prefix, timestamp, CLP):\n","\n","    # Make title #\n","    final_test_acc = history['epoch_test_accuracy'][-1]\n","    if config['augWPL'] is None:\n","        if config['perturb_type'] is None:\n","            title = '{}; final test acc: {:.7f}'.format(config['data_name'], final_test_acc)\n","        else:\n","            title = '{} {}; final test acc: {:.7f}'.format(config['data_name'], config['perturb_type'], final_test_acc)\n","    else:\n","        title = '{} {} WPL{}; final test acc: {:.7f}'.format(config['data_name'], config['perturb_type'], config['augWPL'], final_test_acc)\n","    if CLP is not None:\n","        title = title + '\\n{}'.format(str(CLP))\n","    \n","    # Plot losses #\n","    epochs = len(history['epoch_mean_train_loss'])\n","    plt.figure(figsize=(10, 7))\n","    plt.plot(np.arange(epochs) + 1, history['epoch_mean_train_loss'], label='train')\n","    plt.plot(np.arange(epochs) + 1, history['epoch_mean_test_loss'], label='test')\n","    if config['perturb_type'] is not None:\n","        plt.plot(np.arange(epochs) + 1, history['epoch_mean_perturb_loss'], label=config['perturb_type'])\n","    plt.grid()\n","    plt.legend()\n","    plt.title(title)\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.savefig('{} loss.png'.format(save_folder_dir + '/' + file_prefix + timestamp))\n","    plt.show()\n","\n","    # Plot accuracies #\n","    plt.figure(figsize=(10, 7))\n","    plt.plot(np.arange(epochs) + 1, history['epoch_train_accuracy'], label='train')\n","    plt.plot(np.arange(epochs) + 1, history['epoch_test_accuracy'], label='test')\n","    plt.grid()\n","    plt.legend()\n","    plt.title(title)\n","    plt.xlabel('epoch')\n","    plt.ylabel('accuracy')\n","    plt.savefig('{} accuracy.png'.format(save_folder_dir + '/' + file_prefix + timestamp))\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZHJ6KDHq_Zuf"},"source":["# Save history"]},{"cell_type":"code","metadata":{"id":"p7NYJgSp_ZLZ"},"source":["def save_history(history, save_folder_dir, file_prefix, timestamp, CLP):\n","    json_file_name = file_prefix + timestamp + '.json'\n","    with open(save_folder_dir + '/' + json_file_name, 'w') as fp:\n","        json.dump(history, fp)\n","    if CLP is not None:\n","        with open(save_folder_dir + '/' + 'CLP-' + timestamp + '.txt', 'w') as fp:\n","            fp.write(str(CLP))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pj8Pmco-BKkl"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"JqCX0ryvBKY3"},"source":["def training(train_loader, test_loader, model, optimizer, step_size_scheduler, config):\n","\n","    # History #\n","    history = {\n","        'epoch_mean_train_loss': list(),\n","        'epoch_train_accuracy': list(),\n","        'epoch_mean_test_loss': list(),\n","        'epoch_test_accuracy': list(),\n","    }\n","    if config['perturb_type'] is not None:\n","        history['epoch_mean_perturb_loss'] = list()\n","    \n","    # Define criterion #\n","    if config['criterion_type'] == 'BCE':\n","        criterion = torch.nn.BCEWithLogitsLoss()\n","    elif config['criterion_type'] == 'CE':\n","        criterion = torch.nn.CrossEntropyLoss()\n","\n","    # Decode CLP #\n","    if config['CLP'] is not None:\n","        CLP_details = decode_CLP(config)\n","    \n","    # Start training #\n","    for epoch in range(config['epochs']):\n","        start = time.time()\n","        for i, data in enumerate(train_loader, 0):\n","            model.train()\n","            optimizer.zero_grad()\n","            \n","            # Get inputs and labels #\n","            inputs, labels = data\n","            inputs = inputs.to('cuda')\n","            if config['criterion_type'] == 'BCE':\n","                labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","            elif config['criterion_type'] == 'CE':\n","                labels = labels.type(torch.LongTensor).to('cuda')\n","\n","            # Perturbation #\n","            if config['perturb_type'] == 'mixup':\n","                perturb_inputs, perturb_labels_a, perturb_labels_b, lmbda = mixup(inputs, labels, config['alpha'])\n","            elif config['perturb_type'] == 'mixup sc':\n","                perturb_inputs, perturb_labels, lmbda = mixup_sc(inputs, labels, config['alpha'])\n","            elif config['perturb_type'] == 'mixup nb':\n","                perturb_inputs, perturb_labels_a, perturb_labels_b, lmbda = mixup_nb(inputs, labels, config['geometric_param'], config['alpha'])\n","            elif config['perturb_type'] == 'gauss VRM':\n","                perturb_inputs = gauss_vicinal(inputs, config['gauss_vicinal_std'])\n","            elif config['perturb_type'] == 'downup':\n","                perturb_inputs = downup(inputs, config['downupsize'])\n","            elif config['perturb_type'] == 'whitenoise':\n","                perturb_inputs = white_noise(inputs)\n","            \n","            # Critical Learning Period Deficit Control #\n","            if config['CLP'] is not None:\n","                deficit_proportion = CLP_details[epoch]\n","\n","                # Get the proportioned perturbed inputs / deficit inputs #\n","                perturb_inputs, original_rnd_idx, deficit_rnd_idx, normal_num, deficit_num = CLP_deficit_proportion(perturb_inputs, inputs, deficit_proportion)\n","\n","                # Resolve the labels corresponding to the CLP proportioned perturbed / deficit inputs #  # use cat not hstack to avoid labels in column vector #\n","                if config['perturb_type'] == 'mixup':\n","                    perturb_labels_a = torch.cat((labels[original_rnd_idx][:normal_num], perturb_labels_a[deficit_rnd_idx][:deficit_num]), dim=0)\n","                    perturb_labels_b = torch.cat((labels[original_rnd_idx][:normal_num], perturb_labels_b[deficit_rnd_idx][:deficit_num]), dim=0)\n","                elif config['perturb_type'] == 'mixup sc':\n","                    perturb_labels = torch.cat((labels[original_rnd_idx][:normal_num], perturb_labels[deficit_rnd_idx][:deficit_num]), dim=0)\n","                elif config['perturb_type'] == 'mixup nb':\n","                    perturb_labels_a = torch.cat((labels[original_rnd_idx][:normal_num], perturb_labels_a[deficit_rnd_idx][:deficit_num]), dim=0)\n","                    perturb_labels_b = torch.cat((labels[original_rnd_idx][:normal_num], perturb_labels_b[deficit_rnd_idx][:deficit_num]), dim=0)\n","                elif config['perturb_type'] == 'gauss VRM':\n","                    labels = torch.cat((labels[original_rnd_idx][:normal_num], labels[deficit_rnd_idx][:deficit_num]), dim=0)\n","                elif config['perturb_type'] == 'downup':\n","                    labels = torch.cat((labels[original_rnd_idx][:normal_num], labels[deficit_rnd_idx][:deficit_num]), dim=0)\n","                elif config['perturb_type'] == 'whitenoise':\n","                    labels = torch.cat((labels[original_rnd_idx][:normal_num], labels[deficit_rnd_idx][:deficit_num]), dim=0)\n","            \n","            # Augmentation or not #\n","            if config['augWPL'] is None:\n","                if config['perturb_type'] is None:\n","                    ultimate_inputs = inputs\n","                else:\n","                    ultimate_inputs = perturb_inputs\n","            else:\n","                ultimate_inputs = torch.vstack((inputs, perturb_inputs))\n","\n","            # Get outputs #\n","            ultimate_outputs = model(ultimate_inputs)\n","            if config['augWPL'] is None:\n","                if config['perturb_type'] is None:\n","                    outputs = ultimate_outputs\n","                else:\n","                    perturb_outputs = ultimate_outputs\n","            else:\n","                current_batch_size = labels.size(0)\n","                outputs = ultimate_outputs[:current_batch_size]\n","                perturb_outputs = ultimate_outputs[current_batch_size:]\n","            \n","            # Compute losses #\n","            if config['augWPL'] is None:\n","                if config['perturb_type'] is None:\n","                    ultimate_loss = criterion(outputs, labels)\n","                elif config['perturb_type'] == 'mixup':\n","                    ultimate_loss = mixup_criterion(criterion, perturb_outputs, perturb_labels_a, perturb_labels_b, lmbda)\n","                elif config['perturb_type'] == 'mixup sc':\n","                    ultimate_loss = criterion(perturb_outputs, perturb_labels)\n","                elif config['perturb_type'] == 'mixup nb':\n","                    ultimate_loss = mixup_criterion(criterion, perturb_outputs, perturb_labels_a, perturb_labels_b, lmbda)\n","                elif config['perturb_type'] == 'gauss VRM':\n","                    ultimate_loss = criterion(perturb_outputs, labels)\n","                elif config['perturb_type'] == 'downup':\n","                    ultimate_loss = criterion(perturb_outputs, labels)\n","                elif config['perturb_type'] == 'whitenoise':\n","                    ultimate_loss = criterion(perturb_outputs, labels)\n","            else:\n","                loss = criterion(outputs, labels)\n","                if config['perturb_type'] == 'mixup':\n","                    perturb_loss = mixup_criterion(criterion, perturb_outputs, perturb_labels_a, perturb_labels_b, lmbda)\n","                elif config['perturb_type'] == 'mixup sc':\n","                    perturb_loss = criterion(perturb_outputs, perturb_labels)\n","                elif config['perturb_type'] == 'mixup nb':\n","                    perturb_loss = mixup_criterion(criterion, perturb_outputs, perturb_labels_a, perturb_labels_b, lmbda)\n","                elif config['perturb_type'] == 'gauss VRM':\n","                    perturb_loss = criterion(perturb_outputs, labels)\n","                elif config['perturb_type'] == 'downup':\n","                    perturb_loss = criterion(perturb_outputs, labels)\n","                elif config['perturb_type'] == 'whitenoise':\n","                    perturb_loss = criterion(perturb_outputs, labels)\n","                ultimate_loss = config['augWPL'] * perturb_loss + (1 - config['augWPL']) * loss\n","\n","            # Gradient calculation and optimisation #\n","            ultimate_loss.backward()\n","            optimizer.step()\n","\n","        # Step size scheduler #\n","        step_size_scheduler.step()\n","\n","        # Testing on Train and Test data #\n","        if config['perturb_type'] is None:\n","            epoch_mean_train_loss, epoch_train_accuracy = testing(train_loader, criterion, model, config)\n","            epoch_mean_test_loss, epoch_test_accuracy = testing(test_loader, criterion, model, config)\n","        else:\n","            epoch_mean_train_loss, epoch_train_accuracy, epoch_mean_perturb_loss = testing(train_loader, criterion, model, config)\n","            epoch_mean_test_loss, epoch_test_accuracy, _ = testing(test_loader, criterion, model, config)\n","        history['epoch_mean_train_loss'].append(epoch_mean_train_loss)\n","        history['epoch_train_accuracy'].append(epoch_train_accuracy)\n","        history['epoch_mean_test_loss'].append(epoch_mean_test_loss)\n","        history['epoch_test_accuracy'].append(epoch_test_accuracy)\n","        if config['perturb_type'] is not None:\n","            history['epoch_mean_perturb_loss'].append(epoch_mean_perturb_loss)\n","\n","        # Print losses and accuracies #\n","        end = time.time()\n","        if config['perturb_type'] is None:\n","            print('epoch: {}, train loss: {:.10f}, train acc: {:.5f}, test loss: {:.10f}, test acc: {:.5f}, {:.2f}s'.format(epoch + 1, epoch_mean_train_loss, epoch_train_accuracy, epoch_mean_test_loss, epoch_test_accuracy, end - start))\n","        else:\n","            print('epoch: {}, train loss: {:.10f}, train acc: {:.5f}, perturb loss: {:.10f}, test loss: {:.10f}, test acc: {:.5f}, {:.2f}s'.format(epoch + 1, epoch_mean_train_loss, epoch_train_accuracy, epoch_mean_perturb_loss, epoch_mean_test_loss, epoch_test_accuracy, end - start))\n","    return history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ol2zoyKBsRjb"},"source":["train_loader, test_loader = get_data_loader(config)\n","model = get_model(config)\n","optimizer = torch.optim.SGD(model.parameters(), lr=config['step_size'], momentum=0.9, weight_decay=config['L2_decay'])\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(config['epochs'] * 0.5), int(config['epochs'] * 0.75)], gamma=0.1)\n","history = training(train_loader, test_loader, model, optimizer, step_size_scheduler, config)\n","timestamp = datetime.datetime.now().strftime(\"%d-%m-%y-%H-%M-%S\")\n","save_history(history, save_folder_dir, file_prefix, timestamp, config['CLP'])\n","plot_lines(history, config, save_folder_dir, file_prefix, timestamp, config['CLP'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hGxRYIHg4OKE"},"source":[""],"execution_count":null,"outputs":[]}]}
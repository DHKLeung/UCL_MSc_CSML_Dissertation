{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ultimate General Experiment Framework colab ver 2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPL8WSN20H1n+wgXH88hTfS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"wybNB3V5Y_sB"},"source":["# Configuration"]},{"cell_type":"code","metadata":{"id":"SjFLLO7ipr76"},"source":["# Configuration #\n","config = {\n","    'validation_times': None,\n","    'data_name': None,  # {'breast_cancer', 'cifar10', 'MNIST', 'FashionMNIST', 'spambase', 'abalone', 'iris', 'wine'}\n","    'perturb_type': None,  # {None, 'mixup', 'mixup sc', 'mixup nb', 'gauss VRM'}\n","    'augWPL': None,  # {None, 0.25, 0.5, 0.75, 0.9, 0.99} or others\n","    'geometric_param': None,  # {0.25, 0.5, 0.75, 1.} or others\n","    'gauss_vicinal_std': None,  # {0.25, 0.5, 0.75, 1.} or others\n","    'batch_size': None,\n","    'step_size': None,  # {0.1, 0.01, 0.001, 0.0005} or others\n","    'epochs': None,\n","    'L2_decay': None,  # {0., 1e-4} or others\n","    'alpha': None,  # {0.25, 0.5, 0.75, 1.} or others\n","    'breast_spam_model': None  # {'complex_tanh_sigmoid', 'simple_relu_sigmoid', 'simple_relu_softmax'}; ONLY VALID for legacy 'breast_cancer' and 'spambase'\n","}\n","if config['data_name'] in ['breast_cancer', 'spambase']:\n","    if config['breast_spam_model'] in ['complex_tanh_sigmoid', 'simple_relu_sigmoid']:\n","        config['criterion_type'] = 'BCE'\n","    elif config['breast_spam_model'] in ['simple_relu_softmax']:\n","        config['criterion_type'] = 'CE'\n","elif config['data_name'] in  ['cifar10', 'MNIST', 'FashionMNIST', 'abalone', 'iris', 'wine']:\n","    config['criterion_type'] = 'CE'\n","\n","# Folders and Files #\n","gdrive_dir = '/content/gdrive'\n","experiment_folder_dir = gdrive_dir + '/My Drive/colab'\n","save_folder_dir = experiment_folder_dir + '/{}-{}'.format(str(config['data_name']), str(config['perturb_type']))\n","file_prefix = '{}-{}-{}-{}-'.format(str(config['augWPL']), str(config['alpha']), str(config['geometric_param']), str(config['gauss_vicinal_std']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NpP4X8dP4cGj"},"source":["# Libraries"]},{"cell_type":"code","metadata":{"id":"Cvxw76jyn_lp"},"source":["import torch\n","from torchvision import transforms, datasets, models\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount(gdrive_dir)\n","import sys\n","sys.path.append(experiment_folder_dir)\n","import time\n","import datetime\n","import os\n","import json\n","import resnet\n","import load_external_data\n","import pandas as pd\n","\n","if not os.path.exists(save_folder_dir):\n","    os.mkdir(save_folder_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kKQa_Key4eEZ"},"source":["# Data Augmentation / Perturbation related functions"]},{"cell_type":"code","metadata":{"id":"3Hi7aPfE4XFC"},"source":["def mixup(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    batch_size = labels.size(0)\n","    idx = torch.randperm(batch_size).to('cuda')\n","    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n","    labels_b = labels[idx]\n","    return mixup_inputs, labels, labels_b, lmbda"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D6VcCy1IwgP9"},"source":["def mixup_sc(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_uc_list = list()\n","    labels_uc_list = list()\n","    unique_classes = torch.unique(labels)\n","    for uc in unique_classes:\n","        mask_uc = (labels == uc).flatten()  # flatten to avoid the labels are in column vector\n","        inputs_uc = inputs[mask_uc]\n","        labels_uc = labels[mask_uc]\n","        batch_size_uc = labels_uc.size(0)\n","        idx = torch.randperm(batch_size_uc).to('cuda')\n","        mixup_inputs_uc = lmbda * inputs_uc + (1 - lmbda) * inputs_uc[idx]\n","        mixup_inputs_uc_list.append(mixup_inputs_uc)\n","        labels_uc_list.append(labels_uc)\n","    mixup_inputs_sc = torch.vstack(mixup_inputs_uc_list)\n","    mixup_labels_sc = torch.cat(labels_uc_list, dim=0)  # use cat not hstack to avoid labels are in column vector\n","    return mixup_inputs_sc, mixup_labels_sc, lmbda"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H3615DIf3poh"},"source":["def mixup_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LI_ziZaZ3y_C"},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iBOl0OTV9xpd"},"source":["def gauss_vicinal(inputs, gauss_vicinal_std):\n","    inputs_gauss = torch.normal(inputs, gauss_vicinal_std)\n","    return inputs_gauss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ixum4Iss48sJ"},"source":["# Data Loader"]},{"cell_type":"code","metadata":{"id":"3QLo4lDG4_KN"},"source":["def get_data_loader(config):\n","    if config['data_name'] == 'breast_cancer':\n","        train_data, train_labels, val_data, val_labels, test_data, test_labels = load_external_data.load_skl_data('breast_cancer')\n","        test_data = np.vstack((val_data, test_data))\n","        test_labels = np.hstack((val_labels, test_labels))\n","        train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","        train_labels = torch.from_numpy(train_labels)\n","        test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","        test_labels = torch.from_numpy(test_labels)\n","        train_mean = torch.mean(train_data, 0)\n","        train_std = torch.std(train_data, 0)\n","        train_data = (train_data - train_mean) / train_std\n","        test_data = (test_data - train_mean) / train_std\n","        train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","        test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n","        test_loader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n","    elif config['data_name'] == 'cifar10':\n","        transform_train = transforms.Compose([\n","            transforms.RandomCrop(32, padding=4),  # can omit\n","            transforms.RandomHorizontalFlip(),  # can omit\n","            transforms.ToTensor(),\n","            transforms.Normalize(\n","                (0.4914, 0.4822, 0.4465),\n","                (0.2023, 0.1994, 0.2010)\n","            )\n","        ])\n","        transform_test = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(\n","                (0.4914, 0.4822, 0.4465),\n","                (0.2023, 0.1994, 0.2010)\n","            )\n","        ])\n","        train_set = datasets.CIFAR10(root=experiment_folder_dir, train=True, download=True, transform=transform_train)\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n","        test_set = datasets.CIFAR10(root=experiment_folder_dir, train=False, download=True, transform=transform_test)\n","        test_loader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n","    elif config['data_name'] == 'MNIST':\n","        transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.1307,), (0.3015,))\n","        ])\n","        train_set = datasets.MNIST(root=experiment_folder_dir, train=True, download=True, transform=transform)\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n","        test_set = datasets.MNIST(root=experiment_folder_dir, train=False, download=True, transform=transform)\n","        test_loader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n","    elif config['data_name'] == 'FashionMNIST':\n","        transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.2860,), (0.3205,))\n","        ])\n","        train_set = datasets.FashionMNIST(root=experiment_folder_dir, train=True, download=True, transform=transform)\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n","        test_set = datasets.FashionMNIST(root=experiment_folder_dir, train=False, download=True, transform=transform)\n","        test_loader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n","    elif config['data_name'] == 'spambase':\n","        spambase = pd.read_csv(experiment_folder_dir + '/dataset_44_spambase.csv').to_numpy()\n","        data, labels = spambase[:, :-1], spambase[:, -1]\n","        num_data = labels.shape[0]\n","        idx = np.random.permutation(num_data)\n","        data = data[idx]\n","        labels = labels[idx]\n","        splitpoint = int(num_data * 0.6)\n","        train_data = data[:splitpoint]\n","        train_labels = labels[:splitpoint]\n","        test_data = data[splitpoint:]\n","        test_labels = labels[splitpoint:]\n","        train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","        train_labels = torch.from_numpy(train_labels)\n","        test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","        test_labels = torch.from_numpy(test_labels)\n","        train_mean = torch.mean(train_data, 0)\n","        train_std = torch.std(train_data, 0)\n","        train_data = (train_data - train_mean) / train_std\n","        test_data = (test_data - train_mean) / train_std\n","        train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","        test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n","        test_loader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n","    elif config['data_name'] == 'abalone':\n","        column_names = [\"sex\", \"length\", \"diameter\", \"height\", \"whole weight\", \"shucked weight\", \"viscera weight\", \"shell weight\", \"rings\"]\n","        abalone = pd.read_csv(experiment_folder_dir + '/' + 'abalone.data', names=column_names)\n","        for label in \"MFI\":\n","            abalone[label] = abalone[\"sex\"] == label\n","        del abalone[\"sex\"]\n","        labels = abalone.rings.values\n","        del abalone[\"rings\"]\n","        data = abalone.values.astype(np.float32)\n","        num_data = labels.shape[0]\n","        idx = np.random.permutation(num_data)\n","        data = data[idx]\n","        labels = labels[idx]\n","        labels[labels == 29] = 28\n","        labels = labels - 1\n","        splitpoint = int(num_data * 0.6)\n","        train_data = data[:splitpoint]\n","        train_labels = labels[:splitpoint]\n","        test_data = data[splitpoint:]\n","        test_labels = labels[splitpoint:]\n","        train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","        train_labels = torch.from_numpy(train_labels)\n","        test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","        test_labels = torch.from_numpy(test_labels)\n","        train_mean = torch.mean(train_data, 0)\n","        train_std = torch.std(train_data, 0)\n","        train_data = (train_data - train_mean) / train_std\n","        test_data = (test_data - train_mean) / train_std\n","        train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","        test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n","        test_loader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n","    elif config['data_name'] == 'iris':\n","        train_data, train_labels, val_data, val_labels, test_data, test_labels = load_external_data.load_skl_data('iris')\n","        test_data = np.vstack((val_data, test_data))\n","        test_labels = np.hstack((val_labels, test_labels))\n","        train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","        train_labels = torch.from_numpy(train_labels)\n","        test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","        test_labels = torch.from_numpy(test_labels)\n","        train_mean = torch.mean(train_data, 0)\n","        train_std = torch.std(train_data, 0)\n","        train_data = (train_data - train_mean) / train_std\n","        test_data = (test_data - train_mean) / train_std\n","        train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","        test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n","        test_loader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n","    elif config['data_name'] == 'wine':\n","        train_data, train_labels, val_data, val_labels, test_data, test_labels = load_external_data.load_skl_data('wine')\n","        test_data = np.vstack((val_data, test_data))\n","        test_labels = np.hstack((val_labels, test_labels))\n","        train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","        train_labels = torch.from_numpy(train_labels)\n","        test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","        test_labels = torch.from_numpy(test_labels)\n","        train_mean = torch.mean(train_data, 0)\n","        train_std = torch.std(train_data, 0)\n","        train_data = (train_data - train_mean) / train_std\n","        test_data = (test_data - train_mean) / train_std\n","        train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","        test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","        train_loader = torch.utils.data.DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0)\n","        test_loader = torch.utils.data.DataLoader(test_set, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n","    return train_loader, test_loader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UPZqcYGh65-k"},"source":["# Models"]},{"cell_type":"code","metadata":{"id":"jv3jxDXi7FxA"},"source":["def get_model(config):\n","    if config['data_name'] == 'breast_cancer':\n","        if config['breast_spam_model'] == 'complex_tanh_sigmoid':\n","            class fc_model(nn.Module):\n","                def __init__(self):\n","                    super(fc_model, self).__init__()\n","                    self.fc1 = nn.Linear(30, 128)\n","                    self.fc2 = nn.Linear(128, 64)\n","                    self.fc3 = nn.Linear(64, 32)\n","                    self.fc4 = nn.Linear(32, 1)\n","                def forward(self, inputs):\n","                    fc1_out = F.tanh(self.fc1(inputs))\n","                    fc2_out = F.tanh(self.fc2(fc1_out))\n","                    fc3_out = F.tanh(self.fc3(fc2_out))\n","                    fc4_out = self.fc4(fc3_out)\n","                    return fc4_out\n","        elif config['breast_spam_model'] == 'simple_relu_sigmoid':\n","            class fc_model(nn.Module):\n","                def __init__(self):\n","                    super(fc_model, self).__init__()\n","                    self.fc1 = nn.Linear(30, 128)\n","                    self.fc2 = nn.Linear(128, 128)\n","                    self.fc3 = nn.Linear(128, 1)\n","                def forward(self, inputs):\n","                    fc1_out = F.relu(self.fc1(inputs))\n","                    fc2_out = F.relu(self.fc2(fc1_out))\n","                    fc3_out = self.fc3(fc2_out)\n","                    return fc3_out\n","        elif config['breast_spam_model'] == 'simple_relu_softmax':\n","            class fc_model(nn.Module):\n","                def __init__(self):\n","                    super(fc_model, self).__init__()\n","                    self.fc1 = nn.Linear(30, 128)\n","                    self.fc2 = nn.Linear(128, 128)\n","                    self.fc3 = nn.Linear(128, 2)\n","                def forward(self, inputs):\n","                    fc1_out = F.relu(self.fc1(inputs))\n","                    fc2_out = F.relu(self.fc2(fc1_out))\n","                    fc3_out = self.fc3(fc2_out)\n","                    return fc3_out \n","        model = fc_model()\n","        model.cuda()\n","    elif config['data_name'] == 'cifar10':\n","        model = resnet.ResNet18()\n","        model.cuda()\n","    elif config['data_name'] == 'MNIST':\n","        model = models.resnet18(pretrained=False)\n","        for param in model.parameters():\n","            param.requires_grad = True\n","        model.conv1 = torch.nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n","        model.fc = torch.nn.Linear(512, 10)\n","        model.cuda()\n","    elif config['data_name'] == 'FashionMNIST':\n","        model = models.resnet18(pretrained=False)\n","        for param in model.parameters():\n","            param.requires_grad = True\n","        model.conv1 = torch.nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n","        model.fc = torch.nn.Linear(512, 10)\n","        model.cuda()\n","    elif config['data_name'] == 'spambase':\n","        if config['breast_spam_model'] == 'complex_tanh_sigmoid':\n","            class fc_model(nn.Module):\n","                def __init__(self):\n","                    super(fc_model, self).__init__()\n","                    self.fc1 = nn.Linear(57, 256)\n","                    self.fc2 = nn.Linear(256, 128)\n","                    self.fc3 = nn.Linear(128, 64)\n","                    self.fc4 = nn.Linear(64, 32)\n","                    self.fc5 = nn.Linear(32, 1)\n","                def forward(self, inputs):\n","                    fc1_out = F.tanh(self.fc1(inputs))\n","                    fc2_out = F.tanh(self.fc2(fc1_out))\n","                    fc3_out = F.tanh(self.fc3(fc2_out))\n","                    fc4_out = F.tanh(self.fc4(fc3_out))\n","                    fc5_out = self.fc5(fc4_out)\n","                    return fc5_out\n","        elif config['breast_spam_model'] == 'simple_relu_sigmoid':\n","            class fc_model(nn.Module):\n","                def __init__(self):\n","                    super(fc_model, self).__init__()\n","                    self.fc1 = nn.Linear(57, 128)\n","                    self.fc2 = nn.Linear(128, 128)\n","                    self.fc3 = nn.Linear(128, 1)\n","                def forward(self, inputs):\n","                    fc1_out = F.relu(self.fc1(inputs))\n","                    fc2_out = F.relu(self.fc2(fc1_out))\n","                    fc3_out = self.fc3(fc2_out)\n","                    return fc3_out\n","        elif config['breast_spam_model'] == 'simple_relu_softmax':\n","            class fc_model(nn.Module):\n","                def __init__(self):\n","                    super(fc_model, self).__init__()\n","                    self.fc1 = nn.Linear(57, 128)\n","                    self.fc2 = nn.Linear(128, 128)\n","                    self.fc3 = nn.Linear(128, 2)\n","                def forward(self, inputs):\n","                    fc1_out = F.relu(self.fc1(inputs))\n","                    fc2_out = F.relu(self.fc2(fc1_out))\n","                    fc3_out = self.fc3(fc2_out)\n","                    return fc3_out \n","        model = fc_model()\n","        model.cuda()\n","    elif config['data_name'] == 'abalone':\n","        class fc_model(nn.Module):\n","            def __init__(self):\n","                super(fc_model, self).__init__()\n","                self.fc1 = nn.Linear(10, 128)\n","                self.fc2 = nn.Linear(128, 128)\n","                self.fc3 = nn.Linear(128, 28)\n","            def forward(self, inputs):\n","                fc1_out = F.relu(self.fc1(inputs))\n","                fc2_out = F.relu(self.fc2(fc1_out))\n","                fc3_out = self.fc3(fc2_out)\n","                return fc3_out\n","        model = fc_model()\n","        model.cuda()\n","    elif config['data_name'] == 'iris':\n","        class fc_model(nn.Module):\n","            def __init__(self):\n","                super(fc_model, self).__init__()\n","                self.fc1 = nn.Linear(4, 128)\n","                self.fc2 = nn.Linear(128, 128)\n","                self.fc3 = nn.Linear(128, 3)\n","            def forward(self, inputs):\n","                fc1_out = F.relu(self.fc1(inputs))\n","                fc2_out = F.relu(self.fc2(fc1_out))\n","                fc3_out = self.fc3(fc2_out)\n","                return fc3_out\n","        model = fc_model()\n","        model.cuda()\n","    elif config['data_name'] == 'wine':\n","        class fc_model(nn.Module):\n","            def __init__(self):\n","                super(fc_model, self).__init__()\n","                self.fc1 = nn.Linear(13, 128)\n","                self.fc2 = nn.Linear(128, 128)\n","                self.fc3 = nn.Linear(128, 3)\n","            def forward(self, inputs):\n","                fc1_out = F.relu(self.fc1(inputs))\n","                fc2_out = F.relu(self.fc2(fc1_out))\n","                fc3_out = self.fc3(fc2_out)\n","                return fc3_out\n","        model = fc_model()\n","        model.cuda()\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IARNLbNzBOe0"},"source":["# Testing"]},{"cell_type":"code","metadata":{"id":"XWi9rDWNBYOx"},"source":["def testing(data_loader, criterion, model, config):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    loss = 0.\n","\n","    # Store perturbation loss if required #\n","    if config['perturb_type'] is not None:\n","        perturb_loss = 0\n","    \n","    # Start testing #\n","    with torch.no_grad():\n","        for data in data_loader:\n","\n","            # Get the loss of the batch #\n","            inputs, labels = data\n","            inputs = inputs.to('cuda')\n","            if config['criterion_type'] == 'BCE':\n","                labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","            elif config['criterion_type'] == 'CE':\n","                labels = labels.type(torch.LongTensor).to('cuda')\n","            outputs = model(inputs)\n","            batch_loss = criterion(outputs, labels)\n","            loss += batch_loss.item()\n","\n","            # Get the loss of the perturb batch if required #\n","            if config['perturb_type'] == 'mixup':\n","                mixup_inputs, mixup_labels_a, mixup_labels_b, lmbda = mixup(inputs, labels, config['alpha'])\n","                mixup_outputs = model(mixup_inputs)\n","                batch_mixup_loss = mixup_criterion(criterion, mixup_outputs, mixup_labels_a, mixup_labels_b, lmbda)\n","                perturb_loss += batch_mixup_loss.item()\n","            elif config['perturb_type'] == 'mixup sc':\n","                mixup_inputs_sc, mixup_labels_sc, lmbda = mixup_sc(inputs, labels, config['alpha'])\n","                mixup_outputs_sc = model(mixup_inputs_sc)\n","                batch_mixup_loss_sc = criterion(mixup_outputs_sc, mixup_labels_sc)\n","                perturb_loss += batch_mixup_loss_sc.item()\n","            elif config['perturb_type'] == 'mixup nb':\n","                mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_nb(inputs, labels, config['geometric_param'], config['alpha'])\n","                mixup_outputs_nb = model(mixup_inputs_nb)\n","                batch_mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","                perturb_loss += batch_mixup_loss_nb.item()\n","            elif config['perturb_type'] == 'gauss VRM':\n","                gauss_inputs = gauss_vicinal(inputs, config['gauss_vicinal_std'])\n","                gauss_outputs = model(gauss_inputs)\n","                batch_gauss_loss = criterion(gauss_outputs, labels)\n","                perturb_loss += batch_gauss_loss.item()\n","            \n","            # Compute predictions #\n","            if config['criterion_type'] == 'BCE':\n","                predicts = (torch.sign(outputs) + 1) / 2\n","            elif config['criterion_type'] == 'CE':\n","                _, predicts = torch.max(outputs, 1)\n","\n","            # Accumulation #\n","            total += labels.size(0)\n","            correct += (predicts == labels).sum().item()\n","    \n","    # Compute accuracy #\n","    accuracy = correct / total\n","\n","    # Compute mean losses #\n","    mean_loss = loss / total\n","    if config['perturb_type'] is not None:\n","        mean_perturb_loss = perturb_loss / total\n","\n","    # Return according to required #\n","    model.train()\n","    if config['perturb_type'] is not None:\n","        return mean_loss, accuracy, mean_perturb_loss\n","    else:\n","        return mean_loss, accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"REzrKKutSlQp"},"source":["# Plot"]},{"cell_type":"code","metadata":{"id":"He-pD9w0SjMJ"},"source":["def plot_lines(history, config, save_folder_dir, file_prefix, timestamp):\n","\n","    # Make title #\n","    final_test_acc = history['epoch_test_accuracy'][-1]\n","    if config['augWPL'] is None:\n","        if config['perturb_type'] is None:\n","            title = '{}; final test acc: {:.7f}'.format(config['data_name'], final_test_acc)\n","        else:\n","            title = '{} {}; final test acc: {:.7f}'.format(config['data_name'], config['perturb_type'], final_test_acc)\n","    else:\n","        title = '{} {} WPL{}; final test acc: {:.7f}'.format(config['data_name'], config['perturb_type'], config['augWPL'], final_test_acc)\n","    \n","    # Plot losses #\n","    plt.figure(figsize=(10, 7))\n","    plt.plot(history['epoch_mean_train_loss'], label='train')\n","    plt.plot(history['epoch_mean_test_loss'], label='test')\n","    if config['perturb_type'] is not None:\n","        plt.plot(history['epoch_mean_perturb_loss'], label=config['perturb_type'])\n","    plt.grid()\n","    plt.legend()\n","    plt.title(title)\n","    plt.xlabel('epoch')\n","    plt.ylabel('loss')\n","    plt.savefig('{} loss.png'.format(save_folder_dir + '/' + file_prefix + timestamp))\n","    plt.show()\n","\n","    # Plot accuracies #\n","    plt.figure(figsize=(10, 7))\n","    plt.plot(history['epoch_train_accuracy'], label='train')\n","    plt.plot(history['epoch_test_accuracy'], label='test')\n","    plt.grid()\n","    plt.legend()\n","    plt.title(title)\n","    plt.xlabel('epoch')\n","    plt.ylabel('accuracy')\n","    plt.savefig('{} accuracy.png'.format(save_folder_dir + '/' + file_prefix + timestamp))\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZHJ6KDHq_Zuf"},"source":["# Save history"]},{"cell_type":"code","metadata":{"id":"p7NYJgSp_ZLZ"},"source":["def save_history(history, save_folder_dir, file_prefix, timestamp):\n","    json_file_name = file_prefix + timestamp + '.json'\n","    with open(save_folder_dir + '/' + json_file_name, 'w') as fp:\n","        json.dump(history, fp)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pj8Pmco-BKkl"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"JqCX0ryvBKY3"},"source":["def training(train_loader, test_loader, model, optimizer, step_size_scheduler, config):\n","\n","    # History #\n","    history = {\n","        'epoch_mean_train_loss': list(),\n","        'epoch_train_accuracy': list(),\n","        'epoch_mean_test_loss': list(),\n","        'epoch_test_accuracy': list(),\n","    }\n","    if config['perturb_type'] is not None:\n","        history['epoch_mean_perturb_loss'] = list()\n","    \n","    # Define criterion #\n","    if config['criterion_type'] == 'BCE':\n","        criterion = torch.nn.BCEWithLogitsLoss()\n","    elif config['criterion_type'] == 'CE':\n","        criterion = torch.nn.CrossEntropyLoss()\n","    \n","    # Start training #\n","    for epoch in range(config['epochs']):\n","        start = time.time()\n","        for i, data in enumerate(train_loader, 0):\n","            model.train()\n","            optimizer.zero_grad()\n","            \n","            # Get inputs and labels #\n","            inputs, labels = data\n","            inputs = inputs.to('cuda')\n","            if config['criterion_type'] == 'BCE':\n","                labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","            elif config['criterion_type'] == 'CE':\n","                labels = labels.type(torch.LongTensor).to('cuda')\n","\n","            # Perturbation #\n","            if config['perturb_type'] == 'mixup':\n","                perturb_inputs, perturb_labels_a, perturb_labels_b, lmbda = mixup(inputs, labels, config['alpha'])\n","            elif config['perturb_type'] == 'mixup sc':\n","                perturb_inputs, perturb_labels, lmbda = mixup_sc(inputs, labels, config['alpha'])\n","            elif config['perturb_type'] == 'mixup nb':\n","                perturb_inputs, perturb_labels_a, perturb_labels_b, lmbda = mixup_nb(inputs, labels, config['geometric_param'], config['alpha'])\n","            elif config['perturb_type'] == 'gauss VRM':\n","                perturb_inputs = gauss_vicinal(inputs, config['gauss_vicinal_std'])\n","            \n","            # Augmentation or not #\n","            if config['augWPL'] is None:\n","                if config['perturb_type'] is None:\n","                    ultimate_inputs = inputs\n","                else:\n","                    ultimate_inputs = perturb_inputs\n","            else:\n","                ultimate_inputs = torch.vstack((inputs, perturb_inputs))\n","\n","            # Get outputs #\n","            ultimate_outputs = model(ultimate_inputs)\n","            if config['augWPL'] is None:\n","                if config['perturb_type'] is None:\n","                    outputs = ultimate_outputs\n","                else:\n","                    perturb_outputs = ultimate_outputs\n","            else:\n","                current_batch_size = labels.size(0)\n","                outputs = ultimate_outputs[:current_batch_size]\n","                perturb_outputs = ultimate_outputs[current_batch_size:]\n","            \n","            # Compute losses #\n","            if config['augWPL'] is None:\n","                if config['perturb_type'] is None:\n","                    ultimate_loss = criterion(outputs, labels)\n","                elif config['perturb_type'] == 'mixup':\n","                    ultimate_loss = mixup_criterion(criterion, perturb_outputs, perturb_labels_a, perturb_labels_b, lmbda)\n","                elif config['perturb_type'] == 'mixup sc':\n","                    ultimate_loss = criterion(perturb_outputs, perturb_labels)\n","                elif config['perturb_type'] == 'mixup nb':\n","                    ultimate_loss = mixup_criterion(criterion, perturb_outputs, perturb_labels_a, perturb_labels_b, lmbda)\n","                elif config['perturb_type'] == 'gauss VRM':\n","                    ultimate_loss = criterion(perturb_outputs, labels)\n","            else:\n","                loss = criterion(outputs, labels)\n","                if config['perturb_type'] == 'mixup':\n","                    perturb_loss = mixup_criterion(criterion, perturb_outputs, perturb_labels_a, perturb_labels_b, lmbda)\n","                elif config['perturb_type'] == 'mixup sc':\n","                    perturb_loss = criterion(perturb_outputs, perturb_labels)\n","                elif config['perturb_type'] == 'mixup nb':\n","                    perturb_loss = mixup_criterion(criterion, perturb_outputs, perturb_labels_a, perturb_labels_b, lmbda)\n","                elif config['perturb_type'] == 'gauss VRM':\n","                    perturb_loss = criterion(perturb_outputs, labels)\n","                ultimate_loss = config['augWPL'] * perturb_loss + (1 - config['augWPL']) * loss\n","\n","            # Gradient calculation and optimisation #\n","            ultimate_loss.backward()\n","            optimizer.step()\n","\n","        # Step size scheduler #\n","        step_size_scheduler.step()\n","\n","        # Testing on Train and Test data #\n","        if config['perturb_type'] is None:\n","            epoch_mean_train_loss, epoch_train_accuracy = testing(train_loader, criterion, model, config)\n","            epoch_mean_test_loss, epoch_test_accuracy = testing(test_loader, criterion, model, config)\n","        else:\n","            epoch_mean_train_loss, epoch_train_accuracy, epoch_mean_perturb_loss = testing(train_loader, criterion, model, config)\n","            epoch_mean_test_loss, epoch_test_accuracy, _ = testing(test_loader, criterion, model, config)\n","        history['epoch_mean_train_loss'].append(epoch_mean_train_loss)\n","        history['epoch_train_accuracy'].append(epoch_train_accuracy)\n","        history['epoch_mean_test_loss'].append(epoch_mean_test_loss)\n","        history['epoch_test_accuracy'].append(epoch_test_accuracy)\n","        if config['perturb_type'] is not None:\n","            history['epoch_mean_perturb_loss'].append(epoch_mean_perturb_loss)\n","\n","        # Print losses and accuracies #\n","        end = time.time()\n","        if config['perturb_type'] is None:\n","            print('epoch: {}, train loss: {:.10f}, train acc: {:.5f}, test loss: {:.10f}, test acc: {:.5f}, {:.2f}s'.format(epoch + 1, epoch_mean_train_loss, epoch_train_accuracy, epoch_mean_test_loss, epoch_test_accuracy, end - start))\n","        else:\n","            print('epoch: {}, train loss: {:.10f}, train acc: {:.5f}, perturb loss: {:.10f}, test loss: {:.10f}, test acc: {:.5f}, {:.2f}s'.format(epoch + 1, epoch_mean_train_loss, epoch_train_accuracy, epoch_mean_perturb_loss, epoch_mean_test_loss, epoch_test_accuracy, end - start))\n","    return history"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ol2zoyKBsRjb"},"source":["train_loader, test_loader = get_data_loader(config)\n","model = get_model(config)\n","optimizer = torch.optim.SGD(model.parameters(), lr=config['step_size'], momentum=0.9, weight_decay=config['L2_decay'])\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(config['epochs'] * 0.5), int(config['epochs'] * 0.75)], gamma=0.1)\n","history = training(train_loader, test_loader, model, optimizer, step_size_scheduler, config)\n","timestamp = datetime.datetime.now().strftime(\"%d-%m-%y-%H-%M-%S\")\n","save_history(history, save_folder_dir, file_prefix, timestamp)\n","plot_lines(history, config, save_folder_dir, file_prefix, timestamp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uLNewD5eC1_x"},"source":[""],"execution_count":null,"outputs":[]}]}
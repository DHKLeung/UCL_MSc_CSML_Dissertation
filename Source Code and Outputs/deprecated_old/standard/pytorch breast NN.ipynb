{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "proprietary-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from load_data import load_skl_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "silent-johns",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1953bea2470>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration and Hyperparameters\n",
    "\"\"\"\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU\n",
    "\n",
    "batch_size = 128\n",
    "step_size = 0.005\n",
    "random_seed = 0\n",
    "epochs = 300\n",
    "L2_decay = 1e-4\n",
    "\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compressed-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n",
    "test_data = np.vstack((val_data, test_data))\n",
    "test_labels = np.hstack((val_labels, test_labels))\n",
    "train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n",
    "train_labels = torch.from_numpy(train_labels)\n",
    "test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "test_labels = torch.from_numpy(test_labels)\n",
    "train_mean = torch.mean(train_data, 0)\n",
    "train_std = torch.std(train_data, 0)\n",
    "train_data = (train_data - train_mean) / train_std\n",
    "test_data = (test_data - train_mean) / train_std\n",
    "train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "buried-science",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc_model(\n",
      "  (fc1): Linear(in_features=30, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class fc_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(fc_model, self).__init__()\n",
    "        self.fc1 = nn.Linear(30, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "    def forward(self, inputs):\n",
    "        fc1_out = F.tanh(self.fc1(inputs))\n",
    "        fc2_out = F.tanh(self.fc2(fc1_out))\n",
    "        fc3_out = F.tanh(self.fc3(fc2_out))\n",
    "        fc4_out = self.fc4(fc3_out)\n",
    "        return fc4_out\n",
    "\n",
    "model = fc_model()\n",
    "print(model)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pediatric-namibia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\anaconda3\\envs\\Python3_8\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 2.047994315624237\n",
      "1: 2.010105073451996\n",
      "2: 1.9496511220932007\n",
      "3: 1.8717715740203857\n",
      "4: 1.787783443927765\n",
      "5: 1.6905683875083923\n",
      "6: 1.5823449492454529\n",
      "7: 1.4626256823539734\n",
      "8: 1.352182298898697\n",
      "9: 1.2278039753437042\n",
      "10: 1.1168799102306366\n",
      "11: 0.9960262179374695\n",
      "12: 0.8955446779727936\n",
      "13: 0.7885326743125916\n",
      "14: 0.704965204000473\n",
      "15: 0.6251966804265976\n",
      "16: 0.5694921761751175\n",
      "17: 0.5158136487007141\n",
      "18: 0.47397705912590027\n",
      "19: 0.44774311780929565\n",
      "20: 0.41315560042858124\n",
      "21: 0.40174075961112976\n",
      "22: 0.3805883899331093\n",
      "23: 0.3481157645583153\n",
      "24: 0.34728965163230896\n",
      "25: 0.3316306173801422\n",
      "26: 0.30132095515727997\n",
      "27: 0.3025118187069893\n",
      "28: 0.27912455052137375\n",
      "29: 0.27446646243333817\n",
      "30: 0.26648883521556854\n",
      "31: 0.2732774503529072\n",
      "32: 0.26753462105989456\n",
      "33: 0.24780430644750595\n",
      "34: 0.2473195642232895\n",
      "35: 0.2478741928935051\n",
      "36: 0.23622924089431763\n",
      "37: 0.22828151658177376\n",
      "38: 0.22653429955244064\n",
      "39: 0.23575857281684875\n",
      "40: 0.20936499536037445\n",
      "41: 0.20908019691705704\n",
      "42: 0.20975225046277046\n",
      "43: 0.21118436753749847\n",
      "44: 0.20984386652708054\n",
      "45: 0.19375802390277386\n",
      "46: 0.1974937655031681\n",
      "47: 0.19616146385669708\n",
      "48: 0.20261362940073013\n",
      "49: 0.20992792397737503\n",
      "50: 0.19218112900853157\n",
      "51: 0.19827552512288094\n",
      "52: 0.19804681092500687\n",
      "53: 0.18177485093474388\n",
      "54: 0.18533233925700188\n",
      "55: 0.19275964051485062\n",
      "56: 0.1840939037501812\n",
      "57: 0.1832781545817852\n",
      "58: 0.2060503289103508\n",
      "59: 0.17720187455415726\n",
      "60: 0.1686232965439558\n",
      "61: 0.16718758828938007\n",
      "62: 0.16753790527582169\n",
      "63: 0.18018513172864914\n",
      "64: 0.16628731414675713\n",
      "65: 0.17787951976060867\n",
      "66: 0.16385459154844284\n",
      "67: 0.15766791254281998\n",
      "68: 0.16921888291835785\n",
      "69: 0.17638575285673141\n",
      "70: 0.15531323850154877\n",
      "71: 0.16635756939649582\n",
      "72: 0.1539729591459036\n",
      "73: 0.17745086923241615\n",
      "74: 0.15815484523773193\n",
      "75: 0.17026103287935257\n",
      "76: 0.17390065640211105\n",
      "77: 0.16453658416867256\n",
      "78: 0.15829848870635033\n",
      "79: 0.1610320433974266\n",
      "80: 0.15026632137596607\n",
      "81: 0.16377995535731316\n",
      "82: 0.1578753925859928\n",
      "83: 0.16495232842862606\n",
      "84: 0.1576865389943123\n",
      "85: 0.1494489163160324\n",
      "86: 0.16045723482966423\n",
      "87: 0.14653345569968224\n",
      "88: 0.16266031190752983\n",
      "89: 0.1468699537217617\n",
      "90: 0.1600567288696766\n",
      "91: 0.13891792856156826\n",
      "92: 0.15643272548913956\n",
      "93: 0.15228145942091942\n",
      "94: 0.15626788511872292\n",
      "95: 0.1640964113175869\n",
      "96: 0.15539615601301193\n",
      "97: 0.13966041058301926\n",
      "98: 0.13666364550590515\n",
      "99: 0.14631452411413193\n",
      "100: 0.14383405447006226\n",
      "101: 0.1685500703752041\n",
      "102: 0.1490892469882965\n",
      "103: 0.13433660194277763\n",
      "104: 0.14763065706938505\n",
      "105: 0.1345356572419405\n",
      "106: 0.1363723073154688\n",
      "107: 0.14617450535297394\n",
      "108: 0.14171889051795006\n",
      "109: 0.1422404646873474\n",
      "110: 0.146760992705822\n",
      "111: 0.14112144149839878\n",
      "112: 0.1475407686084509\n",
      "113: 0.1419088002294302\n",
      "114: 0.1503128632903099\n",
      "115: 0.14606391731649637\n",
      "116: 0.1297658011317253\n",
      "117: 0.13828953728079796\n",
      "118: 0.13977105543017387\n",
      "119: 0.14271163009107113\n",
      "120: 0.13746851682662964\n",
      "121: 0.1279197670519352\n",
      "122: 0.1582061294466257\n",
      "123: 0.12928858771920204\n",
      "124: 0.12335200421512127\n",
      "125: 0.14536883495748043\n",
      "126: 0.12667114101350307\n",
      "127: 0.12335070036351681\n",
      "128: 0.1228794613853097\n",
      "129: 0.14126059599220753\n",
      "130: 0.12903565727174282\n",
      "131: 0.12673662975430489\n",
      "132: 0.13137487322092056\n",
      "133: 0.12803984805941582\n",
      "134: 0.12597589939832687\n",
      "135: 0.1322447471320629\n",
      "136: 0.12111629918217659\n",
      "137: 0.1199249541386962\n",
      "138: 0.13052552938461304\n",
      "139: 0.12308264523744583\n",
      "140: 0.1341609936207533\n",
      "141: 0.12407983839511871\n",
      "142: 0.12232167832553387\n",
      "143: 0.13615220040082932\n",
      "144: 0.12106230109930038\n",
      "145: 0.12284195236861706\n",
      "146: 0.12387665174901485\n",
      "147: 0.13391519151628017\n",
      "148: 0.1416739858686924\n",
      "149: 0.1259574368596077\n",
      "150: 0.13088738545775414\n",
      "151: 0.11389271542429924\n",
      "152: 0.14278841018676758\n",
      "153: 0.1284415926784277\n",
      "154: 0.1379232183098793\n",
      "155: 0.13053364679217339\n",
      "156: 0.12293046712875366\n",
      "157: 0.11488116532564163\n",
      "158: 0.11772077344357967\n",
      "159: 0.11799786798655987\n",
      "160: 0.1252613142132759\n",
      "161: 0.12936980836093426\n",
      "162: 0.11799630522727966\n",
      "163: 0.12807177938520908\n",
      "164: 0.11691084690392017\n",
      "165: 0.11969251558184624\n",
      "166: 0.11614529974758625\n",
      "167: 0.1112927021458745\n",
      "168: 0.11383656412363052\n",
      "169: 0.12346882000565529\n",
      "170: 0.1331062000244856\n",
      "171: 0.1260143555700779\n",
      "172: 0.11114812642335892\n",
      "173: 0.12338846549391747\n",
      "174: 0.12748445942997932\n",
      "175: 0.11366495117545128\n",
      "176: 0.12500459514558315\n",
      "177: 0.11229091696441174\n",
      "178: 0.12876050546765327\n",
      "179: 0.11966433748602867\n",
      "180: 0.108448825776577\n",
      "181: 0.12154395878314972\n",
      "182: 0.10843442380428314\n",
      "183: 0.11094658263027668\n",
      "184: 0.11401084437966347\n",
      "185: 0.11955543980002403\n",
      "186: 0.10655771289020777\n",
      "187: 0.10932639241218567\n",
      "188: 0.1112658753991127\n",
      "189: 0.10875831916928291\n",
      "190: 0.11755156144499779\n",
      "191: 0.10837176442146301\n",
      "192: 0.11578766815364361\n",
      "193: 0.1067467238754034\n",
      "194: 0.11853157356381416\n",
      "195: 0.10646272823214531\n",
      "196: 0.11719456501305103\n",
      "197: 0.1104021267965436\n",
      "198: 0.11532177776098251\n",
      "199: 0.1272139847278595\n",
      "200: 0.10867571458220482\n",
      "201: 0.10299847926944494\n",
      "202: 0.1162728313356638\n",
      "203: 0.1047943476587534\n",
      "204: 0.10381685569882393\n",
      "205: 0.10114663187414408\n",
      "206: 0.1221837755292654\n",
      "207: 0.11672140471637249\n",
      "208: 0.1130993664264679\n",
      "209: 0.11469028331339359\n",
      "210: 0.11254630424082279\n",
      "211: 0.11034348234534264\n",
      "212: 0.1019777599722147\n",
      "213: 0.09755342686548829\n",
      "214: 0.10368117690086365\n",
      "215: 0.11063498631119728\n",
      "216: 0.11339739337563515\n",
      "217: 0.09857199527323246\n",
      "218: 0.10298794507980347\n",
      "219: 0.09956910833716393\n",
      "220: 0.11047519370913506\n",
      "221: 0.10463297460228205\n",
      "222: 0.10342048108577728\n",
      "223: 0.10095573961734772\n",
      "224: 0.0965275950729847\n",
      "225: 0.10589003749191761\n",
      "226: 0.10051970183849335\n",
      "227: 0.11815112549811602\n",
      "228: 0.10415978357195854\n",
      "229: 0.09776297584176064\n",
      "230: 0.10396239906549454\n",
      "231: 0.09844011627137661\n",
      "232: 0.10616214200854301\n",
      "233: 0.11719759926199913\n",
      "234: 0.09793783910572529\n",
      "235: 0.09565437212586403\n",
      "236: 0.09534985013306141\n",
      "237: 0.10561155527830124\n",
      "238: 0.0949623491615057\n",
      "239: 0.11229443084448576\n",
      "240: 0.10370771028101444\n",
      "241: 0.10390932392328978\n",
      "242: 0.10000478476285934\n",
      "243: 0.09080278035253286\n",
      "244: 0.09376182965934277\n",
      "245: 0.10308385640382767\n",
      "246: 0.10338493809103966\n",
      "247: 0.09266810305416584\n",
      "248: 0.0897771455347538\n",
      "249: 0.097441665828228\n",
      "250: 0.10125410184264183\n",
      "251: 0.08998617064207792\n",
      "252: 0.09117690194398165\n",
      "253: 0.09330566599965096\n",
      "254: 0.09823340177536011\n",
      "255: 0.10086167603731155\n",
      "256: 0.09442894533276558\n",
      "257: 0.0882130740210414\n",
      "258: 0.10821137949824333\n",
      "259: 0.09257910773158073\n",
      "260: 0.08660163637250662\n",
      "261: 0.10279366560280323\n",
      "262: 0.10522235184907913\n",
      "263: 0.09700419008731842\n",
      "264: 0.09815390780568123\n",
      "265: 0.10659757815301418\n",
      "266: 0.09769175574183464\n",
      "267: 0.09788397699594498\n",
      "268: 0.08931806311011314\n",
      "269: 0.09278867021203041\n",
      "270: 0.08827241323888302\n",
      "271: 0.09039204008877277\n",
      "272: 0.1005121748894453\n",
      "273: 0.0909898541867733\n",
      "274: 0.09437163732945919\n",
      "275: 0.09436366148293018\n",
      "276: 0.0925494059920311\n",
      "277: 0.09074575640261173\n",
      "278: 0.08590545505285263\n",
      "279: 0.08390357997268438\n",
      "280: 0.08382485993206501\n",
      "281: 0.08322197571396828\n",
      "282: 0.09363832697272301\n",
      "283: 0.09125406295061111\n",
      "284: 0.08252195827662945\n",
      "285: 0.0867368746548891\n",
      "286: 0.08247922547161579\n",
      "287: 0.08598623238503933\n",
      "288: 0.08209667913615704\n",
      "289: 0.09257486462593079\n",
      "290: 0.08256659936159849\n",
      "291: 0.07848443160764873\n",
      "292: 0.09129349142313004\n",
      "293: 0.08644378930330276\n",
      "294: 0.09043819084763527\n",
      "295: 0.08174091018736362\n",
      "296: 0.08357873931527138\n",
      "297: 0.08398955408483744\n",
      "298: 0.07848929474130273\n",
      "299: 0.08665585797280073\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training\n",
    "\"\"\"\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('{}: {}'.format(epoch, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brief-details",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), './model_pytorch_breast')\n",
    "model = fc_model()\n",
    "model.load_state_dict(torch.load('./model_pytorch_breast'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "latin-interest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9868421052631579\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n",
    "        outputs = model(inputs)\n",
    "        predicts = (torch.sign(outputs) + 1) / 2\n",
    "        total += labels.size(0)\n",
    "        correct += (predicts == labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "resident-overall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9941348973607038\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n",
    "        outputs = model(inputs)\n",
    "        predicts = (torch.sign(outputs) + 1) / 2\n",
    "        total += labels.size(0)\n",
    "        correct += (predicts == labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-galaxy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup cifar10 NN augmentation 0.9_weighted_perturb_loss.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"romantic-purchase","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624720380469,"user_tz":-60,"elapsed":31125,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"b23d7a2d-1f8a-4d57-ba40-657cc9ea54b6"},"source":["import torch\n","from torchvision import transforms, datasets\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"romantic-purchase","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"__nFHYcCoEzb","executionInfo":{"status":"ok","timestamp":1624720380763,"user_tz":-60,"elapsed":300,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"__nFHYcCoEzb","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silver-clear","executionInfo":{"status":"ok","timestamp":1624720380985,"user_tz":-60,"elapsed":224,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"d802286c-065d-4e97-db8e-e53bb1ad5801"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),  # can omit\n","    transforms.RandomHorizontalFlip(),  # can omit\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","batch_size = 128\n","step_size = 0.1\n","random_seed = 0\n","epochs = 200\n","L2_decay = 1e-4\n","alpha = 1.\n","perturb_loss_weight = 0.9\n","\n","torch.manual_seed(random_seed)"],"id":"silver-clear","execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fb91a100ab0>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"relative-mobility","executionInfo":{"status":"ok","timestamp":1624720392857,"user_tz":-60,"elapsed":11874,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"704d366d-e8ce-4df2-acbb-4d821b9cc1b1"},"source":["\"\"\"\n","Data\n","\"\"\"\n","train_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=True, download=True, transform=transform_train)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=False, download=True, transform=transform_test)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"relative-mobility","execution_count":4,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"starting-chancellor","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624720399637,"user_tz":-60,"elapsed":6782,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"0bdfa3f0-43c1-4380-e4d0-e511e87d87bd"},"source":["model = ResNet18()\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","model.cuda()"],"id":"starting-chancellor","execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"contemporary-gross","executionInfo":{"status":"ok","timestamp":1624720399638,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_cifar10(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    batch_size = labels.size(0)\n","    idx = torch.randperm(batch_size).to('cuda')\n","    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n","    labels_b = labels[idx]\n","    return mixup_inputs, labels, labels_b, lmbda"],"id":"contemporary-gross","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"right-spending","executionInfo":{"status":"ok","timestamp":1624720399638,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"right-spending","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"written-bookmark","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624731235683,"user_tz":-60,"elapsed":10836048,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"806911f4-c119-4429-d88b-c52eddecb577"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_loss = 0.\n","    epoch_mixup_loss = 0.\n","    epoch_org_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        mixup_inputs, labels, labels_b, lmbda = mixup_cifar10(inputs, labels, alpha)\n","        optimizer.zero_grad()\n","        outputs = model(mixup_inputs)\n","        mixup_loss = mixup_criterion(criterion, outputs, labels, labels_b, lmbda)\n","        \n","        ##\n","        outputs_org = model(inputs)\n","        loss_org = criterion(outputs_org, labels)\n","        weighted_total_loss = mixup_loss * perturb_loss_weight + loss_org * (1 - perturb_loss_weight)\n","        \n","        epoch_mixup_loss += mixup_loss.item()\n","        epoch_org_loss += loss_org.item()\n","        \n","        epoch_loss += (mixup_loss.item() + loss_org.item())\n","        \n","        weighted_total_loss.backward()\n","        ##\n","        \n","        optimizer.step()\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_org_loss, epoch_loss))"],"id":"written-bookmark","execution_count":8,"outputs":[{"output_type":"stream","text":["0: 761.6760848760605 650.0318508148193 1411.7079356908798\n","1: 639.9953774809837 460.93656969070435 1100.931947171688\n","2: 586.4313230514526 364.7716567516327 951.2029798030853\n","3: 551.0461567044258 300.232787579298 851.2789442837238\n","4: 532.2043470740318 254.7339595258236 786.9383065998554\n","5: 509.3505961596966 221.92676159739494 731.2773577570915\n","6: 487.39164143800735 199.5257419347763 686.9173833727837\n","7: 487.6272554695606 180.01106123626232 667.638316705823\n","8: 468.7454764544964 173.35169982910156 642.097176283598\n","9: 453.7957092821598 158.8429817557335 612.6386910378933\n","10: 463.23350590467453 143.70425532758236 606.9377612322569\n","11: 463.53374940156937 135.0694096237421 598.6031590253115\n","12: 446.88879016041756 127.01145388185978 573.9002440422773\n","13: 432.8888005912304 119.8022321164608 552.6910327076912\n","14: 444.0829260647297 111.78307227790356 555.8659983426332\n","15: 434.18056175112724 109.08200255036354 543.2625643014908\n","16: 444.9705343991518 102.4355409219861 547.4060753211379\n","17: 410.9658392071724 101.3203414157033 512.2861806228757\n","18: 426.1505952477455 92.38351855427027 518.5341138020158\n","19: 420.3181689232588 91.3804422095418 511.6986111328006\n","20: 417.82542438805103 87.72005869448185 505.5454830825329\n","21: 419.0102724432945 85.08688819408417 504.0971606373787\n","22: 417.4302103072405 83.0114297196269 500.4416400268674\n","23: 410.9394699037075 82.51038757711649 493.449857480824\n","24: 418.85939352214336 79.18971931189299 498.04911283403635\n","25: 399.78064365684986 75.98192565143108 475.76256930828094\n","26: 415.07664255797863 76.22064442187548 491.2972869798541\n","27: 399.4215804338455 73.6984538435936 473.1200342774391\n","28: 398.0030883103609 72.81132963299751 470.8144179433584\n","29: 413.27357901632786 69.45884069427848 482.73241971060634\n","30: 406.3374813199043 67.23782635480165 473.575307674706\n","31: 403.3349729478359 67.38998895138502 470.72496189922094\n","32: 389.8910244628787 69.06925204396248 458.9602765068412\n","33: 394.0222415626049 66.87502302974463 460.89726459234953\n","34: 408.74338825047016 62.88354255259037 471.62693080306053\n","35: 393.65602507442236 62.779056418687105 456.43508149310946\n","36: 402.429080940783 61.37986607477069 463.8089470155537\n","37: 392.16775353997946 63.02572537213564 455.1934789121151\n","38: 395.4627212435007 58.155823703855276 453.618544947356\n","39: 390.47832833230495 64.72215092554688 455.20047925785184\n","40: 380.99531142413616 61.63847986608744 442.6337912902236\n","41: 392.91612546890974 58.293002888560295 451.20912835747004\n","42: 391.74716967344284 55.028642542660236 446.7758122161031\n","43: 389.6113396063447 54.58487572148442 444.1962153278291\n","44: 399.4249568134546 54.04332706332207 453.4682838767767\n","45: 391.18378000706434 54.20225318148732 445.38603318855166\n","46: 383.43227899074554 56.76225182414055 440.1945308148861\n","47: 380.8172088563442 54.56491230241954 435.38212115876377\n","48: 385.2998470067978 53.70350144803524 439.00334845483303\n","49: 378.0481934994459 52.46825813502073 430.51645163446665\n","50: 385.83011797070503 51.60639722645283 437.43651519715786\n","51: 374.0960928350687 53.480526726692915 427.5766195617616\n","52: 382.8296654596925 51.24231033027172 434.0719757899642\n","53: 384.04995622485876 51.025968875736 435.07592510059476\n","54: 382.5117882192135 49.025405971333385 431.53719419054687\n","55: 372.472703486681 49.74747357517481 422.2201770618558\n","56: 392.45820412784815 47.48128226026893 439.9394863881171\n","57: 381.8865530565381 48.36098415777087 430.247537214309\n","58: 379.9669980928302 48.89352114871144 428.8605192415416\n","59: 381.73583214730024 47.78726230747998 429.5230944547802\n","60: 372.5386344566941 47.95073815435171 420.48937261104584\n","61: 384.71506141126156 48.182411409914494 432.89747282117605\n","62: 374.90044133365154 49.34619100019336 424.2466323338449\n","63: 377.42444218322635 45.14594302512705 422.5703852083534\n","64: 367.752067245543 47.63001915067434 415.38208639621735\n","65: 389.24816247075796 45.69516732171178 434.94332979246974\n","66: 368.31266579031944 44.809197410941124 413.12186320126057\n","67: 375.460216447711 46.849444700405 422.309661148116\n","68: 368.936240516603 44.424371398985386 413.3606119155884\n","69: 389.9953376054764 44.15883110463619 434.1541687101126\n","70: 377.00926414877176 45.16686347872019 422.17612762749195\n","71: 367.2872476801276 44.17124607414007 411.4584937542677\n","72: 363.1394656226039 44.56697459332645 407.70644021593034\n","73: 362.62300277873874 43.838472846895456 406.4614756256342\n","74: 379.34152717888355 44.21492971479893 423.5564568936825\n","75: 367.83678286522627 42.68739677593112 410.5241796411574\n","76: 376.5587247386575 42.14580967463553 418.704534413293\n","77: 374.62802447751164 40.89038918726146 415.5184136647731\n","78: 366.8801688402891 42.7999543081969 409.680123148486\n","79: 374.52098041027784 40.02015770133585 414.5411381116137\n","80: 377.1930615603924 42.454266933724284 419.64732849411666\n","81: 372.92556311935186 40.646674674004316 413.5722377933562\n","82: 369.3537510037422 42.35554981045425 411.70930081419647\n","83: 365.81913017481565 39.03142032958567 404.8505505044013\n","84: 365.96147056668997 40.756643859669566 406.71811442635953\n","85: 374.7574447467923 39.90187036432326 414.6593151111156\n","86: 379.10332277417183 40.54116887971759 419.6444916538894\n","87: 368.1855867728591 40.4392093103379 408.624796083197\n","88: 376.55534990131855 39.997672870755196 416.55302277207375\n","89: 370.68522722274065 40.13799159415066 410.8232188168913\n","90: 376.9778616465628 39.52093272097409 416.4987943675369\n","91: 369.08696400374174 40.56883647851646 409.6558004822582\n","92: 365.64207097887993 39.77560898475349 405.4176799636334\n","93: 371.9966740719974 39.36878183670342 411.3654559087008\n","94: 359.5497180297971 39.97657306678593 399.526291096583\n","95: 378.1557159945369 37.82890642620623 415.9846224207431\n","96: 359.9443696588278 39.258787635713816 399.2031572945416\n","97: 360.90942295640707 40.10652466118336 401.0159476175904\n","98: 359.9216618090868 38.61441358923912 398.5360753983259\n","99: 354.12342604249716 39.74834010563791 393.87176614813507\n","100: 377.39357920736074 39.495871499180794 416.88945070654154\n","101: 373.374617818743 36.757276308722794 410.1318941274658\n","102: 359.3301764577627 38.145166288129985 397.4753427458927\n","103: 366.3533179163933 38.562245927751064 404.91556384414434\n","104: 366.1951485797763 37.131452601403 403.3266011811793\n","105: 358.17886224389076 38.795749861747026 396.9746121056378\n","106: 371.2932336181402 37.46629232913256 408.7595259472728\n","107: 366.85722706466913 38.38774753920734 405.24497460387647\n","108: 369.6732366606593 36.40102415904403 406.07426081970334\n","109: 364.42993760854006 37.121651493012905 401.55158910155296\n","110: 359.78097745776176 38.67629706673324 398.457274524495\n","111: 366.15351644158363 36.71159991249442 402.86511635407805\n","112: 366.3807463571429 36.61719238385558 402.9979387409985\n","113: 364.7059525921941 38.680563723668456 403.38651631586254\n","114: 360.6398135200143 39.686389323323965 400.32620284333825\n","115: 371.4437594562769 38.70399554260075 410.14775499887764\n","116: 361.9645717702806 36.47876911237836 398.44334088265896\n","117: 364.59966792166233 34.52355303429067 399.123220955953\n","118: 377.14837273955345 33.70624711550772 410.8546198550612\n","119: 355.1259936541319 37.17879748530686 392.30479113943875\n","120: 358.3542220890522 36.48798878211528 394.8422108711675\n","121: 363.3898931518197 36.21631637215614 399.60620952397585\n","122: 371.3166743516922 36.63852022588253 407.95519457757473\n","123: 367.43124824762344 35.09454070776701 402.52578895539045\n","124: 358.30472399294376 36.083894193172455 394.3886181861162\n","125: 360.91021862626076 36.17303258925676 397.0832512155175\n","126: 376.4043228328228 35.844659466296434 412.24898229911923\n","127: 361.0990225896239 34.856998816132545 395.9560214057565\n","128: 370.6431562155485 38.039754170924425 408.68291038647294\n","129: 360.96002903580666 35.527192167937756 396.4872212037444\n","130: 345.4871034100652 35.32288599014282 380.809989400208\n","131: 374.85348182171583 35.594100828282535 410.44758264999837\n","132: 358.1725715994835 36.73043763265014 394.9030092321336\n","133: 360.37459191679955 37.623515418730676 397.9981073355302\n","134: 365.81165101379156 33.16643722355366 398.9780882373452\n","135: 358.500903904438 34.13180811051279 392.6327120149508\n","136: 360.3972805403173 33.22855736780912 393.6258379081264\n","137: 348.97002109885216 35.36141306441277 384.33143416326493\n","138: 347.5155647993088 35.39263729006052 382.9082020893693\n","139: 363.2558547258377 34.07126010209322 397.3271148279309\n","140: 353.3697817400098 35.46635093539953 388.8361326754093\n","141: 364.61557526886463 35.00089596025646 399.6164712291211\n","142: 359.31511709839106 33.15059301815927 392.4657101165503\n","143: 355.0020016133785 32.6318859225139 387.6338875358924\n","144: 352.0334843546152 35.12975401058793 387.16323836520314\n","145: 355.7714699804783 35.421365328133106 391.1928353086114\n","146: 374.5385972969234 33.95169228129089 408.4902895782143\n","147: 368.2760755941272 34.715820700861514 402.9918962949887\n","148: 360.41002556681633 36.841670433059335 397.25169599987566\n","149: 355.6650305688381 33.930844727903605 389.5958752967417\n","150: 367.78606247901917 34.40594204887748 402.19200452789664\n","151: 360.26234313100576 35.36560656502843 395.6279496960342\n","152: 368.6449084728956 33.89051425270736 402.535422725603\n","153: 353.7093680277467 36.321349604055285 390.03071763180196\n","154: 361.07303841784596 35.34582031145692 396.4188587293029\n","155: 350.74602872505784 33.68153666052967 384.4275653855875\n","156: 360.53750771284103 34.579564173705876 395.1170718865469\n","157: 361.6103949137032 33.084731893613935 394.69512680731714\n","158: 360.75511699169874 34.252380698919296 395.00749769061804\n","159: 363.8575013950467 31.73101499117911 395.5885163862258\n","160: 351.7099451981485 34.34700074605644 386.0569459442049\n","161: 349.8570541739464 32.460468142293394 382.3175223162398\n","162: 369.060853689909 32.818969521671534 401.8798232115805\n","163: 360.3296996317804 32.90364572033286 393.23334535211325\n","164: 352.8874379582703 36.27609940432012 389.16353736259043\n","165: 357.23912739008665 34.57071001641452 391.8098374065012\n","166: 365.2183116003871 32.51214641332626 397.73045801371336\n","167: 357.9997412264347 31.916255986317992 389.9159972127527\n","168: 356.61730609834194 32.95018207747489 389.56748817581683\n","169: 355.2514805532992 32.781913332641125 388.0333938859403\n","170: 366.0291211977601 34.36777972429991 400.39690092206\n","171: 356.58286468684673 34.43978414312005 391.0226488299668\n","172: 347.6203316524625 32.66651551425457 380.28684716671705\n","173: 339.1483262628317 32.758252636529505 371.9065788993612\n","174: 363.53852582350373 32.58837638515979 396.1269022086635\n","175: 358.7904736325145 34.010196106508374 392.80066973902285\n","176: 352.38330951333046 32.52892867941409 384.91223819274455\n","177: 356.68721786513925 32.16549961082637 388.8527174759656\n","178: 367.8186128884554 31.75451397895813 399.5731268674135\n","179: 357.148521438241 31.38776270393282 388.5362841421738\n","180: 355.19401809200644 30.813794632442296 386.00781272444874\n","181: 367.316157117486 31.326457713730633 398.64261483121663\n","182: 361.34880720824003 32.51594749838114 393.86475470662117\n","183: 362.33650451898575 32.527806248515844 394.8643107675016\n","184: 347.87336434423923 36.76123473793268 384.6345990821719\n","185: 362.76797038316727 29.268903298303485 392.03687368147075\n","186: 356.73233215510845 32.142072414048016 388.87440456915647\n","187: 355.33451114594936 32.606956388801336 387.9414675347507\n","188: 342.9684637412429 33.08007080759853 376.0485345488414\n","189: 357.421723715961 31.476369619369507 388.8980933353305\n","190: 361.68526165932417 32.80951393023133 394.4947755895555\n","191: 351.0342362560332 32.02757599577308 383.06181225180626\n","192: 352.4052013903856 33.664053939282894 386.0692553296685\n","193: 354.841888025403 31.944509117864072 386.7863971432671\n","194: 344.9690280929208 33.68560190778226 378.65463000070304\n","195: 359.27877090126276 34.31195400189608 393.59072490315884\n","196: 366.3297093808651 34.21634830534458 400.5460576862097\n","197: 372.47170482575893 32.79241899214685 405.2641238179058\n","198: 359.85547100380063 35.24592776969075 395.1013987734914\n","199: 357.82658982276917 31.522135756444186 389.34872557921335\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"frozen-damage","executionInfo":{"status":"ok","timestamp":1624731235684,"user_tz":-60,"elapsed":10,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_cifar10_augment')\n","# model = ResNet18()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_cifar10_augment'))"],"id":"frozen-damage","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"aboriginal-lafayette","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624731239885,"user_tz":-60,"elapsed":4210,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"18672152-fe10-4b0d-9bda-4b7143be5570"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"aboriginal-lafayette","execution_count":10,"outputs":[{"output_type":"stream","text":["0.9164\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"therapeutic-orlando","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624731268556,"user_tz":-60,"elapsed":28679,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"99138183-522d-4367-ea20-5cc714db5cf4"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"therapeutic-orlando","execution_count":11,"outputs":[{"output_type":"stream","text":["0.96676\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"commercial-heavy","executionInfo":{"status":"ok","timestamp":1624731268556,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"commercial-heavy","execution_count":11,"outputs":[]}]}
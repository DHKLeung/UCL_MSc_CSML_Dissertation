{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "romantic-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "silver-clear",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1438294f4d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration and Hyperparameters\n",
    "\"\"\"\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  # can omit\n",
    "    transforms.RandomHorizontalFlip(),  # can omit\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        (0.4914, 0.4822, 0.4465),\n",
    "        (0.2023, 0.1994, 0.2010)\n",
    "    )\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        (0.4914, 0.4822, 0.4465),\n",
    "        (0.2023, 0.1994, 0.2010)\n",
    "    )\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "step_size = 0.1\n",
    "random_seed = 0\n",
    "epochs = 200\n",
    "L2_decay = 1e-4\n",
    "alpha = 1.\n",
    "\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "relative-mobility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data\n",
    "\"\"\"\n",
    "train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "starting-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.__dict__['ResNet18']()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "contemporary-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_cifar10(inputs, labels, alpha):\n",
    "    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample()\n",
    "    batch_size = labels.size(0)\n",
    "    idx = torch.randperm(batch_size)\n",
    "    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n",
    "    labels_b = labels[idx]\n",
    "    return mixup_inputs, labels, labels_b, lmbda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "right-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n",
    "    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n",
    "    return mixup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "written-bookmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1046.9220373630524 1033.1639646291733 2080.086000919342\n",
      "1: 899.0625511407852 889.7913702726364 1788.85391664505\n",
      "2: 886.5929219722748 875.3143594264984 1761.907283782959\n",
      "3: 834.3424344062805 771.0263348817825 1605.3687677383423\n",
      "4: 812.0760315656662 736.9753563404083 1549.0513889789581\n",
      "5: 800.7928663492203 709.0497913360596 1509.8426570892334\n",
      "6: 774.7856444120407 668.9724642038345 1443.7581074237823\n",
      "7: 756.2956650257111 635.4324868917465 1391.728154182434\n",
      "8: 731.7959481477737 606.5924001932144 1338.3883481025696\n",
      "9: 726.9354506731033 579.6769610643387 1306.612412929535\n",
      "10: 709.7506102323532 545.1766381263733 1254.9272515773773\n",
      "11: 686.1230628490448 502.7082750797272 1188.8313403129578\n",
      "12: 667.2529697418213 462.5438187122345 1129.7967891693115\n",
      "13: 661.6548303365707 429.3269785642624 1090.981811761856\n",
      "14: 632.4234305024147 395.2798002958298 1027.7032333612442\n",
      "15: 609.4051832556725 361.0895655155182 970.4947484731674\n",
      "16: 586.1013903021812 328.14155757427216 914.2429468631744\n",
      "17: 578.5154417157173 303.6094520688057 882.124893784523\n",
      "18: 556.3307791352272 278.9149169623852 835.2456945180893\n",
      "19: 568.8279420137405 263.5142995119095 832.3422408103943\n",
      "20: 559.3208318352699 248.55220088362694 807.8730347156525\n",
      "21: 553.9973233938217 236.112220287323 790.1095440983772\n",
      "22: 527.3432337939739 217.53575375676155 744.8789881467819\n",
      "23: 524.2419490218163 207.2542938888073 731.4962437748909\n",
      "24: 515.9039911329746 194.0497024357319 709.9536930918694\n",
      "25: 505.5695931017399 192.1674638390541 697.737056016922\n",
      "26: 503.91468116641045 178.00991544127464 681.9245949983597\n",
      "27: 498.14568611979485 170.5147844105959 668.6604697108269\n",
      "28: 491.33054053783417 163.92314213514328 655.2536814212799\n",
      "29: 492.7637506723404 157.6092128008604 650.372964322567\n",
      "30: 493.0614425987005 150.41758674383163 643.4790297150612\n",
      "31: 505.7812032401562 146.7557032108307 652.5369069576263\n",
      "32: 488.149043828249 144.7470918595791 632.896134853363\n",
      "33: 485.21599957346916 135.69082662463188 620.9068264961243\n",
      "34: 477.9263935983181 132.13722363114357 610.0636177062988\n",
      "35: 475.0817767828703 123.4990806132555 598.5808571577072\n",
      "36: 473.14744190871716 121.03812849521637 594.1855711936951\n",
      "37: 467.6125050187111 116.62401351332664 584.2365186810493\n",
      "38: 472.15550054609776 111.66447784006596 583.8199782967567\n",
      "39: 465.94293081760406 109.60741602629423 575.5503471493721\n",
      "40: 469.0007067322731 114.2839682996273 583.2846745848656\n",
      "41: 461.9185233861208 104.08635064959526 566.0048740506172\n",
      "42: 455.16902220249176 101.27737584710121 556.4463982582092\n",
      "43: 438.3331897854805 98.53121877461672 536.8644099831581\n",
      "44: 451.07904912531376 95.69465282559395 546.7737024128437\n",
      "45: 454.0253238081932 91.52824752777815 545.5535710453987\n",
      "46: 455.8952512294054 88.89159597456455 544.7868469953537\n",
      "47: 441.4074295759201 86.91750804334879 528.3249376118183\n",
      "48: 458.56292758882046 83.91883715242147 542.481764703989\n",
      "49: 447.4487618505955 83.6282363012433 531.0769979655743\n",
      "50: 434.28786370158195 81.01704173535109 515.304904282093\n",
      "51: 449.25380462408066 81.05001586675644 530.3038206994534\n",
      "52: 458.9203888773918 75.7863804847002 534.7067697048187\n",
      "53: 429.35613180696964 77.1311104670167 506.4872415959835\n",
      "54: 422.1339050233364 76.0273164883256 498.16122049093246\n",
      "55: 448.3787551522255 72.47289667278528 520.8516514599323\n",
      "56: 439.76925653219223 69.46399950608611 509.2332558333874\n",
      "57: 435.693348094821 69.74412362277508 505.43747276067734\n",
      "58: 442.27075839042664 67.43727283552289 509.7080312371254\n",
      "59: 433.2173003256321 67.63044913113117 500.8477492928505\n",
      "60: 445.26394633948803 65.9180055335164 511.18195167183876\n",
      "61: 436.2131353318691 62.74630508571863 498.9594405144453\n",
      "62: 430.0327287390828 63.558618541806936 493.5913470983505\n",
      "63: 419.4928796067834 64.69330945611 484.1861892938614\n",
      "64: 425.9315744638443 60.48071073368192 486.41228526830673\n",
      "65: 423.1486173123121 59.55481378734112 482.7034310400486\n",
      "66: 441.71249260008335 57.41741872578859 499.1299121975899\n",
      "67: 431.90727746486664 57.563720770180225 489.47099858522415\n",
      "68: 423.3664590343833 58.554867926985025 481.9213262796402\n",
      "69: 430.90447810292244 56.7549485526979 487.6594266295433\n",
      "70: 433.9538267105818 58.677623603492975 492.631450176239\n",
      "71: 424.20504772663116 56.18376176059246 480.38880866765976\n",
      "72: 425.9863191097975 57.92185791954398 483.90817607939243\n",
      "73: 422.7880067527294 54.032378021627665 476.8203847631812\n",
      "74: 422.8697012066841 53.41146766021848 476.2811690568924\n",
      "75: 417.6922671496868 53.50969937071204 471.201967343688\n",
      "76: 426.8583953306079 52.44249504990876 479.30089020729065\n",
      "77: 426.6465038433671 50.34260779619217 476.9891108125448\n",
      "78: 419.5942593961954 49.148599207401276 468.7428576052189\n",
      "79: 406.7650199905038 49.16849720850587 455.9335153847933\n",
      "80: 423.62497368454933 48.10579043440521 471.7307638376951\n",
      "81: 414.5869876742363 49.576967407017946 464.16395503282547\n",
      "82: 416.4803459569812 48.50553381629288 464.9858794361353\n",
      "83: 418.4059856981039 48.705963384360075 467.11194908618927\n",
      "84: 415.2487267330289 50.35368474945426 465.60241198539734\n",
      "85: 406.14517087489367 49.01225013844669 455.15742091834545\n",
      "86: 414.0467475503683 47.04402421414852 461.0907710492611\n",
      "87: 420.0417031943798 42.778143145143986 462.8198466747999\n",
      "88: 439.5184620283544 46.104521717876196 485.62298369407654\n",
      "89: 410.8586247563362 43.76834254898131 454.62696696817875\n",
      "90: 419.77804739773273 44.998148238286376 464.7761954367161\n",
      "91: 407.6454782560468 43.362121956422925 451.00760097801685\n",
      "92: 413.08588832616806 45.09765816107392 458.18354676663876\n",
      "93: 403.2503654435277 44.663596890866756 447.9139621257782\n",
      "94: 427.22060960531235 43.85212248377502 471.0727306380868\n",
      "95: 403.85596039146185 41.29380061104894 445.1497615426779\n",
      "96: 422.2844667285681 41.7437177747488 464.02818474173546\n",
      "97: 417.500768058002 41.906593915075064 459.4073609262705\n",
      "98: 416.75169663876295 40.671625163406134 457.42332096397877\n",
      "99: 404.5385712608695 41.088603895157576 445.6271734535694\n",
      "100: 408.3790055066347 40.18513626419008 448.5641418248415\n",
      "101: 411.672799564898 42.717392400838435 454.3901928216219\n",
      "102: 421.7078194320202 39.69988143630326 461.40770050883293\n",
      "103: 397.8209591731429 39.48062374815345 437.30158354341984\n",
      "104: 405.7239629328251 41.24388546682894 446.9678485542536\n",
      "105: 400.27953520417213 39.27748951688409 439.5570240467787\n",
      "106: 400.6722379550338 39.900222009047866 440.57246029376984\n",
      "107: 426.88067827001214 36.70477434433997 463.5854530185461\n",
      "108: 415.7132141068578 43.00127004832029 458.71448424458504\n",
      "109: 418.7686564773321 40.37882063537836 459.14747685194016\n",
      "110: 404.6224486492574 40.260458620265126 444.8829074651003\n",
      "111: 398.45796497911215 38.397147924639285 436.8551119565964\n",
      "112: 391.99928966909647 41.25784603971988 433.2571365535259\n",
      "113: 412.6565270498395 37.320582242682576 449.9771096408367\n",
      "114: 393.65412009879947 36.89871947094798 430.55283918976784\n",
      "115: 404.3528361096978 37.38852689228952 441.7413626462221\n",
      "116: 395.20265086740255 39.69326727092266 434.8959182500839\n",
      "117: 407.69115027040243 35.895242189988494 443.58639247715473\n",
      "118: 400.38535214215517 35.131491938605905 435.5168437361717\n",
      "119: 402.22250440716743 38.02211179584265 440.2446163892746\n",
      "120: 402.29069789126515 35.57041755318642 437.86111584305763\n",
      "121: 408.60635232925415 35.045333000831306 443.6516856998205\n",
      "122: 405.72223464399576 34.602154571563005 440.3243897408247\n",
      "123: 417.84495701640844 33.93766728788614 451.78262481093407\n",
      "124: 392.768094599247 34.82492618821561 427.5930196940899\n",
      "125: 410.37680903822184 36.77627380378544 447.1530834212899\n",
      "126: 404.5633128285408 34.11070077307522 438.6740140840411\n",
      "127: 386.63257621973753 34.70070994924754 421.333285599947\n",
      "128: 397.4921623840928 37.01033553853631 434.50249718129635\n",
      "129: 388.2727484665811 36.23180315177888 424.50455191731453\n",
      "130: 408.8520628735423 32.60985954850912 441.4619220048189\n",
      "131: 401.34405659139156 36.92098324932158 438.26503916084766\n",
      "132: 393.59179627895355 33.16818408109248 426.75998018682003\n",
      "133: 395.1425334736705 33.26762361545116 428.4101574420929\n",
      "134: 411.26080076768994 31.83425048366189 443.0950512960553\n",
      "135: 399.5405358374119 31.992265919223428 431.53280092030764\n",
      "136: 398.1261733919382 34.284529716707766 432.41070372611284\n",
      "137: 389.32259487360716 35.93158212304115 425.2541770488024\n",
      "138: 394.6266064159572 31.335014125332236 425.961620926857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139: 400.83112689107656 36.14966180920601 436.9807878732681\n",
      "140: 393.3728911951184 31.382340705022216 424.7552315965295\n",
      "141: 396.978108279407 32.6730083366856 429.6511160880327\n",
      "142: 403.46090932935476 31.491242913529277 434.9521514028311\n",
      "143: 415.3072191141546 29.15646169986576 444.463681653142\n",
      "144: 393.80555418878794 32.60920046363026 426.4147554785013\n",
      "145: 394.51234447211027 31.394308095797896 425.9066520780325\n",
      "146: 403.4901583790779 29.589088609442115 433.07924646139145\n",
      "147: 398.0671839043498 33.24901222810149 431.3161955177784\n",
      "148: 399.2701533064246 33.7746922057122 433.0448445677757\n",
      "149: 391.10661690309644 30.530025801621377 421.6366432160139\n",
      "150: 390.6029792651534 32.49053769744933 423.09351728856564\n",
      "151: 406.7510558851063 29.99967616610229 436.75073243677616\n",
      "152: 385.8974779769778 32.325360112823546 418.22283759713173\n",
      "153: 393.64088344573975 30.050944171845913 423.69182761758566\n",
      "154: 402.7366585060954 31.421455931849778 434.1581141650677\n",
      "155: 397.35986759513617 33.389322243630886 430.74919002503157\n",
      "156: 402.25027786940336 29.662548111751676 431.91282491385937\n",
      "157: 402.56468093395233 30.220806622877717 432.78548757731915\n",
      "158: 400.21731845662 30.492872302420437 430.7101902887225\n",
      "159: 393.2191224824637 31.425356057472527 424.6444788351655\n",
      "160: 395.0768338255584 30.873032878153026 425.94986671954393\n",
      "161: 400.40781596675515 29.02622244041413 429.434037566185\n",
      "162: 391.6895276084542 31.42578167747706 423.1153100878\n",
      "163: 401.9699009619653 33.14707190915942 435.11697260290384\n",
      "164: 379.2964980751276 32.69450665730983 411.9910051226616\n",
      "165: 396.3467838987708 32.59825438261032 428.94503881037235\n",
      "166: 382.5743304863572 29.67401178041473 412.24834352731705\n",
      "167: 396.33703342825174 31.107081793248653 427.44411538541317\n",
      "168: 388.76056611537933 30.4782408233732 419.23880682885647\n",
      "169: 385.63857405632734 30.874708666466177 416.51328290998936\n",
      "170: 392.42075337842107 30.752171797677875 423.17292423546314\n",
      "171: 397.30341076105833 27.369676847942173 424.6730883717537\n",
      "172: 396.989333614707 28.671457921154797 425.6607910245657\n",
      "173: 404.0695289596915 28.752443238161504 432.82197247445583\n",
      "174: 395.6195644624531 27.70900527201593 423.32856991142035\n",
      "175: 404.1489152163267 29.723524135537446 433.8724393248558\n",
      "176: 390.50870606675744 30.631075126118958 421.1397821903229\n",
      "177: 392.96412809193134 31.32612261362374 424.2902507632971\n",
      "178: 386.3028531819582 28.633432996459305 414.9362854883075\n",
      "179: 393.4606682807207 29.522623335011303 422.9832915440202\n",
      "180: 385.1955613605678 30.50877227075398 415.704334102571\n",
      "181: 388.80042258650064 27.78928178269416 416.5897048190236\n",
      "182: 392.42781153321266 29.676403857767582 422.10421454161406\n",
      "183: 394.5611498951912 30.717560683377087 425.2787106707692\n",
      "184: 390.1305965296924 27.404416032135487 417.53501285612583\n",
      "185: 400.0177456513047 29.056598695460707 429.07434471696615\n",
      "186: 387.02359575405717 27.87929594796151 414.90289149433374\n",
      "187: 391.1972566731274 27.325390055775642 418.5226473622024\n",
      "188: 399.3826772123575 29.32402601838112 428.7067032456398\n",
      "189: 387.5983817651868 28.211096030659974 415.80947656184435\n",
      "190: 396.10270876437426 28.37627262622118 424.47898209095\n",
      "191: 390.8748961985111 28.955097236670554 419.82999347150326\n",
      "192: 388.48688815534115 27.230338958092034 415.7172272205353\n",
      "193: 378.0521725229919 28.88646644167602 406.9386383071542\n",
      "194: 387.17635218426585 27.0841778004542 414.2605308368802\n",
      "195: 388.48242277279496 29.61667083017528 418.0990942120552\n",
      "196: 382.39195446670055 29.431100266985595 411.8230549618602\n",
      "197: 380.33519353345037 28.73015351779759 409.06534791737795\n",
      "198: 379.19688188284636 27.321832072921097 406.51871405541897\n",
      "199: 386.71796710044146 29.58113578055054 416.299102678895\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training\n",
    "\"\"\"\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.\n",
    "    epoch_mixup_loss = 0.\n",
    "    epoch_org_loss = 0.\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        mixup_inputs, labels, labels_b, lmbda = mixup_cifar10(inputs, labels, alpha)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mixup_inputs)\n",
    "        mixup_loss = mixup_criterion(criterion, outputs, labels, labels_b, lmbda)\n",
    "        \n",
    "        ##\n",
    "        outputs_org = model(inputs)\n",
    "        loss_org = criterion(outputs_org, labels)\n",
    "        total_loss = mixup_loss + loss_org\n",
    "        \n",
    "        epoch_mixup_loss += mixup_loss.item()\n",
    "        epoch_org_loss += loss_org.item()\n",
    "        \n",
    "        epoch_loss += total_loss.item()\n",
    "        total_loss.backward()\n",
    "        ##\n",
    "        \n",
    "        optimizer.step()\n",
    "    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_org_loss, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "frozen-damage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), './mixup_model_pytorch_cifar10_augment')\n",
    "model = models.__dict__['ResNet18']()\n",
    "model.load_state_dict(torch.load('./mixup_model_pytorch_cifar10_augment'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aboriginal-lafayette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        outputs = model(inputs)\n",
    "        _, predicts = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicts == labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "therapeutic-orlando",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97558\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        outputs = model(inputs)\n",
    "        _, predicts = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicts == labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-heavy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

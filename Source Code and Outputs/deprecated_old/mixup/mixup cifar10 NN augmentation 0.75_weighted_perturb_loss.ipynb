{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup cifar10 NN augmentation 0.75_weighted_perturb_loss.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"romantic-purchase","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624720392036,"user_tz":-60,"elapsed":37083,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"8017e3d8-564c-4a87-bb32-3e0d95c7f3a7"},"source":["import torch\n","from torchvision import transforms, datasets\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"romantic-purchase","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"__nFHYcCoEzb","executionInfo":{"status":"ok","timestamp":1624720392451,"user_tz":-60,"elapsed":422,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"__nFHYcCoEzb","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silver-clear","executionInfo":{"status":"ok","timestamp":1624720392452,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"541a8011-f921-4f8d-b99d-1e77c00a8a99"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),  # can omit\n","    transforms.RandomHorizontalFlip(),  # can omit\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","batch_size = 128\n","step_size = 0.1\n","random_seed = 0\n","epochs = 200\n","L2_decay = 1e-4\n","alpha = 1.\n","perturb_loss_weight = 0.75\n","\n","torch.manual_seed(random_seed)"],"id":"silver-clear","execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fc16277cab0>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"relative-mobility","executionInfo":{"status":"ok","timestamp":1624720412291,"user_tz":-60,"elapsed":19842,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"7d1f6314-7973-4fc9-f428-dd67b6849454"},"source":["\"\"\"\n","Data\n","\"\"\"\n","train_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=True, download=True, transform=transform_train)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=False, download=True, transform=transform_test)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"relative-mobility","execution_count":4,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"starting-chancellor","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624720418754,"user_tz":-60,"elapsed":6464,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"059b453e-2d0d-44ed-b248-df97f10e10e1"},"source":["model = ResNet18()\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","model.cuda()"],"id":"starting-chancellor","execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"contemporary-gross","executionInfo":{"status":"ok","timestamp":1624720418755,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_cifar10(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    batch_size = labels.size(0)\n","    idx = torch.randperm(batch_size).to('cuda')\n","    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n","    labels_b = labels[idx]\n","    return mixup_inputs, labels, labels_b, lmbda"],"id":"contemporary-gross","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"right-spending","executionInfo":{"status":"ok","timestamp":1624720418755,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"right-spending","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"written-bookmark","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624731172783,"user_tz":-60,"elapsed":10754031,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"a7170c69-77b6-4d62-b943-95813d3e6d1f"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_loss = 0.\n","    epoch_mixup_loss = 0.\n","    epoch_org_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        mixup_inputs, labels, labels_b, lmbda = mixup_cifar10(inputs, labels, alpha)\n","        optimizer.zero_grad()\n","        outputs = model(mixup_inputs)\n","        mixup_loss = mixup_criterion(criterion, outputs, labels, labels_b, lmbda)\n","        \n","        ##\n","        outputs_org = model(inputs)\n","        loss_org = criterion(outputs_org, labels)\n","        weighted_total_loss = mixup_loss * perturb_loss_weight + loss_org * (1 - perturb_loss_weight)\n","        \n","        epoch_mixup_loss += mixup_loss.item()\n","        epoch_org_loss += loss_org.item()\n","        \n","        epoch_loss += (mixup_loss.item() + loss_org.item())\n","        \n","        weighted_total_loss.backward()\n","        ##\n","        \n","        optimizer.step()\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_org_loss, epoch_loss))"],"id":"written-bookmark","execution_count":8,"outputs":[{"output_type":"stream","text":["0: 764.7630363702774 646.7030034065247 1411.466039776802\n","1: 640.2440081238747 447.9709674715996 1088.2149755954742\n","2: 582.5468072295189 341.3630031943321 923.909810423851\n","3: 549.2316036820412 279.86023139953613 829.0918350815773\n","4: 526.8756674528122 230.05745747685432 756.9331249296665\n","5: 505.17997738718987 198.2179128229618 703.3978902101517\n","6: 483.42863616347313 179.18702167272568 662.6156578361988\n","7: 484.05935606360435 160.6871270686388 644.7464831322432\n","8: 462.6598149240017 146.03037589788437 608.6901908218861\n","9: 448.224804520607 135.66431415081024 583.8891186714172\n","10: 459.23446077108383 123.6601395457983 582.8946003168821\n","11: 459.82359781861305 114.40295046567917 574.2265482842922\n","12: 443.53461073338985 107.70927065610886 551.2438813894987\n","13: 430.2752748578787 102.81162346154451 533.0868983194232\n","14: 442.381454333663 94.83912242949009 537.2205767631531\n","15: 430.39604490995407 89.86045753210783 520.2565024420619\n","16: 441.29514299333096 83.85194374248385 525.1470867358148\n","17: 406.4052177518606 83.80712094902992 490.21233870089054\n","18: 424.5456410944462 78.90071968734264 503.4463607817888\n","19: 417.5055876225233 75.06051462888718 492.5661022514105\n","20: 415.6541749984026 71.62525171786547 487.27942671626806\n","21: 416.31492944061756 69.30451796203852 485.6194474026561\n","22: 418.12922180444 69.66467610746622 487.79389791190624\n","23: 408.6940450966358 65.63209654763341 474.3261416442692\n","24: 417.22065468132496 63.03889040648937 480.25954508781433\n","25: 399.21246396005154 61.67013793811202 460.88260189816356\n","26: 413.0460788011551 59.02626804634929 472.0723468475044\n","27: 398.35418101400137 57.792059041559696 456.14624005556107\n","28: 397.1005494296551 58.377750501036644 455.4782999306917\n","29: 412.9517197608948 53.98146890103817 466.93318866193295\n","30: 406.0582359135151 53.8939786888659 459.952214602381\n","31: 403.24595092236996 54.07655137963593 457.3225023020059\n","32: 389.00215135514736 54.6960971839726 443.69824853911996\n","33: 392.4497145265341 52.59729655086994 445.047011077404\n","34: 408.4503816217184 48.690299078822136 457.14068070054054\n","35: 393.62936360388994 49.04883902519941 442.67820262908936\n","36: 402.92060232907534 47.007957296445966 449.9285596255213\n","37: 391.3982635997236 48.38169568218291 439.7799592819065\n","38: 397.8252967298031 48.13921656087041 445.9645132906735\n","39: 387.53181678056717 47.09821601584554 434.6300327964127\n","40: 380.37525341659784 47.84022341109812 428.21547682769597\n","41: 393.82153932005167 44.46692373417318 438.28846305422485\n","42: 392.8099839538336 43.327556962147355 436.13754091598094\n","43: 390.2457859516144 42.672687985002995 432.9184739366174\n","44: 400.24348729103804 40.731563398614526 440.97505068965256\n","45: 390.88181441277266 41.37472637929022 432.2565407920629\n","46: 384.01330925524235 43.23563629668206 427.2489455519244\n","47: 381.925876699388 42.54366694018245 424.4695436395705\n","48: 384.86323185265064 40.80370971374214 425.6669415663928\n","49: 380.07689779251814 43.51219359971583 423.58909139223397\n","50: 386.93029817938805 39.08434087783098 426.01463905721903\n","51: 374.19161059707403 41.41042987816036 415.6020404752344\n","52: 383.9801306463778 38.50389640033245 422.48402704671025\n","53: 382.8601416796446 37.37710012122989 420.2372418008745\n","54: 383.8792539983988 37.32662049308419 421.205874491483\n","55: 374.09814443439245 37.40462916903198 411.50277360342443\n","56: 394.50308868288994 36.066528379917145 430.5696170628071\n","57: 384.00455193966627 36.64971791766584 420.6542698573321\n","58: 380.86937388032675 37.28680461831391 418.15617849864066\n","59: 382.10399018600583 38.480256758630276 420.5842469446361\n","60: 374.8021694086492 37.96065742801875 412.76282683666795\n","61: 385.4437111541629 36.92959070391953 422.3733018580824\n","62: 375.89931804686785 38.298665599897504 414.19798364676535\n","63: 378.2705245837569 34.75071866065264 413.02124324440956\n","64: 370.75983740761876 36.30988980084658 407.06972720846534\n","65: 389.5860692076385 35.066411884501576 424.6524810921401\n","66: 371.1130191385746 34.69193810131401 405.8049572398886\n","67: 377.37823339924216 35.40165155660361 412.7798849558458\n","68: 370.8021661005914 34.42819610238075 405.2303622029722\n","69: 391.26498125493526 33.679478915408254 424.9444601703435\n","70: 379.54816292226315 34.91675469093025 414.4649176131934\n","71: 368.0632619038224 32.94216790795326 401.0054298117757\n","72: 365.2141899392009 35.23305615223944 400.4472460914403\n","73: 363.94407579675317 35.56578451488167 399.50986031163484\n","74: 380.5950560644269 32.72538804914802 413.3204441135749\n","75: 369.1904628574848 32.489257155917585 401.6797200134024\n","76: 378.4300649985671 32.77201824449003 411.20208324305713\n","77: 377.17872646450996 32.53520046360791 409.71392692811787\n","78: 366.1887906715274 30.85622103139758 397.04501170292497\n","79: 376.5264195650816 29.969280661083758 406.49570022616535\n","80: 378.9516499340534 32.509414782747626 411.46106471680105\n","81: 374.71067690849304 31.531168822199106 406.24184573069215\n","82: 370.3514855802059 32.24963233806193 402.60111791826785\n","83: 367.6996112316847 30.439980456605554 398.13959168829024\n","84: 366.97095039114356 32.63036816846579 399.60131855960935\n","85: 376.1326190009713 30.780673695728183 406.9132926966995\n","86: 380.2566977441311 30.425379106774926 410.682076850906\n","87: 367.6648590564728 29.7768649822101 397.4417240386829\n","88: 377.878897331655 30.48790369508788 408.3668010267429\n","89: 372.097155418247 30.86825923062861 402.9654146488756\n","90: 379.2646288648248 30.752255239523947 410.0168841043487\n","91: 369.536602191627 30.94050174765289 400.4771039392799\n","92: 368.29862079024315 32.07294038962573 400.3715611798689\n","93: 374.5943844765425 30.70713320747018 405.30151768401265\n","94: 361.0989435389638 30.337899616919458 391.43684315588325\n","95: 380.75248469412327 29.84138525184244 410.5938699459657\n","96: 360.4250438027084 29.612504961900413 390.0375487646088\n","97: 363.65031850524247 31.293610281310976 394.94392878655344\n","98: 361.458928398788 31.32886752486229 392.78779592365026\n","99: 355.3425163961947 30.540717867203057 385.88323426339775\n","100: 378.4569329023361 29.574369145557284 408.0313020478934\n","101: 376.4145283252001 29.30183823686093 405.716366562061\n","102: 362.6134109944105 30.118824404664338 392.73223539907485\n","103: 367.497014939785 29.060463937930763 396.55747887771577\n","104: 366.79899080097675 27.8705035392195 394.66949434019625\n","105: 358.9471191391349 29.372462837025523 388.3195819761604\n","106: 373.51571921259165 29.395563259720802 402.91128247231245\n","107: 368.984552256763 29.93598483875394 398.9205370955169\n","108: 370.33013170957565 27.398133437149227 397.7282651467249\n","109: 365.54000705480576 28.88488729391247 394.4248943487182\n","110: 361.11264704167843 29.394697034731507 390.50734407640994\n","111: 368.2503237873316 28.7697133468464 397.020037134178\n","112: 367.87191040813923 28.934737915173173 396.8066483233124\n","113: 366.29896089434624 29.675763863138855 395.9747247574851\n","114: 361.6652782931924 30.192728630267084 391.85800692345947\n","115: 372.94717163592577 29.97260172944516 402.91977336537093\n","116: 363.99717669934034 28.645420738495886 392.64259743783623\n","117: 367.4184663966298 27.668928738683462 395.0873951353133\n","118: 378.87658224254847 26.454201862215996 405.33078410476446\n","119: 356.896581299603 29.695063835941255 386.59164513554424\n","120: 359.7782137989998 29.729325000662357 389.50753879966214\n","121: 364.9987668916583 28.55118485726416 393.54995174892247\n","122: 371.7550113350153 26.560459738131613 398.3154710731469\n","123: 368.70893137156963 26.630517879500985 395.3394492510706\n","124: 359.7281989008188 28.41847887635231 388.14667777717113\n","125: 360.348000632599 27.167699781246483 387.5157004138455\n","126: 380.31369322538376 29.06109485309571 409.37478807847947\n","127: 362.69589610397816 26.412583249155432 389.1084793531336\n","128: 370.65525068342686 27.580947974696755 398.2361986581236\n","129: 363.2487891279161 27.811770172789693 391.0605593007058\n","130: 349.5120036676526 29.989245031028986 379.5012486986816\n","131: 377.61727233976126 27.65581361297518 405.27308595273644\n","132: 359.8284464776516 27.444062689319253 387.27250916697085\n","133: 360.47179140523076 27.89689476089552 388.3686861661263\n","134: 368.5666057989001 27.109306998550892 395.675912797451\n","135: 360.47955463826656 26.66441380698234 387.1439684452489\n","136: 363.1230246722698 27.03095257561654 390.15397724788636\n","137: 350.26731111109257 27.828302646055818 378.0956137571484\n","138: 349.66351537778974 28.926071502268314 378.58958688005805\n","139: 366.3182805329561 28.04020811151713 394.35848864447325\n","140: 354.92861756682396 27.642959009855986 382.57157657667994\n","141: 366.17604745179415 26.94658239139244 393.1226298431866\n","142: 359.75449000671506 25.662218816578388 385.41670882329345\n","143: 358.8186763674021 28.61559659987688 387.43427296727896\n","144: 354.04239435493946 27.207272292114794 381.24966664705426\n","145: 355.38288137316704 26.305136986076832 381.68801835924387\n","146: 376.5287539064884 27.215906461235136 403.74466036772355\n","147: 367.922574095428 24.87197129242122 392.7945453878492\n","148: 360.05086124688387 27.771845694631338 387.8227069415152\n","149: 356.02727124094963 24.987032663542777 381.0143039044924\n","150: 368.7940342873335 27.549905406311154 396.34393969364464\n","151: 360.00575322657824 26.611990375909954 386.6177436024882\n","152: 368.7929526269436 25.615404853830114 394.4083574807737\n","153: 354.21894496679306 26.370127554051578 380.58907252084464\n","154: 360.312465056777 25.289689761120826 385.6021548178978\n","155: 352.04542750120163 26.945383202750236 378.99081070395187\n","156: 360.0033863801509 24.49328899383545 384.49667537398636\n","157: 366.4350274838507 27.944370427168906 394.3793979110196\n","158: 362.92706548050046 27.218894302845 390.14595978334546\n","159: 365.6672551333904 26.731389010325074 392.3986441437155\n","160: 350.920961253345 26.39118518680334 377.31214644014835\n","161: 352.2159940190613 27.131485962308943 379.34747998137027\n","162: 373.7430837675929 27.662498810328543 401.40558257792145\n","163: 362.4375708475709 26.44132885709405 388.87889970466495\n","164: 353.1772342324257 28.480418264865875 381.65765249729156\n","165: 359.395400069654 27.501739679370075 386.89713974902406\n","166: 366.3596934527159 23.412611661478877 389.77230511419475\n","167: 360.93353632837534 26.015648967120796 386.94918529549614\n","168: 357.80553841404617 26.35708054434508 384.16261895839125\n","169: 356.8652906343341 24.587129888124764 381.45242052245885\n","170: 368.4627196714282 26.160318081267178 394.6230377526954\n","171: 360.4572694376111 30.10672122752294 390.56399066513404\n","172: 349.2856982126832 25.789659513160586 375.0753577258438\n","173: 342.9007599130273 27.65960166975856 370.56036158278584\n","174: 365.1423030421138 26.012604624964297 391.1549076670781\n","175: 360.02556482702494 26.09666675236076 386.1222315793857\n","176: 353.8879156932235 25.18274004943669 379.07065574266016\n","177: 362.44415951892734 27.64875405887142 390.09291357779875\n","178: 368.88798811659217 23.330397601239383 392.21838571783155\n","179: 359.56111931055784 25.322766908444464 384.8838862190023\n","180: 356.1039877664298 24.329411984886974 380.43339975131676\n","181: 368.4523818194866 24.62297879345715 393.07536061294377\n","182: 363.5531707778573 25.746266666799784 389.2994374446571\n","183: 363.43780294060707 24.971609274856746 388.4094122154638\n","184: 347.7014936283231 27.724315578583628 375.4258092069067\n","185: 364.7876983433962 24.225768440403044 389.01346678379923\n","186: 359.4390341192484 25.746804222930223 385.1858383421786\n","187: 355.81606160104275 23.84752295189537 379.6635845529381\n","188: 343.7659796476364 26.392566967755556 370.15854661539197\n","189: 360.3078758455813 27.542325081303716 387.850200926885\n","190: 363.95268034935 23.688615861348808 387.6412962106988\n","191: 351.875602029264 24.418022353667766 376.29362438293174\n","192: 355.42105135321617 28.31759483460337 383.73864618781954\n","193: 355.8000434041023 25.372338491491973 381.1723818955943\n","194: 347.45554272830486 26.39856591448188 373.85410864278674\n","195: 358.990593098104 24.27003317978233 383.26062627788633\n","196: 365.40045788139105 23.52375454362482 388.92421242501587\n","197: 374.46288275532424 25.878420733381063 400.3413034887053\n","198: 360.68878531455994 28.103152440860868 388.7919377554208\n","199: 358.6146042086184 23.926804961171 382.5414091697894\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"frozen-damage","executionInfo":{"status":"ok","timestamp":1624731172784,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_cifar10_augment')\n","# model = ResNet18()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_cifar10_augment'))"],"id":"frozen-damage","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"aboriginal-lafayette","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624731176325,"user_tz":-60,"elapsed":3545,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"5e0a9baf-5e7f-49b7-b6b1-63f6bf010ff0"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"aboriginal-lafayette","execution_count":10,"outputs":[{"output_type":"stream","text":["0.9242\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"therapeutic-orlando","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624731197278,"user_tz":-60,"elapsed":20956,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"a4856da7-1134-4384-d4cc-d2806199a896"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"therapeutic-orlando","execution_count":11,"outputs":[{"output_type":"stream","text":["0.976\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"commercial-heavy","executionInfo":{"status":"ok","timestamp":1624731197278,"user_tz":-60,"elapsed":10,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"commercial-heavy","execution_count":11,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup cifar10 NN augmentation 0.99_weighted_perturb_loss.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"romantic-purchase","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624720399875,"user_tz":-60,"elapsed":44490,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"b775ba47-37bc-4b19-99e2-d9699449ed72"},"source":["import torch\n","from torchvision import transforms, datasets\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"romantic-purchase","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"__nFHYcCoEzb","executionInfo":{"status":"ok","timestamp":1624720400329,"user_tz":-60,"elapsed":456,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"__nFHYcCoEzb","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silver-clear","executionInfo":{"status":"ok","timestamp":1624720400329,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"15b6eac9-905b-4e2b-c7cb-3feaea96b3d6"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),  # can omit\n","    transforms.RandomHorizontalFlip(),  # can omit\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","batch_size = 128\n","step_size = 0.1\n","random_seed = 0\n","epochs = 200\n","L2_decay = 1e-4\n","alpha = 1.\n","perturb_loss_weight = 0.99\n","\n","torch.manual_seed(random_seed)"],"id":"silver-clear","execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fb445e4aa90>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"relative-mobility","executionInfo":{"status":"ok","timestamp":1624720411657,"user_tz":-60,"elapsed":11331,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"37f050ae-c18f-4fef-cf87-981c7c499e5f"},"source":["\"\"\"\n","Data\n","\"\"\"\n","train_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=True, download=True, transform=transform_train)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=False, download=True, transform=transform_test)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"relative-mobility","execution_count":4,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"starting-chancellor","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624720418048,"user_tz":-60,"elapsed":6400,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"6a5d2027-7544-46d8-d8a5-d88ff7b4c6a4"},"source":["model = ResNet18()\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","model.cuda()"],"id":"starting-chancellor","execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"contemporary-gross","executionInfo":{"status":"ok","timestamp":1624720418048,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_cifar10(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    batch_size = labels.size(0)\n","    idx = torch.randperm(batch_size).to('cuda')\n","    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n","    labels_b = labels[idx]\n","    return mixup_inputs, labels, labels_b, lmbda"],"id":"contemporary-gross","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"right-spending","executionInfo":{"status":"ok","timestamp":1624720418048,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"right-spending","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"written-bookmark","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624731171098,"user_tz":-60,"elapsed":10753052,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"51f5117d-a38f-43c1-95f1-d035b08cb854"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_loss = 0.\n","    epoch_mixup_loss = 0.\n","    epoch_org_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        mixup_inputs, labels, labels_b, lmbda = mixup_cifar10(inputs, labels, alpha)\n","        optimizer.zero_grad()\n","        outputs = model(mixup_inputs)\n","        mixup_loss = mixup_criterion(criterion, outputs, labels, labels_b, lmbda)\n","        \n","        ##\n","        outputs_org = model(inputs)\n","        loss_org = criterion(outputs_org, labels)\n","        weighted_total_loss = mixup_loss * perturb_loss_weight + loss_org * (1 - perturb_loss_weight)\n","        \n","        epoch_mixup_loss += mixup_loss.item()\n","        epoch_org_loss += loss_org.item()\n","        \n","        epoch_loss += (mixup_loss.item() + loss_org.item())\n","        \n","        weighted_total_loss.backward()\n","        ##\n","        \n","        optimizer.step()\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_org_loss, epoch_loss))"],"id":"written-bookmark","execution_count":8,"outputs":[{"output_type":"stream","text":["0: 778.601233124733 682.3701876401901 1460.971420764923\n","1: 661.9098603725433 502.6831756234169 1164.5930359959602\n","2: 604.294224023819 402.01337629556656 1006.3076003193855\n","3: 565.6802163124084 337.6306462287903 903.3108625411987\n","4: 540.9242900013924 288.1737778186798 829.0980678200722\n","5: 518.6493279337883 258.3730217516422 777.0223496854305\n","6: 497.0507006049156 231.20106768608093 728.2517682909966\n","7: 494.39497900009155 208.0145850777626 702.4095640778542\n","8: 473.7161278426647 189.21330109238625 662.929428935051\n","9: 460.7242885529995 177.934929728508 638.6592182815075\n","10: 469.046707123518 163.01060336828232 632.0573104918003\n","11: 468.1877313256264 155.48464477062225 623.6723760962486\n","12: 452.3106158673763 146.49898290634155 598.8095987737179\n","13: 439.14601115882397 138.56724575161934 577.7132569104433\n","14: 450.2386888861656 133.53133817017078 583.7700270563364\n","15: 439.5983989238739 127.99303928017616 567.5914382040501\n","16: 448.8667234927416 122.14432001113892 571.0110435038805\n","17: 415.55340625345707 119.37334348261356 534.9267497360706\n","18: 431.43041908741 113.2692194133997 544.6996385008097\n","19: 425.6448723375797 111.75394950807095 537.3988218456507\n","20: 422.2559054046869 106.9946535974741 529.250559002161\n","21: 423.4441207051277 103.24884451925755 526.6929652243853\n","22: 422.1713309288025 102.01565957814455 524.186990506947\n","23: 416.21963089704514 102.61527884751558 518.8349097445607\n","24: 423.1345981359482 97.55534873902798 520.6899468749762\n","25: 405.31783063709736 95.29176941514015 500.6096000522375\n","26: 418.42012095451355 93.0701230764389 511.49024403095245\n","27: 404.1833546757698 91.14194832742214 495.32530300319195\n","28: 403.35067231953144 90.53618141263723 493.8868537321687\n","29: 416.82961739599705 90.9119525179267 507.74156991392374\n","30: 410.88966435194016 86.19706242531538 497.08672677725554\n","31: 406.34505642950535 85.05554017424583 491.4005966037512\n","32: 393.75636561214924 84.64171484857798 478.3980804607272\n","33: 398.1843452602625 83.61937160044909 481.8037168607116\n","34: 413.07861080765724 85.27206735312939 498.35067816078663\n","35: 398.4377162158489 81.79468004405499 480.2323962599039\n","36: 405.0009298622608 79.80775443464518 484.808684296906\n","37: 395.7004365622997 80.21716305613518 475.9175996184349\n","38: 399.9247867614031 79.86439823359251 479.7891849949956\n","39: 391.5242622047663 78.39456784725189 469.91883005201817\n","40: 384.4758283495903 79.52864431589842 464.0044726654887\n","41: 396.95059794187546 76.52556622028351 473.47616416215897\n","42: 395.1171351522207 75.14642994850874 470.26356510072947\n","43: 393.6154551655054 74.56911062449217 468.1845657899976\n","44: 404.0109126120806 73.95492728799582 477.9658399000764\n","45: 393.6987415999174 71.47802250832319 465.1767641082406\n","46: 386.7132553383708 72.00379904359579 458.7170543819666\n","47: 384.126257956028 71.21519213169813 455.3414500877261\n","48: 389.143989905715 72.48934129625559 461.6333312019706\n","49: 381.8002662807703 71.23226077854633 453.03252705931664\n","50: 388.934678837657 68.1392038539052 457.0738826915622\n","51: 376.4570210725069 70.16053552925587 446.6175566017628\n","52: 385.72539380192757 67.92712373659015 453.6525175385177\n","53: 386.29988691210747 66.22856128960848 452.52844820171595\n","54: 385.6209974735975 68.71166158467531 454.33265905827284\n","55: 377.3422119319439 66.20536401495337 443.54757594689727\n","56: 395.73636639118195 65.03394152224064 460.7703079134226\n","57: 384.8785853087902 65.51342264562845 450.39200795441866\n","58: 383.34996923804283 67.74080799520016 451.090777233243\n","59: 383.890471637249 64.55503591150045 448.44550754874945\n","60: 375.4034587740898 63.926902670413256 439.33036144450307\n","61: 387.3251846730709 63.48361921310425 450.80880388617516\n","62: 377.2251477986574 64.09588197618723 441.32102977484465\n","63: 379.75320095568895 61.15816316008568 440.91136411577463\n","64: 370.9220494180918 62.55020461976528 433.47225403785706\n","65: 393.31765922904015 63.061702474951744 456.3793617039919\n","66: 372.0444968044758 61.96637896448374 434.0108757689595\n","67: 378.8032532930374 61.003727216273546 439.80698050931096\n","68: 371.71904534846544 59.944369558244944 431.6634149067104\n","69: 393.4291218370199 60.78817003965378 454.2172918766737\n","70: 381.25844928622246 60.66059967875481 441.91904896497726\n","71: 371.063062466681 59.303082913160324 430.3661453798413\n","72: 367.6203937307 61.212962314486504 428.8333560451865\n","73: 365.4335717111826 59.78995729610324 425.22352900728583\n","74: 381.18403478711843 58.1246594004333 439.30869418755174\n","75: 371.4832309707999 58.84410407021642 430.32733504101634\n","76: 380.4434424266219 60.036239355802536 440.47968178242445\n","77: 377.0594955533743 57.44176075980067 434.50125631317496\n","78: 369.51604966819286 57.48291126638651 426.9989609345794\n","79: 378.3454123586416 57.87804524973035 436.223457608372\n","80: 379.9471705853939 56.81965835392475 436.76682893931866\n","81: 374.65847674012184 53.89429786801338 428.5527746081352\n","82: 371.8430914580822 57.29428332298994 429.13737478107214\n","83: 369.68488340079784 56.20105802640319 425.88594142720103\n","84: 370.94845470786095 56.17867334932089 427.12712805718184\n","85: 378.03998516499996 56.515214931219816 434.5552000962198\n","86: 382.37184189260006 56.62208693102002 438.9939288236201\n","87: 370.9305568188429 55.45576931536198 426.38632613420486\n","88: 378.3417474627495 53.892872493714094 432.2346199564636\n","89: 374.612932279706 55.59227354452014 430.20520582422614\n","90: 380.1602710932493 56.51697802916169 436.677249122411\n","91: 370.73629850149155 54.34380059316754 425.0800990946591\n","92: 369.25048630684614 55.4542742036283 424.70476051047444\n","93: 376.33565290272236 55.730545338243246 432.0661982409656\n","94: 363.1744505688548 55.21655060350895 418.39100117236376\n","95: 381.19522477686405 53.449259143322706 434.64448392018676\n","96: 362.19922582805157 53.78517591021955 415.9844017382711\n","97: 364.2284289598465 53.035510927438736 417.26393988728523\n","98: 363.73317567259073 54.24626763537526 417.979443307966\n","99: 357.28695421665907 52.94002815335989 410.22698237001896\n","100: 380.70915626734495 54.64614683389664 435.3553031012416\n","101: 376.986568916589 52.16427810117602 429.15084701776505\n","102: 364.31523180007935 52.85407504439354 417.1693068444729\n","103: 369.07138618826866 53.12434235587716 422.1957285441458\n","104: 367.9603258743882 50.11882486194372 418.07915073633194\n","105: 358.8188929259777 51.26030929014087 410.0792022161186\n","106: 374.67222851514816 53.44310795515776 428.1153364703059\n","107: 369.2843309044838 53.41188419237733 422.6962150968611\n","108: 372.1012675091624 52.64071586728096 424.7419833764434\n","109: 366.3714052885771 51.15710247308016 417.52850776165724\n","110: 361.0487789809704 51.010584469884634 412.059363450855\n","111: 369.64784698188305 52.46459439024329 422.11244137212634\n","112: 369.5600700005889 52.506510730832815 422.0665807314217\n","113: 367.0107591301203 51.18800900876522 418.1987681388855\n","114: 361.8626432865858 52.56629362329841 414.4289369098842\n","115: 371.60777032375336 48.37591986358166 419.983690187335\n","116: 365.19235698133707 51.10673592239618 416.29909290373325\n","117: 367.6175747215748 49.671226762235165 417.28880148380995\n","118: 379.64755626022816 48.54965366795659 428.19720992818475\n","119: 358.36534713208675 51.31569119170308 409.68103832378983\n","120: 359.4092085212469 49.38555176183581 408.7947602830827\n","121: 366.6295090094209 50.868049301207066 417.49755831062794\n","122: 373.4382621049881 49.407495856285095 422.8457579612732\n","123: 369.9259505569935 50.428413432091475 420.35436398908496\n","124: 360.6016159579158 50.12908274307847 410.73069870099425\n","125: 362.05291890352964 50.017676681280136 412.0705955848098\n","126: 378.408042550087 50.48216212540865 428.8902046754956\n","127: 364.4392611980438 50.64059879630804 415.07985999435186\n","128: 372.23383374512196 49.68941401690245 421.9232477620244\n","129: 364.4409005269408 51.05028835311532 415.49118888005614\n","130: 348.96923346817493 51.038672372698784 400.0079058408737\n","131: 377.1695114374161 49.76421328634024 426.9337247237563\n","132: 361.9792764261365 51.265178713947535 413.244455140084\n","133: 362.3217984139919 49.97662201710045 412.2984204310924\n","134: 370.3779084086418 49.7623869292438 420.1402953378856\n","135: 361.3265382498503 49.109901305288076 410.43643955513835\n","136: 364.8811886906624 49.98187818005681 414.8630668707192\n","137: 353.2806852608919 51.26140362024307 404.542088881135\n","138: 349.19674725830555 48.756987396627665 397.9537346549332\n","139: 365.6022530719638 48.58122184127569 414.1834749132395\n","140: 355.74739111214876 46.7624661847949 402.50985729694366\n","141: 366.5434893593192 48.434685472398996 414.9781748317182\n","142: 362.1990848556161 48.42593570612371 410.6250205617398\n","143: 357.7516522333026 46.589501816779375 404.34115405008197\n","144: 353.85213296860456 47.927002377808094 401.77913534641266\n","145: 356.0494421720505 45.42952476069331 401.4789669327438\n","146: 376.1596550568938 47.36966625601053 423.52932131290436\n","147: 369.49378606677055 47.20868173614144 416.702467802912\n","148: 361.4398383721709 47.84341735392809 409.283255726099\n","149: 357.6191190928221 47.36573151871562 404.9848506115377\n","150: 368.08470460772514 47.85314500331879 415.93784961104393\n","151: 361.9531921893358 48.445326309651136 410.39851849898696\n","152: 370.26428976655006 46.52606251835823 416.7903522849083\n","153: 355.8015983477235 46.983696568757296 402.7852949164808\n","154: 361.9592882245779 47.89871275052428 409.8580009751022\n","155: 353.84030909091234 46.89107502065599 400.73138411156833\n","156: 360.69417161494493 45.086826890707016 405.78099850565195\n","157: 364.9457244873047 47.72206076234579 412.6677852496505\n","158: 362.2412512153387 46.89519762247801 409.1364488378167\n","159: 365.4336504638195 46.174263786524534 411.60791425034404\n","160: 351.8279826566577 46.037705302238464 397.86568795889616\n","161: 352.7327636703849 47.57748656719923 400.3102502375841\n","162: 370.8958259373903 47.02430262416601 417.92012856155634\n","163: 362.7743940129876 47.739473417401314 410.5138674303889\n","164: 353.96502608060837 49.70881473645568 403.67384081706405\n","165: 358.70720233768225 46.88189619779587 405.5890985354781\n","166: 366.4256353005767 45.64064522832632 412.066280528903\n","167: 361.95700481534004 47.64205186441541 409.59905667975545\n","168: 359.93446303904057 49.727168433368206 409.66163147240877\n","169: 357.6715593524277 45.52592265047133 403.19748200289905\n","170: 367.3539341837168 48.2908330000937 415.6447671838105\n","171: 359.5689085908234 48.865272894501686 408.4341814853251\n","172: 349.9380208104849 45.630376033484936 395.5683968439698\n","173: 342.8725833296776 46.626128543168306 389.4987118728459\n","174: 364.13647362589836 43.51377487927675 407.6502485051751\n","175: 360.193669013679 45.79295760020614 405.98662661388516\n","176: 354.36652824282646 44.88019797205925 399.2467262148857\n","177: 360.59317127615213 46.163478299975395 406.75664957612753\n","178: 369.9630655273795 43.864369466900826 413.82743499428034\n","179: 359.953036442399 45.354917000979185 405.3079534433782\n","180: 358.6135575771332 44.94345347955823 403.5570110566914\n","181: 367.9648260101676 45.56721604987979 413.5320420600474\n","182: 363.39014745503664 45.565483920276165 408.9556313753128\n","183: 364.83534543961287 45.98728163540363 410.8226270750165\n","184: 348.96702697873116 46.418445555493236 395.3854725342244\n","185: 365.33718959242105 44.38834024593234 409.7255298383534\n","186: 358.9220286235213 46.0809451341629 405.00297375768423\n","187: 357.50961419194937 45.7899848818779 403.29959907382727\n","188: 346.2891985476017 47.026920625939965 393.31611917354167\n","189: 359.6941588893533 45.391288463026285 405.08544735237956\n","190: 364.2259946092963 45.72774263843894 409.95373724773526\n","191: 352.46170450747013 44.51398674398661 396.97569125145674\n","192: 355.4813682362437 46.243825413286686 401.7251936495304\n","193: 355.80787923932076 45.51825467124581 401.32613391056657\n","194: 347.8248854279518 46.880998365581036 394.70588379353285\n","195: 361.2690711170435 45.382987316697836 406.65205843374133\n","196: 368.10212756693363 44.565398782491684 412.6675263494253\n","197: 374.50435972213745 45.52714245393872 420.0315021760762\n","198: 361.3820303231478 46.31049756705761 407.6925278902054\n","199: 360.0235389918089 44.42467923089862 404.4482182227075\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"frozen-damage","executionInfo":{"status":"ok","timestamp":1624731171099,"user_tz":-60,"elapsed":10,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_cifar10_augment')\n","# model = ResNet18()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_cifar10_augment'))"],"id":"frozen-damage","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"aboriginal-lafayette","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624731174340,"user_tz":-60,"elapsed":3248,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"712b0d1f-6392-475a-845f-405e3a476f23"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"aboriginal-lafayette","execution_count":10,"outputs":[{"output_type":"stream","text":["0.9195\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"therapeutic-orlando","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624731195241,"user_tz":-60,"elapsed":20902,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"689dada6-e13e-407d-e247-15b2265c1869"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"therapeutic-orlando","execution_count":11,"outputs":[{"output_type":"stream","text":["0.96298\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"commercial-heavy","executionInfo":{"status":"ok","timestamp":1624731195241,"user_tz":-60,"elapsed":15,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"commercial-heavy","execution_count":11,"outputs":[]}]}
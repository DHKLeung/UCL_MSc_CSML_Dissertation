{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "continental-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cultural-recorder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2690df6f4d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration and Hyperparameters\n",
    "\"\"\"\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "step_size = 0.01\n",
    "random_seed = 0\n",
    "epochs = 100\n",
    "L2_decay = 1e-4\n",
    "alpha = 1.\n",
    "perturb_loss_weight = 0.25\n",
    "\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "international-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data\n",
    "\"\"\"\n",
    "train_set = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "test_set = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brown-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "model.conv1 = torch.nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n",
    "model.fc = torch.nn.Linear(512, 10)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "directed-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_MNIST(inputs, labels, alpha):\n",
    "    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample()\n",
    "    batch_size = labels.size(0)\n",
    "    idx = torch.randperm(batch_size)\n",
    "    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n",
    "    labels_b = labels[idx]\n",
    "    return mixup_inputs, labels, labels_b, lmbda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "democratic-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n",
    "    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n",
    "    return mixup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adaptive-short",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 527.7763129249215 84.1710719903931 611.9473849153146\n",
      "1: 414.74778794310987 23.981839226093143 438.729627169203\n",
      "2: 384.7026916705072 15.21763153502252 399.9203232055297\n",
      "3: 365.67381589859724 10.883181015669834 376.5569969142671\n",
      "4: 348.25201917625964 8.565018788678572 356.8170379649382\n",
      "5: 340.40702239610255 6.099915596598294 346.50693799270084\n",
      "6: 329.48911689128727 4.827730320394039 334.3168472116813\n",
      "7: 331.9413714101538 4.1328621644643135 336.0742335746181\n",
      "8: 328.0972791975364 3.1402558732806938 331.2375350708171\n",
      "9: 323.68782244529575 3.3666595634422265 327.054482008738\n",
      "10: 317.5547550357878 2.434990468536853 319.9897455043247\n",
      "11: 323.487574853003 2.6318596876371885 326.1194345406402\n",
      "12: 312.2003298578784 2.0144832966761896 314.2148131545546\n",
      "13: 311.4894963670522 2.0779541322845034 313.5674504993367\n",
      "14: 307.92447229102254 1.5615098061898607 309.4859820972124\n",
      "15: 306.7695453176275 1.615375899222272 308.38492121684976\n",
      "16: 299.89626279845834 1.7575965531359543 301.6538593515943\n",
      "17: 301.4786708392203 1.6871954022717546 303.16586624149204\n",
      "18: 300.1098553421907 1.484699163287587 301.5945545054783\n",
      "19: 296.0977746658027 1.2011983135598712 297.2989729793626\n",
      "20: 293.69501398014836 1.3650501041847747 295.06006408433313\n",
      "21: 298.0529812518507 1.0333152258317568 299.0862964776825\n",
      "22: 279.9033936645137 0.5388373199966736 280.4422309845104\n",
      "23: 284.8240934247151 1.1471678830348537 285.97126130774996\n",
      "24: 292.812147423625 0.8786672671849374 293.69081469080993\n",
      "25: 285.58963408879936 1.1739704129577149 286.7636045017571\n",
      "26: 282.5757365487516 0.729012951534969 283.30474950028656\n",
      "27: 284.69153744354844 0.5654811109779985 285.25701855452644\n",
      "28: 279.892115755938 0.5780240675521782 280.4701398234902\n",
      "29: 284.7156245643273 1.0914362314360915 285.8070607957634\n",
      "30: 280.0917341024615 1.4500395818031393 281.5417736842646\n",
      "31: 284.4565576836467 0.7571353158491547 285.21369299949583\n",
      "32: 276.27657347824425 0.34495387766946806 276.6215273559137\n",
      "33: 276.1081993756816 0.4257257038661919 276.53392507954777\n",
      "34: 275.88291400601156 0.4360370623435301 276.3189510683551\n",
      "35: 275.087472287938 0.7806644757838512 275.86813676372185\n",
      "36: 269.4846649011597 0.5333855444259825 270.0180504455857\n",
      "37: 273.5706375837326 1.0263147306359315 274.59695231436854\n",
      "38: 273.7538910526782 1.9186643632201594 275.6725554158984\n",
      "39: 271.4129312084988 1.0281803314646822 272.44111153996346\n",
      "40: 273.3386629372835 0.49346180758584524 273.83212474486936\n",
      "41: 277.74489816627465 0.8709158577512426 278.6158140240259\n",
      "42: 270.983858323656 0.4547393296597875 271.4385976533158\n",
      "43: 269.6396493855864 0.39028210625838256 270.02993149184476\n",
      "44: 268.1113487237599 0.5377130401211616 268.64906176388104\n",
      "45: 269.3182388214627 0.8046441356236755 270.1228829570864\n",
      "46: 270.2484504254535 0.3069291298961616 270.55537955534965\n",
      "47: 267.75276200752705 1.1954734837563592 268.9482354912834\n",
      "48: 271.58489860221744 0.734743422264728 272.31964202448216\n",
      "49: 269.1937995702028 0.5721754986698215 269.76597506887265\n",
      "50: 269.89864405896515 0.7712541792134289 270.6698982381786\n",
      "51: 266.18720678053796 0.21832569110119948 266.40553247163916\n",
      "52: 268.91555413231254 0.5562696313209017 269.47182376363344\n",
      "53: 260.4557483266108 0.8660127227303747 261.32176104934115\n",
      "54: 273.0373656898737 0.9735855323597207 274.0109512222334\n",
      "55: 266.58740592096 1.138445952507027 267.72585187346704\n",
      "56: 262.43994070217013 0.8988442413574376 263.33878494352757\n",
      "57: 265.4262625724077 0.254561272005958 265.6808238444137\n",
      "58: 262.52404199540615 0.5037212614697637 263.0277632568759\n",
      "59: 262.5589814116247 0.7469930902370834 263.3059745018618\n",
      "60: 258.1750307958573 0.2585701076204714 258.4336009034778\n",
      "61: 265.9090969366953 1.6678946527390508 267.57699158943433\n",
      "62: 259.6959642022848 0.6961402053748316 260.39210440765964\n",
      "63: 260.6968325471971 0.499717115668318 261.19654966286544\n",
      "64: 260.96176807722077 0.22226681500251289 261.1840348922233\n",
      "65: 254.9789220020175 0.8049367117928341 255.78385871381033\n",
      "66: 262.1902116667479 0.4921758678756305 262.68238753462356\n",
      "67: 263.0265171250794 0.614837958386488 263.6413550834659\n",
      "68: 253.54669979959726 0.5541236334247515 254.10082343302201\n",
      "69: 256.73848628927954 1.4627942162223917 258.2012805055019\n",
      "70: 265.2492731916718 0.6150328320145491 265.86430602368637\n",
      "71: 256.66885101795197 0.21387665664951783 256.8827276746015\n",
      "72: 251.69806133117527 0.48151075856003445 252.1795720897353\n",
      "73: 256.2362904944457 0.43355863889155444 256.66984913333727\n",
      "74: 252.70156225562096 0.6206634463742375 253.3222257019952\n",
      "75: 250.39302275585942 0.16394558619504096 250.55696834205446\n",
      "76: 251.7242062627338 0.15661940386416973 251.88082566659796\n",
      "77: 262.89594625821337 1.1102232103803544 264.0061694685937\n",
      "78: 251.32023175735958 0.3556480504812498 251.67587980784083\n",
      "79: 255.65200397279114 0.09616011563775828 255.7481640884289\n",
      "80: 253.89316943613812 0.42742232152522774 254.32059175766335\n",
      "81: 255.1514423005283 0.4557504068579874 255.60719270738628\n",
      "82: 251.81218370050192 0.5844992788697709 252.3966829793717\n",
      "83: 253.6917038243264 0.20335045899628312 253.89505428332268\n",
      "84: 248.48329820483923 0.13409818889340386 248.61739639373263\n",
      "85: 256.5189336123876 0.09934656370023731 256.61828017608786\n",
      "86: 241.90392621885985 0.8894099135031865 242.79333613236304\n",
      "87: 255.51476917229593 1.8880005374085158 257.40276970970444\n",
      "88: 251.0244060587138 0.39490552119968925 251.41931157991348\n",
      "89: 249.0645678639412 0.16327374775937642 249.22784161170057\n",
      "90: 244.65428884210996 0.4704282336315373 245.1247170757415\n",
      "91: 243.6092999689281 0.64416850419002 244.25346847311812\n",
      "92: 250.69630429148674 0.11808153599486104 250.8143858274816\n",
      "93: 258.61428366601467 0.691122622381954 259.3054062883966\n",
      "94: 251.25686497427523 1.5854955574104679 252.8423605316857\n",
      "95: 250.9074033740908 0.9797489534175838 251.88715232750837\n",
      "96: 256.2328603491187 0.8095303043883177 257.042390653507\n",
      "97: 246.0680859880522 0.5227976583191776 246.59088364637137\n",
      "98: 243.40403890075686 0.12707953495919355 243.53111843571605\n",
      "99: 238.3744405694306 0.15846460057946388 238.53290517001005\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training\n",
    "\"\"\"\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.\n",
    "    epoch_mixup_loss = 0.\n",
    "    epoch_org_loss = 0.\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        mixup_inputs, labels, labels_b, lmbda = mixup_MNIST(inputs, labels, alpha)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mixup_inputs)\n",
    "        mixup_loss = mixup_criterion(criterion, outputs, labels, labels_b, lmbda)\n",
    "        \n",
    "        ##\n",
    "        outputs_org = model(inputs)\n",
    "        loss_org = criterion(outputs_org, labels)\n",
    "        weighted_total_loss = mixup_loss * perturb_loss_weight + loss_org * (1 - perturb_loss_weight)\n",
    "        \n",
    "        epoch_mixup_loss += mixup_loss.item()\n",
    "        epoch_org_loss += loss_org.item()\n",
    "        \n",
    "        epoch_loss += (mixup_loss.item() + loss_org.item())\n",
    "        \n",
    "        weighted_total_loss.backward()\n",
    "        ##\n",
    "        \n",
    "        optimizer.step()\n",
    "    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_org_loss, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "armed-contact",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), './mixup_model_pytorch_mnist_augment')\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.conv1 = torch.nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n",
    "model.fc = torch.nn.Linear(512, 10)\n",
    "model.load_state_dict(torch.load('./mixup_model_pytorch_mnist_augment'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "competitive-penny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9948\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        outputs = model(inputs)\n",
    "        _, predicts = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicts == labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "victorian-financing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999666666666667\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        outputs = model(inputs)\n",
    "        _, predicts = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicts == labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-situation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

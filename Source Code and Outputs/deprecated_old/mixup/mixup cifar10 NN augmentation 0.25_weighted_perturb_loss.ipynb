{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "romantic-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "silver-clear",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12a0fd7c4d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration and Hyperparameters\n",
    "\"\"\"\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  # can omit\n",
    "    transforms.RandomHorizontalFlip(),  # can omit\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        (0.4914, 0.4822, 0.4465),\n",
    "        (0.2023, 0.1994, 0.2010)\n",
    "    )\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        (0.4914, 0.4822, 0.4465),\n",
    "        (0.2023, 0.1994, 0.2010)\n",
    "    )\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "step_size = 0.1\n",
    "random_seed = 0\n",
    "epochs = 200\n",
    "L2_decay = 1e-4\n",
    "alpha = 1.\n",
    "perturb_loss_weight = 0.25\n",
    "\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "relative-mobility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data\n",
    "\"\"\"\n",
    "train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "starting-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.__dict__['ResNet18']()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "contemporary-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_cifar10(inputs, labels, alpha):\n",
    "    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample()\n",
    "    batch_size = labels.size(0)\n",
    "    idx = torch.randperm(batch_size)\n",
    "    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n",
    "    labels_b = labels[idx]\n",
    "    return mixup_inputs, labels, labels_b, lmbda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "right-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n",
    "    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n",
    "    return mixup_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "written-bookmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 767.0674430131912 612.601745724678 1379.6691887378693\n",
      "1: 658.5494607686996 397.94827073812485 1056.4977315068245\n",
      "2: 589.4818803668022 301.91297778487206 891.3948581516743\n",
      "3: 561.0910027623177 243.89913454651833 804.990137308836\n",
      "4: 521.4682038724422 207.94182029366493 729.4100241661072\n",
      "5: 520.1608279645443 181.24125941097736 701.4020873755217\n",
      "6: 503.65187108516693 161.4518569111824 665.1037279963493\n",
      "7: 495.77005141973495 146.46370485424995 642.2337562739849\n",
      "8: 466.72070440649986 132.97498874366283 599.6956931501627\n",
      "9: 477.62203177809715 121.72698210924864 599.3490138873458\n",
      "10: 473.1333854794502 113.18511267006397 586.3184981495142\n",
      "11: 464.9162220209837 104.51001756638288 569.4262395873666\n",
      "12: 465.80310471355915 97.85699523240328 563.6600999459624\n",
      "13: 482.4604536741972 90.89984782785177 573.360301502049\n",
      "14: 459.62832494080067 85.40313965082169 545.0314645916224\n",
      "15: 450.9483634084463 81.02963697910309 531.9780003875494\n",
      "16: 439.08563935756683 77.13340177386999 516.2190411314368\n",
      "17: 446.3400616943836 72.48610536009073 518.8261670544744\n",
      "18: 429.0231688320637 68.80799159407616 497.83116042613983\n",
      "19: 455.38395519554615 65.40990740805864 520.7938626036048\n",
      "20: 448.25941986590624 64.10799037665129 512.3674102425575\n",
      "21: 454.5637629330158 62.11678259447217 516.680545527488\n",
      "22: 433.0197527334094 59.841250505298376 492.8610032387078\n",
      "23: 435.0249901711941 58.06964599713683 493.0946361683309\n",
      "24: 430.06884793937206 54.888590555638075 484.95743849501014\n",
      "25: 417.65612041950226 53.31672690063715 470.9728473201394\n",
      "26: 426.9554258733988 52.69199785962701 479.6474237330258\n",
      "27: 425.93524519354105 53.176914263516665 479.1121594570577\n",
      "28: 421.7379237115383 49.9082046803087 471.646128391847\n",
      "29: 423.5247679948807 49.51883463934064 473.0436026342213\n",
      "30: 425.33906422555447 46.32342058978975 471.6624848153442\n",
      "31: 442.89815382659435 46.33472481369972 489.2328786402941\n",
      "32: 426.1431443542242 46.44134987331927 472.5844942275435\n",
      "33: 429.0843786895275 45.34212648309767 474.4265051726252\n",
      "34: 427.4924126788974 45.78255753777921 473.2749702166766\n",
      "35: 428.56090719997883 42.98476943746209 471.5456766374409\n",
      "36: 423.772112429142 43.21412186510861 466.9862342942506\n",
      "37: 421.4142195880413 42.228320298716426 463.64253988675773\n",
      "38: 425.97453965991735 39.744078202173114 465.71861786209047\n",
      "39: 423.688369050622 41.171503495424986 464.859872546047\n",
      "40: 418.0315897241235 40.070772571489215 458.1023622956127\n",
      "41: 420.7549139633775 39.69421008601785 460.4491240493953\n",
      "42: 417.98013385385275 38.23169321939349 456.21182707324624\n",
      "43: 401.8577187657356 39.36790681257844 441.22562557831407\n",
      "44: 416.00599325448275 39.196315720677376 455.2023089751601\n",
      "45: 418.5429428219795 37.345739686861634 455.88868250884116\n",
      "46: 421.4317281022668 36.31364765390754 457.7453757561743\n",
      "47: 409.78247052431107 37.44013246335089 447.22260298766196\n",
      "48: 427.7492031827569 34.96984971873462 462.7190529014915\n",
      "49: 417.12691359966993 36.923812897875905 454.05072649754584\n",
      "50: 403.92429865151644 35.33775262068957 439.262051272206\n",
      "51: 422.0963968783617 36.50001884251833 458.59641572088003\n",
      "52: 433.0856327712536 33.90799344237894 466.9936262136325\n",
      "53: 403.43733362853527 36.520354149863124 439.9576877783984\n",
      "54: 393.70606891810894 35.45027955900878 429.1563484771177\n",
      "55: 423.4355090186 35.00943893752992 458.4449479561299\n",
      "56: 418.79646783322096 35.19060031324625 453.9870681464672\n",
      "57: 414.7361050993204 35.36651887744665 450.10262397676706\n",
      "58: 419.07986845076084 34.49374595656991 453.57361440733075\n",
      "59: 411.9358162134886 34.50094474386424 446.4367609573528\n",
      "60: 423.9393899217248 32.261683880351484 456.2010738020763\n",
      "61: 416.80541759729385 33.12272969260812 449.928147289902\n",
      "62: 413.7012291327119 35.5058232024312 449.2070523351431\n",
      "63: 400.49173568561673 33.4375565610826 433.92929224669933\n",
      "64: 407.426821783185 32.54622570052743 439.97304748371243\n",
      "65: 404.7508161365986 32.05019364506006 436.80100978165865\n",
      "66: 425.70454224944115 32.82985263131559 458.53439488075674\n",
      "67: 413.4898913651705 31.368158439174294 444.8580498043448\n",
      "68: 404.79619293659925 33.03954562963918 437.83573856623843\n",
      "69: 414.7464884072542 30.247133052907884 444.9936214601621\n",
      "70: 416.81939685344696 31.31014392338693 448.1295407768339\n",
      "71: 407.9550737440586 31.90576204098761 439.8608357850462\n",
      "72: 410.5533975958824 32.391126880422235 442.94452447630465\n",
      "73: 407.1802499368787 30.83075865264982 438.0110085895285\n",
      "74: 415.47097036987543 34.07528566662222 449.54625603649765\n",
      "75: 413.8969116061926 33.35495854727924 447.2518701534718\n",
      "76: 417.6095398925245 32.41597652621567 450.02551641874015\n",
      "77: 419.27658697217703 32.03192844800651 451.30851542018354\n",
      "78: 413.9250849336386 33.22178240958601 447.1468673432246\n",
      "79: 397.39741637185216 31.54864322114736 428.9460595929995\n",
      "80: 415.9730196520686 29.907051848247647 445.88007150031626\n",
      "81: 406.5111418403685 32.44809181522578 438.9592336555943\n",
      "82: 410.2824586369097 32.08741457480937 442.3698732117191\n",
      "83: 408.4396407753229 31.752037638798356 440.19167841412127\n",
      "84: 402.966827891767 31.339679468423128 434.30650736019015\n",
      "85: 399.6776374615729 33.39006012212485 433.06769758369774\n",
      "86: 403.3703914284706 29.10992509778589 432.4803165262565\n",
      "87: 413.5359435789287 30.164941493421793 443.7008850723505\n",
      "88: 432.215966001153 31.40575024113059 463.6217162422836\n",
      "89: 404.50436406582594 31.42447575274855 435.9288398185745\n",
      "90: 412.13441959023476 28.969569629058242 441.103989219293\n",
      "91: 401.84914147108793 31.10993758123368 432.9590790523216\n",
      "92: 406.07285460829735 31.276315631344914 437.34917023964226\n",
      "93: 394.0348989367485 29.278009988367558 423.31290892511606\n",
      "94: 424.53474490344524 31.326816994696856 455.8615618981421\n",
      "95: 397.5925564914942 29.010491239838302 426.6030477313325\n",
      "96: 423.31419705599546 29.927017893642187 453.24121494963765\n",
      "97: 413.1121493652463 31.023022926878184 444.1351722921245\n",
      "98: 414.16462806612253 30.372662105597556 444.5372901717201\n",
      "99: 400.52390483766794 28.897392661310732 429.4212974989787\n",
      "100: 405.9207865744829 30.59737880062312 436.51816537510604\n",
      "101: 404.62032360211015 29.53907689731568 434.15940049942583\n",
      "102: 419.8588644564152 29.665986070409417 449.5248505268246\n",
      "103: 395.21168277412653 30.090298647992313 425.30198142211884\n",
      "104: 403.8341235294938 31.928426324389875 435.7625498538837\n",
      "105: 397.54349429160357 30.028856023214757 427.5723503148183\n",
      "106: 396.5376679599285 29.901416935026646 426.43908489495516\n",
      "107: 425.9006311148405 29.16709941998124 455.06773053482175\n",
      "108: 408.8819833993912 28.573618337512016 437.4556017369032\n",
      "109: 414.0141912251711 29.39641340263188 443.41060462780297\n",
      "110: 404.1231846138835 28.339828258380294 432.4630128722638\n",
      "111: 397.1799796670675 29.264524024911225 426.44450369197875\n",
      "112: 389.1404068171978 30.387908852659166 419.52831566985697\n",
      "113: 408.95121958851814 28.783968560397625 437.73518814891577\n",
      "114: 390.12185130268335 27.558564980514348 417.6804162831977\n",
      "115: 404.2327346280217 29.146107277832925 433.37884190585464\n",
      "116: 393.05550476163626 30.373399594798684 423.42890435643494\n",
      "117: 407.39431768655777 27.83339361101389 435.22771129757166\n",
      "118: 402.4247882962227 29.00494096800685 431.42972926422954\n",
      "119: 399.0215302705765 28.59088064916432 427.6124109197408\n",
      "120: 402.9182023406029 28.99012739676982 431.9083297373727\n",
      "121: 410.13600949570537 29.628949835896492 439.76495933160186\n",
      "122: 404.6205339729786 26.27755412645638 430.898088099435\n",
      "123: 419.5137289837003 29.469627323560417 448.9833563072607\n",
      "124: 394.295362804085 27.981973707675934 422.27733651176095\n",
      "125: 408.6600347273052 28.435640526004136 437.0956752533093\n",
      "126: 404.7310660742223 28.910803644917905 433.64186971914023\n",
      "127: 388.3990091867745 28.90186449047178 417.3008736772463\n",
      "128: 397.78238243982196 28.55758542753756 426.3399678673595\n",
      "129: 388.6400886140764 28.251474025659263 416.89156263973564\n",
      "130: 412.0984558239579 29.676293580792844 441.77474940475076\n",
      "131: 408.878506526351 28.754587742500007 437.633094268851\n",
      "132: 398.33176593482494 27.928863362874836 426.2606292976998\n",
      "133: 398.52129374071956 26.611528898589313 425.13282263930887\n",
      "134: 414.9965157881379 26.8203542008996 441.8168699890375\n",
      "135: 406.93690552562475 28.49115084670484 435.4280563723296\n",
      "136: 400.08136812224984 27.986118633300066 428.0674867555499\n",
      "137: 390.2064693272114 29.4639621572569 419.6704314844683\n",
      "138: 398.7778647467494 28.464329906739295 427.2421946534887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139: 411.5562709271908 28.37197949644178 439.92825042363256\n",
      "140: 402.50873450934887 26.660237081348896 429.16897159069777\n",
      "141: 404.1802623048425 29.74114882014692 433.9214111249894\n",
      "142: 411.6448399014771 29.10661331564188 440.751453217119\n",
      "143: 422.92730287835 27.299012386240065 450.2263152645901\n",
      "144: 404.4747318401933 31.469325029291213 435.9440568694845\n",
      "145: 403.99895613640547 27.72436537919566 431.7233215156011\n",
      "146: 413.38020070269704 28.55961978342384 441.9398204861209\n",
      "147: 401.7484167739749 27.201019409112632 428.9494361830875\n",
      "148: 405.14574658870697 29.737382613122463 434.88312920182943\n",
      "149: 401.0727546662092 28.962763283401728 430.03551794961095\n",
      "150: 396.15144269913435 29.290206218138337 425.4416489172727\n",
      "151: 411.3299970626831 24.9108269023709 436.240823965054\n",
      "152: 390.4558980539441 28.49078883137554 418.94668688531965\n",
      "153: 401.4206244535744 29.28459222242236 430.7052166759968\n",
      "154: 410.1003971397877 27.496427604928613 437.5968247447163\n",
      "155: 404.8542650863528 29.287790559232235 434.14205564558506\n",
      "156: 407.28913041204214 24.133577016182244 431.4227074282244\n",
      "157: 412.24661253392696 29.235784522723407 441.48239705665037\n",
      "158: 414.07002253085375 29.95818249741569 444.02820502826944\n",
      "159: 398.6736292205751 27.572120868600905 426.245750089176\n",
      "160: 400.3361992277205 26.71238963957876 427.04858886729926\n",
      "161: 409.29682379588485 27.325712035410106 436.62253583129495\n",
      "162: 398.00347493588924 27.19314479175955 425.1966197276488\n",
      "163: 407.5552162863314 26.585100525524467 434.1403168118559\n",
      "164: 387.50644908100367 26.55057544913143 414.0570245301351\n",
      "165: 402.6816183552146 26.852760551963 429.5343789071776\n",
      "166: 390.06464693695307 28.17135303840041 418.2359999753535\n",
      "167: 402.64239525794983 27.475871978327632 430.11826723627746\n",
      "168: 395.80708714574575 26.73909815819934 422.5461853039451\n",
      "169: 390.9982694461942 26.26768385898322 417.2659533051774\n",
      "170: 401.0779851414263 28.524727141484618 429.60271228291094\n",
      "171: 407.0764612555504 26.6706549692899 433.7471162248403\n",
      "172: 405.48627683520317 26.66933380626142 432.1556106414646\n",
      "173: 413.6603701338172 27.702727898955345 441.36309803277254\n",
      "174: 404.6733869537711 26.008170643355697 430.6815575971268\n",
      "175: 415.51831909269094 27.021164817735553 442.5394839104265\n",
      "176: 402.84568716958165 27.840826361440122 430.6865135310218\n",
      "177: 400.4121627546847 27.152205361053348 427.56436811573803\n",
      "178: 394.08966632932425 25.91248280974105 420.0021491390653\n",
      "179: 401.8665052950382 27.601691596210003 429.4681968912482\n",
      "180: 393.53568156808615 27.666629542596638 421.2023111106828\n",
      "181: 401.2907803542912 27.084833807311952 428.37561416160315\n",
      "182: 402.8186556994915 27.53043440170586 430.34909010119736\n",
      "183: 404.2475501149893 26.90944375563413 431.1569938706234\n",
      "184: 400.0749371498823 25.647135303355753 425.72207245323807\n",
      "185: 409.55605044960976 24.993803650140762 434.5498540997505\n",
      "186: 401.5242416039109 27.09757538419217 428.6218169881031\n",
      "187: 401.0093182101846 26.876864900812507 427.8861831109971\n",
      "188: 408.7732265740633 24.566209423355758 433.33943599741906\n",
      "189: 400.0866016894579 27.70189725421369 427.7884989436716\n",
      "190: 405.0485827848315 25.852205493021756 430.9007882778533\n",
      "191: 403.0251121073961 28.528322060592473 431.5534341679886\n",
      "192: 400.27883821725845 25.603468745946884 425.88230696320534\n",
      "193: 390.79139314591885 29.264276235364377 420.0556693812832\n",
      "194: 399.229595862329 27.238001494202763 426.46759735653177\n",
      "195: 395.6876057423651 26.31060215178877 421.9982078941539\n",
      "196: 392.6851352080703 27.500733505003154 420.18586871307343\n",
      "197: 393.4441280178726 25.461408478207886 418.90553649608046\n",
      "198: 391.277858203277 26.358288820832968 417.63614702410996\n",
      "199: 396.344667609781 27.96744745504111 424.31211506482214\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training\n",
    "\"\"\"\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.\n",
    "    epoch_mixup_loss = 0.\n",
    "    epoch_org_loss = 0.\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        mixup_inputs, labels, labels_b, lmbda = mixup_cifar10(inputs, labels, alpha)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(mixup_inputs)\n",
    "        mixup_loss = mixup_criterion(criterion, outputs, labels, labels_b, lmbda)\n",
    "        \n",
    "        ##\n",
    "        outputs_org = model(inputs)\n",
    "        loss_org = criterion(outputs_org, labels)\n",
    "        weighted_total_loss = mixup_loss * perturb_loss_weight + loss_org * (1 - perturb_loss_weight)\n",
    "        \n",
    "        epoch_mixup_loss += mixup_loss.item()\n",
    "        epoch_org_loss += loss_org.item()\n",
    "        \n",
    "        epoch_loss += (mixup_loss.item() + loss_org.item())\n",
    "        \n",
    "        weighted_total_loss.backward()\n",
    "        ##\n",
    "        \n",
    "        optimizer.step()\n",
    "    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_org_loss, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "frozen-damage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), './mixup_model_pytorch_cifar10_augment')\n",
    "model = models.__dict__['ResNet18']()\n",
    "model.load_state_dict(torch.load('./mixup_model_pytorch_cifar10_augment'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aboriginal-lafayette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9174\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        outputs = model(inputs)\n",
    "        _, predicts = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicts == labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "therapeutic-orlando",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9776\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        outputs = model(inputs)\n",
    "        _, predicts = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicts == labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-heavy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

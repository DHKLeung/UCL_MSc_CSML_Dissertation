{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "romantic-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "silver-clear",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e8efab04d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration and Hyperparameters\n",
    "\"\"\"\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),  # can omit\n",
    "    transforms.RandomHorizontalFlip(),  # can omit\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        (0.4914, 0.4822, 0.4465),\n",
    "        (0.2023, 0.1994, 0.2010)\n",
    "    )\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        (0.4914, 0.4822, 0.4465),\n",
    "        (0.2023, 0.1994, 0.2010)\n",
    "    )\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "step_size = 0.1\n",
    "random_seed = 0\n",
    "epochs = 200\n",
    "L2_decay = 1e-4\n",
    "gauss_vicinal_std = 0.25\n",
    "\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "relative-mobility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data\n",
    "\"\"\"\n",
    "train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "starting-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.__dict__['ResNet18']()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "attended-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_vicinal(inputs, gauss_vicinal_std):\n",
    "    inputs_gauss = torch.normal(inputs, gauss_vicinal_std)\n",
    "    return inputs_gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "written-bookmark",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1161.6744626760483 1161.4809753894806 2323.1554374694824\n",
      "1: 850.4431853294373 849.4572459459305 1699.9004335403442\n",
      "2: 764.6267102956772 764.16606092453 1528.7927684783936\n",
      "3: 720.5175825357437 719.9069302082062 1440.4245092868805\n",
      "4: 691.79572057724 691.005676984787 1382.8013954162598\n",
      "5: 671.4562151432037 670.320422410965 1341.7766382694244\n",
      "6: 650.6410090923309 649.0979682207108 1299.7389776706696\n",
      "7: 629.0471322536469 627.2152560949326 1256.2623903751373\n",
      "8: 597.5138113498688 595.733440041542 1193.2472529411316\n",
      "9: 576.4861139059067 574.5415465831757 1151.0276608467102\n",
      "10: 552.275195479393 549.5759927034378 1101.8511872291565\n",
      "11: 518.8602335453033 515.2573970556259 1034.1176326274872\n",
      "12: 500.80974209308624 496.4472051858902 997.2569470405579\n",
      "13: 444.09115368127823 437.46394473314285 881.5550981760025\n",
      "14: 403.1249688863754 395.17415457963943 798.2991243600845\n",
      "15: 372.9325177669525 364.1077842116356 737.0403020381927\n",
      "16: 347.1266422867775 336.9481421113014 684.074785232544\n",
      "17: 326.10256546735764 315.3052362203598 641.4078022241592\n",
      "18: 305.8157483935356 294.3255028128624 600.141252040863\n",
      "19: 293.24609273672104 281.56117457151413 574.8072680234909\n",
      "20: 277.82994851469994 265.4976164698601 543.3275647163391\n",
      "21: 266.12730622291565 252.5699617266655 518.6972677111626\n",
      "22: 253.63539934158325 239.6475499868393 493.2829491496086\n",
      "23: 245.0432389676571 231.3275682926178 476.3708068728447\n",
      "24: 231.52041095495224 217.64936566352844 449.1697759628296\n",
      "25: 222.5427320599556 208.05479514598846 430.59752690792084\n",
      "26: 215.67115572094917 199.75185722112656 415.4230127334595\n",
      "27: 207.4263350367546 192.04219734668732 399.4685329794884\n",
      "28: 202.2854346036911 185.8748612999916 388.1602963209152\n",
      "29: 196.2054934501648 179.6831910610199 375.8886846899986\n",
      "30: 188.37305814027786 171.24562910199165 359.6186874508858\n",
      "31: 182.6745158135891 166.5662826001644 349.2407990694046\n",
      "32: 178.77958557009697 162.10638581216335 340.8859706223011\n",
      "33: 175.25466430187225 157.90248942375183 333.1571537256241\n",
      "34: 168.26260554790497 150.82501596212387 319.0876215994358\n",
      "35: 163.2735187113285 146.54323269426823 309.8167514204979\n",
      "36: 159.80115818977356 141.73257198929787 301.53373035788536\n",
      "37: 156.70081934332848 139.00746016204357 295.70827889442444\n",
      "38: 153.17935492098331 135.20113794505596 288.38049349188805\n",
      "39: 148.1388859897852 128.73271311074495 276.8715990483761\n",
      "40: 145.7066731005907 126.50163772702217 272.2083107829094\n",
      "41: 142.03096988797188 121.86860340833664 263.8995735049248\n",
      "42: 139.0901020169258 119.04629163444042 258.13639375567436\n",
      "43: 135.25924982130527 116.29958102107048 251.5588303208351\n",
      "44: 130.9730166196823 110.58607871830463 241.55909490585327\n",
      "45: 131.34734062850475 110.86810870468616 242.2154498398304\n",
      "46: 128.44008213281631 108.22241961956024 236.66250151395798\n",
      "47: 124.84488818049431 104.79591700434685 229.64080581068993\n",
      "48: 123.62451223284006 103.19246388226748 226.81697621941566\n",
      "49: 120.41276954114437 100.50141072273254 220.91418054699898\n",
      "50: 117.38388648629189 97.57137954980135 214.95526668429375\n",
      "51: 116.69929058849812 96.07309921085835 212.7723897099495\n",
      "52: 116.25620305538177 95.47851001098752 211.7347133755684\n",
      "53: 112.3610129058361 91.74314066022635 204.10415410995483\n",
      "54: 110.98566503822803 89.86242561042309 200.84809036552906\n",
      "55: 108.49243141710758 87.83565543591976 196.328086912632\n",
      "56: 107.44046100229025 86.92281661182642 194.3632776737213\n",
      "57: 106.12204085290432 85.0925537571311 191.21459448337555\n",
      "58: 104.29474716633558 82.82910150289536 187.1238490641117\n",
      "59: 102.2790515050292 81.00944173336029 183.28849329054356\n",
      "60: 101.09080093353987 79.77971041202545 180.87051187455654\n",
      "61: 101.33894830942154 80.23717002570629 181.57611840963364\n",
      "62: 99.20271589607 78.34653493762016 177.54925125837326\n",
      "63: 98.6714437007904 77.31167333573103 175.98311702907085\n",
      "64: 95.32240671664476 74.79853815585375 170.12094512581825\n",
      "65: 94.26152543723583 73.52327561378479 167.78480115532875\n",
      "66: 97.21405796706676 75.93008717894554 173.14414554834366\n",
      "67: 95.61406227201223 74.73684447258711 170.35090735554695\n",
      "68: 92.41424012929201 71.32360062003136 163.7378407418728\n",
      "69: 92.75771380960941 71.57278806716204 164.33050227165222\n",
      "70: 89.43285290896893 69.37343582138419 158.80628876388073\n",
      "71: 88.89079014211893 68.97494007647038 157.86573040485382\n",
      "72: 86.77251409739256 66.06832130253315 152.84083554148674\n",
      "73: 87.91841008514166 66.52300336956978 154.44141349196434\n",
      "74: 88.11748659610748 68.05534601211548 156.17283281683922\n",
      "75: 86.30777475237846 66.20193613693118 152.50971096754074\n",
      "76: 88.47658501565456 67.21612340584397 155.69270826876163\n",
      "77: 84.02051915228367 62.88064298033714 146.9011623710394\n",
      "78: 87.18101831153035 65.42353767156601 152.60455606877804\n",
      "79: 81.94154572486877 61.67282821983099 143.61437426507473\n",
      "80: 80.65480749309063 61.073587477207184 141.7283952832222\n",
      "81: 82.18306853622198 62.149595871567726 144.33266453444958\n",
      "82: 81.52430253475904 60.36336062476039 141.8876629471779\n",
      "83: 78.09686580300331 58.64662127569318 136.74348683655262\n",
      "84: 80.74396786093712 60.07034266740084 140.81431052088737\n",
      "85: 80.83974306285381 60.14283699542284 140.9825802668929\n",
      "86: 77.81980907171965 56.919244572520256 134.73905350267887\n",
      "87: 76.4696538746357 56.749010398983955 133.21866427361965\n",
      "88: 77.87356805056334 58.02041072398424 135.89397883415222\n",
      "89: 77.5122273042798 56.5866122469306 134.09883977472782\n",
      "90: 77.63389437273145 56.63410619646311 134.26800045371056\n",
      "91: 76.27759182453156 55.8060487806797 132.08364063501358\n",
      "92: 78.53351493924856 58.72968254983425 137.26319739222527\n",
      "93: 75.31020506843925 55.449789337813854 130.7599944025278\n",
      "94: 77.79943915456533 56.44733785092831 134.24677712470293\n",
      "95: 87.62245846539736 64.97822128608823 152.6006799787283\n",
      "96: 74.08592065051198 54.41570219770074 128.5016229748726\n",
      "97: 74.08514528721571 53.93322631716728 128.0183715671301\n",
      "98: 74.47559650242329 53.682042475789785 128.1576385498047\n",
      "99: 73.05319009721279 53.94657485187054 126.9997647702694\n",
      "100: 72.91191761940718 52.92759161442518 125.83950918912888\n",
      "101: 71.08006140589714 50.45898362249136 121.53904453665018\n",
      "102: 72.58697916567326 53.076244950294495 125.66322408616543\n",
      "103: 71.22200645506382 51.46014556661248 122.68215180933475\n",
      "104: 72.271962672472 51.54921404272318 123.82117629796267\n",
      "105: 72.65316972136497 52.703715939074755 125.35688535869122\n",
      "106: 69.69342441856861 49.50307475402951 119.19649923592806\n",
      "107: 70.4175512753427 50.52382738515735 120.94137865304947\n",
      "108: 69.72617477923632 50.27229351550341 119.99846834689379\n",
      "109: 69.05564955249429 49.344278670847416 118.39992818236351\n",
      "110: 68.45759193226695 49.00269903615117 117.46029127389193\n",
      "111: 68.01480005681515 49.07105999812484 117.08586041629314\n",
      "112: 67.25011708214879 48.36689976230264 115.61701703071594\n",
      "113: 66.70795110985637 47.2666835449636 113.97463469952345\n",
      "114: 66.19955169409513 47.36818041279912 113.56773203611374\n",
      "115: 68.46726917847991 48.22099542990327 116.68826472014189\n",
      "116: 68.33077486604452 48.85158723965287 117.18236181885004\n",
      "117: 68.8702882565558 49.197777792811394 118.06806622445583\n",
      "118: 69.3008372373879 49.18065179511905 118.48148871958256\n",
      "119: 62.03411300852895 43.94082766585052 105.97494053840637\n",
      "120: 69.7595247477293 49.28824385255575 119.04776863753796\n",
      "121: 66.62331475317478 47.20990462601185 113.83321935683489\n",
      "122: 65.35719376057386 45.53089786320925 110.88809168338776\n",
      "123: 65.02120527997613 45.18673257343471 110.2079376578331\n",
      "124: 65.92881573736668 46.927356937900186 112.85617255046964\n",
      "125: 66.01213492453098 46.134154569357634 112.14628975093365\n",
      "126: 65.76271740719676 46.112400230020285 111.87511757016182\n",
      "127: 67.6155720949173 47.146775893867016 114.76234818994999\n",
      "128: 65.32022244483232 44.3756082393229 109.695830732584\n",
      "129: 61.6097578369081 42.832470355555415 104.44222844392061\n",
      "130: 65.76973764970899 45.7436242364347 111.51336207985878\n",
      "131: 63.4788467772305 43.796853832900524 107.27570102363825\n",
      "132: 62.866872917860746 43.43872909620404 106.30560176074505\n",
      "133: 65.21804651990533 46.11105287261307 111.32909967005253\n",
      "134: 60.201881755143404 42.48058141954243 102.68246292322874\n",
      "135: 61.4857337847352 42.147984962910414 103.63371876627207\n",
      "136: 64.9255932122469 44.71645359322429 109.64204659312963\n",
      "137: 63.9112728536129 44.5370515473187 108.44832430779934\n",
      "138: 62.35872381553054 43.17253427579999 105.53125811368227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139: 62.563542526215315 42.7742359302938 105.33777837455273\n",
      "140: 65.2666614279151 45.998181488364935 111.26484271138906\n",
      "141: 62.74090764671564 43.31332487612963 106.0542325079441\n",
      "142: 59.116795010864735 40.93722117319703 100.05401629954576\n",
      "143: 62.02079317346215 42.57574432902038 104.59653729945421\n",
      "144: 59.88389694690704 40.82175459899008 100.70565160363913\n",
      "145: 62.58954340219498 43.25301801599562 105.8425617069006\n",
      "146: 62.0268337354064 42.347330309450626 104.37416391074657\n",
      "147: 60.52692857384682 41.12804680131376 101.65497513860464\n",
      "148: 61.92218919843435 43.2915018722415 105.21369085460901\n",
      "149: 63.370417792350054 43.03940077871084 106.40981876850128\n",
      "150: 61.979921743273735 43.551486445590854 105.53140823543072\n",
      "151: 59.699112337082624 40.529290771111846 100.22840318828821\n",
      "152: 60.14089797437191 41.1532640773803 101.29416233301163\n",
      "153: 59.40754874423146 41.07672647200525 100.48427543789148\n",
      "154: 60.20600155740976 40.631647162139416 100.83764889091253\n",
      "155: 61.6229289509356 42.19437710382044 103.81730594485998\n",
      "156: 59.93170841783285 40.49224345199764 100.42395194619894\n",
      "157: 60.60909555479884 41.68557283654809 102.29466826468706\n",
      "158: 59.280728589743376 40.43264999240637 99.7133784815669\n",
      "159: 60.55117694661021 40.25711682625115 100.8082937002182\n",
      "160: 60.63511288166046 41.70367360673845 102.33878634124994\n",
      "161: 60.636253371834755 40.213305147364736 100.849558532238\n",
      "162: 58.09086788073182 38.923866556957364 97.01473439484835\n",
      "163: 59.992672234773636 40.7770640719682 100.76973623037338\n",
      "164: 59.629823945462704 40.82891659066081 100.45874056220055\n",
      "165: 58.16182341426611 39.727396968752146 97.88922043889761\n",
      "166: 55.963043093681335 37.68964214809239 93.65268521010876\n",
      "167: 60.05476748943329 40.18169227056205 100.23645962774754\n",
      "168: 57.34737651050091 38.14830893650651 95.49568542093039\n",
      "169: 60.16705248877406 40.48873405717313 100.65578684210777\n",
      "170: 58.57494942843914 39.27410155907273 97.84905092418194\n",
      "171: 58.69182734936476 39.525758761912584 98.21758619323373\n",
      "172: 56.79724660888314 37.932658610865474 94.72990513965487\n",
      "173: 58.77701783552766 39.97707122564316 98.75408893078566\n",
      "174: 55.33105752244592 37.528551341965795 92.8596086986363\n",
      "175: 58.86565251275897 39.193189000710845 98.05884145200253\n",
      "176: 56.62528384849429 37.64616636186838 94.27145021408796\n",
      "177: 57.76252098381519 39.05898283608258 96.82150376588106\n",
      "178: 57.73356443271041 38.739744298160076 96.47330907359719\n",
      "179: 57.834617760032415 39.3402280472219 97.17484574019909\n",
      "180: 57.78087885864079 39.18830007687211 96.969178840518\n",
      "181: 57.4945683516562 37.98417723365128 95.47874580696225\n",
      "182: 57.00293965451419 38.53724371641874 95.54018347337842\n",
      "183: 57.99432102590799 39.707108840346336 97.70142983645201\n",
      "184: 58.56022826209664 39.35785650461912 97.91808464750648\n",
      "185: 58.178463105112314 38.734457932412624 96.91292087733746\n",
      "186: 55.99127317778766 37.56659953389317 93.55787270888686\n",
      "187: 56.97711205109954 37.70423976518214 94.68135192245245\n",
      "188: 59.622903037816286 40.109426606446505 99.73232947289944\n",
      "189: 56.14476713910699 36.802436243742704 92.94720347225666\n",
      "190: 54.54254075139761 35.89020701125264 90.43274764716625\n",
      "191: 58.774895057082176 38.36022726073861 97.13512235879898\n",
      "192: 59.040432550013065 39.33294049091637 98.37337300181389\n",
      "193: 53.52309586107731 36.03151385486126 89.55460975319147\n",
      "194: 56.4943066239357 37.535183891654015 94.02949035167694\n",
      "195: 57.41078595072031 37.815500631928444 95.2262864857912\n",
      "196: 56.087107833474874 36.86932408250868 92.95643175393343\n",
      "197: 55.278781678527594 36.79929803498089 92.078079611063\n",
      "198: 56.06910514086485 37.57625966146588 93.64536485821009\n",
      "199: 53.35100922361016 35.83457893319428 89.18558829277754\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training\n",
    "\"\"\"\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.\n",
    "    epoch_gauss_loss = 0.\n",
    "    epoch_org_loss = 0.\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        inputs_gauss = gauss_vicinal(inputs, gauss_vicinal_std)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs_gauss)\n",
    "        \n",
    "        ##\n",
    "        gauss_loss = criterion(outputs, labels)\n",
    "        \n",
    "        outputs_org = model(inputs)\n",
    "        loss_org = criterion(outputs_org, labels)\n",
    "        total_loss = gauss_loss + loss_org\n",
    "        \n",
    "        epoch_gauss_loss += gauss_loss.item()\n",
    "        epoch_org_loss += loss_org.item()\n",
    "        \n",
    "        epoch_loss += total_loss.item()\n",
    "        total_loss.backward()\n",
    "        ##\n",
    "\n",
    "        optimizer.step()\n",
    "    print('{}: {} {} {}'.format(epoch, epoch_gauss_loss, epoch_org_loss, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "frozen-damage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), './gauss_model_pytorch_cifar10_augment')\n",
    "model = models.__dict__['ResNet18']()\n",
    "model.load_state_dict(torch.load('./gauss_model_pytorch_cifar10_augment'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aboriginal-lafayette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8887\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        outputs = model(inputs)\n",
    "        _, predicts = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicts == labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "therapeutic-orlando",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9714\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        outputs = model(inputs)\n",
    "        _, predicts = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicts == labels).sum().item()\n",
    "print(correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-michael",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN nb0.75 aug WPL0.25.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625274897179,"user_tz":-60,"elapsed":38985,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"1de20fbb-4f4e-4e54-8904-34647802812f"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625274898172,"user_tz":-60,"elapsed":995,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625274898517,"user_tz":-60,"elapsed":346,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625274898517,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"f173385b-a8ef-47b2-e294-76f0f589b0f1"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","geometric_param = 0.75\n","perturb_loss_weight = 0.25\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f5f14242a90>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625274898518,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625274905348,"user_tz":-60,"elapsed":6833,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"28eda07d-c220-4fed-8926-112125394730"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1625274905349,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"id":"quiet-module","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1625274905349,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625274910019,"user_tz":-60,"elapsed":4673,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"1eb48bba-6565-4809-841e-b85c96e068b5"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup with random neighbour perturbation #\n","        mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_breast_nb(inputs, labels, geometric_param, alpha)\n","        \n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_nb))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_nb = augment_outputs[original_num:]\n","        mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_nb + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_nb.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_nb.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print decomposed losses #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:151: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n","  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.133974552154541 2.131200909614563 4.265175461769104\n","1: 2.0986042618751526 2.0941107869148254 4.192715048789978\n","2: 2.0405324697494507 2.036980628967285 4.077513098716736\n","3: 1.9757307767868042 1.9654263257980347 3.941157102584839\n","4: 1.8881003260612488 1.884027361869812 3.772127687931061\n","5: 1.7974199056625366 1.7964527606964111 3.5938726663589478\n","6: 1.6986089944839478 1.699033796787262 3.3976427912712097\n","7: 1.5978111624717712 1.5981075167655945 3.1959186792373657\n","8: 1.4874131381511688 1.4889493584632874 2.976362496614456\n","9: 1.38067626953125 1.3830525577068329 2.763728827238083\n","10: 1.2527793645858765 1.259812355041504 2.5125917196273804\n","11: 1.1409783959388733 1.1508995592594147 2.291877955198288\n","12: 1.0332291722297668 1.0394657254219055 2.0726948976516724\n","13: 0.9101642966270447 0.9485308825969696 1.8586951792240143\n","14: 0.8106133341789246 0.8366410732269287 1.6472544074058533\n","15: 0.7293159514665604 0.7554156482219696 1.48473159968853\n","16: 0.6237537860870361 0.6791218817234039 1.30287566781044\n","17: 0.6016800403594971 0.628211185336113 1.22989122569561\n","18: 0.5453334003686905 0.5529426634311676 1.098276063799858\n","19: 0.5190597176551819 0.5337262749671936 1.0527859926223755\n","20: 0.4731139689683914 0.4839527904987335 0.9570667594671249\n","21: 0.3922770917415619 0.45208823680877686 0.8443653285503387\n","22: 0.47226645052433014 0.43910419940948486 0.911370649933815\n","23: 0.4175231158733368 0.38963768631219864 0.8071608021855354\n","24: 0.3997325599193573 0.3714568689465523 0.7711894288659096\n","25: 0.3971729800105095 0.366487555205822 0.7636605352163315\n","26: 0.3619610518217087 0.35522639751434326 0.7171874493360519\n","27: 0.3739333599805832 0.33901001513004303 0.7129433751106262\n","28: 0.3241255655884743 0.31096091866493225 0.6350864842534065\n","29: 0.32567835599184036 0.30579356104135513 0.6314719170331955\n","30: 0.32300112396478653 0.29083914309740067 0.6138402670621872\n","31: 0.3558562248945236 0.278609536588192 0.6344657614827156\n","32: 0.3278668522834778 0.2665024474263191 0.5943692997097969\n","33: 0.3141718655824661 0.2576357200741768 0.5718075856566429\n","34: 0.26628611981868744 0.24721452593803406 0.5135006457567215\n","35: 0.25649531185626984 0.24473952502012253 0.5012348368763924\n","36: 0.2809436321258545 0.24264204502105713 0.5235856771469116\n","37: 0.21795671433210373 0.2242502048611641 0.4422069191932678\n","38: 0.2767084613442421 0.22155184671282768 0.4982603080570698\n","39: 0.3065883368253708 0.2251202091574669 0.5317085459828377\n","40: 0.26978588104248047 0.2126874290406704 0.48247331008315086\n","41: 0.2647569552063942 0.2117726318538189 0.4765295870602131\n","42: 0.19922351837158203 0.20570328459143639 0.4049268029630184\n","43: 0.26332080364227295 0.21087856218218803 0.474199365824461\n","44: 0.2513336278498173 0.19599591940641403 0.4473295472562313\n","45: 0.2041373923420906 0.20340017974376678 0.4075375720858574\n","46: 0.21640560775995255 0.18811091035604477 0.4045165181159973\n","47: 0.27628689259290695 0.19777857512235641 0.47406546771526337\n","48: 0.23956501856446266 0.18949253484606743 0.4290575534105301\n","49: 0.2524655684828758 0.18338312208652496 0.4358486905694008\n","50: 0.19080987572669983 0.17008393816649914 0.36089381389319897\n","51: 0.2766745164990425 0.17257953062653542 0.4492540471255779\n","52: 0.24713103100657463 0.17051398009061813 0.41764501109719276\n","53: 0.2173256240785122 0.18078982084989548 0.39811544492840767\n","54: 0.18694030866026878 0.163762416690588 0.3507027253508568\n","55: 0.27651645243167877 0.1742761805653572 0.450792632997036\n","56: 0.2768063321709633 0.16765476763248444 0.4444610998034477\n","57: 0.30750449001789093 0.16648641973733902 0.47399090975522995\n","58: 0.22677508741617203 0.16592592000961304 0.39270100742578506\n","59: 0.21915405243635178 0.1649944670498371 0.3841485194861889\n","60: 0.22075404226779938 0.15737276524305344 0.3781268075108528\n","61: 0.19920365139842033 0.1514746155589819 0.35067826695740223\n","62: 0.18884315341711044 0.15190167725086212 0.34074483066797256\n","63: 0.2564435750246048 0.16320038214325905 0.41964395716786385\n","64: 0.2226148024201393 0.15533969551324844 0.37795449793338776\n","65: 0.22870630770921707 0.15845632925629616 0.38716263696551323\n","66: 0.21765302494168282 0.1565752625465393 0.3742282874882221\n","67: 0.2584766447544098 0.14359963312745094 0.40207627788186073\n","68: 0.2244461588561535 0.15119897946715355 0.37564513832330704\n","69: 0.23439037799835205 0.14222368970513344 0.3766140677034855\n","70: 0.25415829569101334 0.14976579882204533 0.40392409451305866\n","71: 0.26839637383818626 0.15315859206020832 0.4215549658983946\n","72: 0.1945117637515068 0.14523889124393463 0.33975065499544144\n","73: 0.20359592139720917 0.13595955446362495 0.3395554758608341\n","74: 0.23302694782614708 0.1533399559557438 0.38636690378189087\n","75: 0.18355270475149155 0.1327483430504799 0.31630104780197144\n","76: 0.217255350202322 0.150125864893198 0.36738121509552\n","77: 0.22012841701507568 0.14415980130434036 0.36428821831941605\n","78: 0.2301458790898323 0.13526508957147598 0.3654109686613083\n","79: 0.2442578449845314 0.1352680567651987 0.3795259017497301\n","80: 0.2812011167407036 0.12750471010804176 0.40870582684874535\n","81: 0.20267897471785545 0.12374647334218025 0.3264254480600357\n","82: 0.23586921393871307 0.12439840845763683 0.3602676223963499\n","83: 0.18585265800356865 0.13300372660160065 0.3188563846051693\n","84: 0.2479310855269432 0.12309234216809273 0.37102342769503593\n","85: 0.20335691794753075 0.12649162113666534 0.3298485390841961\n","86: 0.20581206306815147 0.1313707958906889 0.33718285895884037\n","87: 0.19940296560525894 0.13342534564435482 0.33282831124961376\n","88: 0.16832281276583672 0.12772838026285172 0.29605119302868843\n","89: 0.30947739630937576 0.12604619935154915 0.4355235956609249\n","90: 0.22726378589868546 0.11979952454566956 0.347063310444355\n","91: 0.19221311807632446 0.11979947611689568 0.31201259419322014\n","92: 0.24044083058834076 0.12129057571291924 0.36173140630126\n","93: 0.19199899397790432 0.12080288678407669 0.312801880761981\n","94: 0.18302137032151222 0.12072978913784027 0.3037511594593525\n","95: 0.23090169206261635 0.1290358006954193 0.35993749275803566\n","96: 0.2321685440838337 0.112397700548172 0.3445662446320057\n","97: 0.19993162155151367 0.11693727597594261 0.3168688975274563\n","98: 0.1822923682630062 0.116212148219347 0.2985045164823532\n","99: 0.1717579886317253 0.12104370072484016 0.2928016893565655\n","100: 0.18723540008068085 0.11247050389647484 0.2997059039771557\n","101: 0.22180885449051857 0.11403774283826351 0.3358465973287821\n","102: 0.205777695402503 0.12227306701242924 0.32805076241493225\n","103: 0.20105057209730148 0.1204110849648714 0.3214616570621729\n","104: 0.15849846974015236 0.1169731579720974 0.27547162771224976\n","105: 0.21344182640314102 0.11713310331106186 0.3305749297142029\n","106: 0.1807110719382763 0.11410991102457047 0.29482098296284676\n","107: 0.2072383239865303 0.1070332583039999 0.3142715822905302\n","108: 0.2342880181968212 0.11241140216588974 0.34669942036271095\n","109: 0.1777481995522976 0.11091870814561844 0.28866690769791603\n","110: 0.131230641156435 0.10555985197424889 0.2367904931306839\n","111: 0.16907037049531937 0.11359994672238827 0.28267031721770763\n","112: 0.19461309909820557 0.10661058872938156 0.3012236878275871\n","113: 0.2999379336833954 0.10764028504490852 0.4075782187283039\n","114: 0.26190055534243584 0.11326329782605171 0.37516385316848755\n","115: 0.2040914073586464 0.10576638951897621 0.3098577968776226\n","116: 0.24405360221862793 0.1031455509364605 0.3471991531550884\n","117: 0.22372720390558243 0.11075004190206528 0.3344772458076477\n","118: 0.2639438137412071 0.10506841167807579 0.3690122254192829\n","119: 0.16236070543527603 0.11590271443128586 0.2782634198665619\n","120: 0.1457812450826168 0.10784897953271866 0.25363022461533546\n","121: 0.15503321588039398 0.102918341755867 0.257951557636261\n","122: 0.17384672537446022 0.10221407189965248 0.2760607972741127\n","123: 0.20950733497738838 0.1001579649746418 0.3096652999520302\n","124: 0.19639142230153084 0.10859865508973598 0.3049900773912668\n","125: 0.25120168551802635 0.10129577852785587 0.3524974640458822\n","126: 0.1847584806382656 0.09708206169307232 0.28184054233133793\n","127: 0.19557014293968678 0.0998426079750061 0.2954127509146929\n","128: 0.26674462109804153 0.09735390543937683 0.36409852653741837\n","129: 0.2150484435260296 0.09520153142511845 0.31024997495114803\n","130: 0.22994788363575935 0.1066502146422863 0.33659809827804565\n","131: 0.275036983191967 0.09389417618513107 0.3689311593770981\n","132: 0.18776968866586685 0.0933692418038845 0.28113893046975136\n","133: 0.23911942541599274 0.09693138487637043 0.33605081029236317\n","134: 0.20376554876565933 0.09151476342231035 0.2952803121879697\n","135: 0.22404420003294945 0.09274122677743435 0.3167854268103838\n","136: 0.12274986691772938 0.09140482544898987 0.21415469236671925\n","137: 0.2903391718864441 0.09196035377681255 0.38229952566325665\n","138: 0.12251826003193855 0.0910197626799345 0.21353802271187305\n","139: 0.15457233972847462 0.08922940865159035 0.24380174838006496\n","140: 0.17321352660655975 0.09493138827383518 0.26814491488039494\n","141: 0.23446454852819443 0.09785827063024044 0.33232281915843487\n","142: 0.2058684080839157 0.09035169146955013 0.29622009955346584\n","143: 0.15705883875489235 0.08901573531329632 0.24607457406818867\n","144: 0.1881663091480732 0.08802547864615917 0.27619178779423237\n","145: 0.12763117626309395 0.09739180654287338 0.22502298280596733\n","146: 0.1948249638080597 0.09053697809576988 0.2853619419038296\n","147: 0.16490618884563446 0.08639546390622854 0.251301652751863\n","148: 0.16829099413007498 0.10322234593331814 0.2715133400633931\n","149: 0.20465390011668205 0.08755453117191792 0.29220843128859997\n","150: 0.17466679960489273 0.08718935772776604 0.26185615733265877\n","151: 0.21392111480236053 0.08703099191188812 0.30095210671424866\n","152: 0.21230024844408035 0.08940768241882324 0.3017079308629036\n","153: 0.19129947945475578 0.08941578306257725 0.28071526251733303\n","154: 0.19460687041282654 0.09214405063539743 0.286750921048224\n","155: 0.18853150308132172 0.08580566197633743 0.27433716505765915\n","156: 0.2614238001406193 0.10241327621042728 0.36383707635104656\n","157: 0.2181425839662552 0.08750546723604202 0.3056480512022972\n","158: 0.18656843155622482 0.09267932176589966 0.2792477533221245\n","159: 0.26024283468723297 0.10142799653112888 0.36167083121836185\n","160: 0.17606272548437119 0.08813201077282429 0.2641947362571955\n","161: 0.18786364793777466 0.10168066993355751 0.28954431787133217\n","162: 0.24338771402835846 0.0875362940132618 0.33092400804162025\n","163: 0.1968776434659958 0.08729472942650318 0.28417237289249897\n","164: 0.21213983744382858 0.08604041300714016 0.29818025045096874\n","165: 0.20576363056898117 0.09374434873461723 0.2995079793035984\n","166: 0.24648289382457733 0.0941434521228075 0.34062634594738483\n","167: 0.15960119292140007 0.08891405537724495 0.24851524829864502\n","168: 0.24341941624879837 0.0934552364051342 0.33687465265393257\n","169: 0.3205760568380356 0.0943235345184803 0.4148995913565159\n","170: 0.19390160217881203 0.09059720672667027 0.2844988089054823\n","171: 0.25502146035432816 0.09307803772389889 0.34809949807822704\n","172: 0.10867445729672909 0.08661949448287487 0.19529395177960396\n","173: 0.21711712330579758 0.0917708296328783 0.3088879529386759\n","174: 0.1999787613749504 0.08695201762020588 0.2869307789951563\n","175: 0.19300195202231407 0.08463681489229202 0.2776387669146061\n","176: 0.2629026994109154 0.09671846590936184 0.3596211653202772\n","177: 0.18828243017196655 0.08871589414775372 0.27699832431972027\n","178: 0.31011011451482773 0.08621174655854702 0.39632186107337475\n","179: 0.2365790531039238 0.0916263535618782 0.328205406665802\n","180: 0.15268222615122795 0.09426756948232651 0.24694979563355446\n","181: 0.2119972389191389 0.08981609158217907 0.301813330501318\n","182: 0.14267485961318016 0.08851370308548212 0.23118856269866228\n","183: 0.20796751976013184 0.08595173433423042 0.29391925409436226\n","184: 0.150307172909379 0.10125112067908049 0.2515582935884595\n","185: 0.27384139597415924 0.10631019249558449 0.38015158846974373\n","186: 0.25432737171649933 0.08786500804126263 0.34219237975776196\n","187: 0.18850941210985184 0.08897469006478786 0.2774841021746397\n","188: 0.18147670291364193 0.10275914706289768 0.2842358499765396\n","189: 0.21496496722102165 0.09687790460884571 0.31184287182986736\n","190: 0.26864293217658997 0.0886022336781025 0.35724516585469246\n","191: 0.18687137216329575 0.09638926759362221 0.28326063975691795\n","192: 0.2414812073111534 0.0839793523773551 0.3254605596885085\n","193: 0.226750448346138 0.08737526275217533 0.31412571109831333\n","194: 0.2860991321504116 0.08404510747641325 0.37014423962682486\n","195: 0.2031891793012619 0.08508526533842087 0.28827444463968277\n","196: 0.16611348092556 0.09079761244356632 0.2569110933691263\n","197: 0.20120129361748695 0.09605873003602028 0.29726002365350723\n","198: 0.19539256393909454 0.08141350792720914 0.2768060718663037\n","199: 0.16592219471931458 0.08847460150718689 0.25439679622650146\n","200: 0.19181736186146736 0.08537621982395649 0.27719358168542385\n","201: 0.26716621220111847 0.08453304320573807 0.35169925540685654\n","202: 0.17238174378871918 0.08546636626124382 0.257848110049963\n","203: 0.2062862478196621 0.092237020842731 0.2985232686623931\n","204: 0.24823402613401413 0.08822934702038765 0.3364633731544018\n","205: 0.2779509350657463 0.09041407145559788 0.3683650065213442\n","206: 0.15572553407400846 0.08286191616207361 0.23858745023608208\n","207: 0.24401938542723656 0.08452553115785122 0.3285449165850878\n","208: 0.2695476934313774 0.08531281165778637 0.3548605050891638\n","209: 0.13280954957008362 0.09161356277763844 0.22442311234772205\n","210: 0.16601328551769257 0.09225200675427914 0.2582652922719717\n","211: 0.2547273822128773 0.0958460196852684 0.3505734018981457\n","212: 0.17754627019166946 0.09117413498461246 0.26872040517628193\n","213: 0.16660933569073677 0.09406054206192493 0.2606698777526617\n","214: 0.20436264388263226 0.09257200919091702 0.29693465307354927\n","215: 0.1727963611483574 0.08434649370610714 0.25714285485446453\n","216: 0.31101570278406143 0.08318047411739826 0.3941961769014597\n","217: 0.24664880707859993 0.09279118664562702 0.33943999372422695\n","218: 0.15512230433523655 0.08679547347128391 0.24191777780652046\n","219: 0.21204660460352898 0.08894275315105915 0.3009893577545881\n","220: 0.17058411613106728 0.08627600967884064 0.2568601258099079\n","221: 0.22766108810901642 0.08451434597373009 0.3121754340827465\n","222: 0.17598704993724823 0.0910932645201683 0.26708031445741653\n","223: 0.17075348645448685 0.09328372031450272 0.26403720676898956\n","224: 0.176088098436594 0.09390505030751228 0.2699931487441063\n","225: 0.115105539560318 0.09180588275194168 0.20691142231225967\n","226: 0.18234661780297756 0.08583098836243153 0.2681776061654091\n","227: 0.2798490673303604 0.09310119226574898 0.3729502595961094\n","228: 0.23026374727487564 0.08239847887307405 0.3126622261479497\n","229: 0.27756451442837715 0.09015446528792381 0.36771897971630096\n","230: 0.20831778645515442 0.08721782825887203 0.29553561471402645\n","231: 0.13728422299027443 0.08611031621694565 0.22339453920722008\n","232: 0.19712870568037033 0.08621913008391857 0.2833478357642889\n","233: 0.21560930088162422 0.08364428952336311 0.29925359040498734\n","234: 0.15146271884441376 0.09528030641376972 0.24674302525818348\n","235: 0.16637403517961502 0.08064297726377845 0.24701701244339347\n","236: 0.12126228585839272 0.0876799076795578 0.20894219353795052\n","237: 0.1980677992105484 0.09885410219430923 0.29692190140485764\n","238: 0.19361376017332077 0.08281033392995596 0.27642409410327673\n","239: 0.10366283729672432 0.08283855207264423 0.18650138936936855\n","240: 0.2621660530567169 0.0922776460647583 0.3544436991214752\n","241: 0.1619195118546486 0.0838991068303585 0.2458186186850071\n","242: 0.2386317327618599 0.09940826147794724 0.33803999423980713\n","243: 0.2498929277062416 0.08312146551907063 0.33301439322531223\n","244: 0.1283582728356123 0.08296904433518648 0.21132731717079878\n","245: 0.19126862939447165 0.08013985166326165 0.2714084810577333\n","246: 0.10925441421568394 0.08268662914633751 0.19194104336202145\n","247: 0.285685945302248 0.08744573965668678 0.3731316849589348\n","248: 0.24586214870214462 0.08529097959399223 0.33115312829613686\n","249: 0.23459897935390472 0.0865936428308487 0.3211926221847534\n","250: 0.25640757381916046 0.08797475881874561 0.3443823326379061\n","251: 0.20301508903503418 0.08727014623582363 0.2902852352708578\n","252: 0.22542981803417206 0.09314396232366562 0.3185737803578377\n","253: 0.13597151823341846 0.08100308105349541 0.21697459928691387\n","254: 0.24707810953259468 0.08944665268063545 0.33652476221323013\n","255: 0.22425474599003792 0.08511973731219769 0.3093744833022356\n","256: 0.21291954815387726 0.08158369455486536 0.2945032427087426\n","257: 0.22733632661402225 0.08511265926063061 0.31244898587465286\n","258: 0.23526249825954437 0.08380798622965813 0.3190704844892025\n","259: 0.17510288581252098 0.08976735174655914 0.2648702375590801\n","260: 0.24424798227846622 0.08582135289907455 0.3300693351775408\n","261: 0.12264733202755451 0.09229890257120132 0.21494623459875584\n","262: 0.2036239430308342 0.08277341909706593 0.2863973621279001\n","263: 0.1497470811009407 0.0934236366301775 0.2431707177311182\n","264: 0.10288076102733612 0.0811183825135231 0.18399914354085922\n","265: 0.23116834461688995 0.08996299467980862 0.32113133929669857\n","266: 0.21770123485475779 0.0869887676090002 0.304690002463758\n","267: 0.12143328599631786 0.09498280007392168 0.21641608607023954\n","268: 0.2182021550834179 0.09043256007134914 0.30863471515476704\n","269: 0.15802600234746933 0.08954035304486752 0.24756635539233685\n","270: 0.21862995252013206 0.08312796242535114 0.3017579149454832\n","271: 0.2069064863026142 0.08357689529657364 0.29048338159918785\n","272: 0.27107636258006096 0.08880520798265934 0.3598815705627203\n","273: 0.18651696294546127 0.07956160698086023 0.2660785699263215\n","274: 0.22830230742692947 0.09940585680305958 0.32770816422998905\n","275: 0.17696093767881393 0.08790917135775089 0.2648701090365648\n","276: 0.25258126109838486 0.08321433886885643 0.3357955999672413\n","277: 0.13765208795666695 0.08903701230883598 0.22668910026550293\n","278: 0.216356186196208 0.08315339870750904 0.29950958490371704\n","279: 0.21133821457624435 0.08862465899437666 0.299962873570621\n","280: 0.1692100465297699 0.08231883123517036 0.25152887776494026\n","281: 0.1614331752061844 0.0804140493273735 0.2418472245335579\n","282: 0.20597822964191437 0.08450542390346527 0.29048365354537964\n","283: 0.23401226475834846 0.08226293884217739 0.31627520360052586\n","284: 0.23537014052271843 0.08020362071692944 0.31557376123964787\n","285: 0.2211514338850975 0.08060268126428127 0.3017541151493788\n","286: 0.2938715070486069 0.08316159807145596 0.37703310512006283\n","287: 0.22600679844617844 0.08631845377385616 0.3123252522200346\n","288: 0.23889167048037052 0.08781339600682259 0.3267050664871931\n","289: 0.12260062247514725 0.08855031803250313 0.21115094050765038\n","290: 0.2404099889099598 0.0816460382193327 0.3220560271292925\n","291: 0.1348772719502449 0.09230749309062958 0.22718476504087448\n","292: 0.22707593068480492 0.07851112727075815 0.30558705795556307\n","293: 0.17492205649614334 0.08317066356539726 0.2580927200615406\n","294: 0.4313480481505394 0.08795893751084805 0.5193069856613874\n","295: 0.09508317708969116 0.08592331688851118 0.18100649397820234\n","296: 0.18408162891864777 0.08307855017483234 0.2671601790934801\n","297: 0.24283570796251297 0.08541095443069935 0.3282466623932123\n","298: 0.1687888726592064 0.0814225822687149 0.2502114549279213\n","299: 0.25263654440641403 0.08441860973834991 0.33705515414476395\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625274910020,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625274910375,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"c757507c-2bb0-481f-90d9-0893d0a1c483"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9605263157894737\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625274910375,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"3beba503-ce4c-4e2e-fe8d-626e53b9672e"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9912023460410557\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625274910375,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":12,"outputs":[]}]}
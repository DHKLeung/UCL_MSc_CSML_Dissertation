{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN nb0.5 aug WPL0.25.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625274457250,"user_tz":-60,"elapsed":25293,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"4c6f3446-1903-477d-a49b-cc28bc968da1"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625274458116,"user_tz":-60,"elapsed":868,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625274458484,"user_tz":-60,"elapsed":369,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625274458485,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"ece12ac8-e8c7-4867-f6ac-96c5fd1671fb"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","geometric_param = 0.5\n","perturb_loss_weight = 0.25\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f921c64aa70>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625274458485,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625274465564,"user_tz":-60,"elapsed":7081,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"4d8214ab-e31e-4185-9280-7b2afb498c84"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1625274465565,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"id":"quiet-module","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1625274465565,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625274468949,"user_tz":-60,"elapsed":3387,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"39694518-48ff-4c85-de5e-74fd6ecf2a44"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup with random neighbour perturbation #\n","        mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_breast_nb(inputs, labels, geometric_param, alpha)\n","        \n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_nb))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_nb = augment_outputs[original_num:]\n","        mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_nb + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_nb.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_nb.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print decomposed losses #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:151: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n","  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.1343568563461304 2.130814552307129 4.265171408653259\n","1: 2.104564845561981 2.0978519916534424 4.202416837215424\n","2: 2.0434651970863342 2.0411052107810974 4.084570407867432\n","3: 1.9689775109291077 1.9709945917129517 3.9399721026420593\n","4: 1.8921749591827393 1.8910199403762817 3.783194899559021\n","5: 1.8053032159805298 1.8042911291122437 3.6095943450927734\n","6: 1.7028751969337463 1.7028754949569702 3.4057506918907166\n","7: 1.605056881904602 1.6067581176757812 3.2118149995803833\n","8: 1.497049868106842 1.5009614825248718 2.998011350631714\n","9: 1.3777694404125214 1.3873117864131927 2.765081226825714\n","10: 1.244436889886856 1.267717570066452 2.512154459953308\n","11: 1.1588716804981232 1.1630695760250092 2.3219412565231323\n","12: 1.0274676084518433 1.0537925958633423 2.0812602043151855\n","13: 0.9410691261291504 0.9619193375110626 1.902988463640213\n","14: 0.8732361793518066 0.8639712929725647 1.7372074723243713\n","15: 0.7758279144763947 0.778933048248291 1.5547609627246857\n","16: 0.6461541056632996 0.6888302266597748 1.3349843323230743\n","17: 0.6410896629095078 0.6636906564235687 1.3047803193330765\n","18: 0.592502698302269 0.6018089354038239 1.1943116337060928\n","19: 0.5460474938154221 0.5450146943330765 1.0910621881484985\n","20: 0.5184520781040192 0.540832445025444 1.0592845231294632\n","21: 0.4407775551080704 0.48000742495059967 0.92078498005867\n","22: 0.47960856556892395 0.4666862189769745 0.9462947845458984\n","23: 0.4412767142057419 0.42807841300964355 0.8693551272153854\n","24: 0.4269603043794632 0.4066211208701134 0.8335814252495766\n","25: 0.3930486738681793 0.38717561960220337 0.7802242934703827\n","26: 0.365920715034008 0.3567712754011154 0.7226919904351234\n","27: 0.34757304936647415 0.3468647748231888 0.6944378241896629\n","28: 0.3460204601287842 0.3385654389858246 0.6845858991146088\n","29: 0.36284495145082474 0.3092183619737625 0.6720633134245872\n","30: 0.35010387748479843 0.3162536174058914 0.6663574948906898\n","31: 0.2907124161720276 0.29725416004657745 0.587966576218605\n","32: 0.28830574452877045 0.2856007143855095 0.5739064589142799\n","33: 0.30836792290210724 0.26991524174809456 0.5782831646502018\n","34: 0.29537829756736755 0.2871510088443756 0.5825293064117432\n","35: 0.2781287878751755 0.2809455171227455 0.559074304997921\n","36: 0.28745531290769577 0.26302721351385117 0.5504825264215469\n","37: 0.26691021770238876 0.24663277715444565 0.5135429948568344\n","38: 0.27666953951120377 0.26988352090120316 0.5465530604124069\n","39: 0.2805255055427551 0.24414034187793732 0.5246658474206924\n","40: 0.3007861226797104 0.25435570627450943 0.5551418289542198\n","41: 0.3379562571644783 0.2510262057185173 0.5889824628829956\n","42: 0.24990610778331757 0.23012922331690788 0.48003533110022545\n","43: 0.3008584789931774 0.21964898332953453 0.520507462322712\n","44: 0.30368898808956146 0.2243148684501648 0.5280038565397263\n","45: 0.27915333211421967 0.21920128166675568 0.49835461378097534\n","46: 0.24307222664356232 0.20860958099365234 0.45168180763721466\n","47: 0.26863913610577583 0.2168070413172245 0.48544617742300034\n","48: 0.24160191044211388 0.2126164361834526 0.4542183466255665\n","49: 0.30502116680145264 0.22865328192710876 0.5336744487285614\n","50: 0.2130090817809105 0.19591674581170082 0.4089258275926113\n","51: 0.31620118767023087 0.21689749136567116 0.533098679035902\n","52: 0.2954845130443573 0.20643851533532143 0.5019230283796787\n","53: 0.21243080869317055 0.19046228379011154 0.4028930924832821\n","54: 0.2495231181383133 0.2116919532418251 0.4612150713801384\n","55: 0.27724960446357727 0.1954617127776146 0.47271131724119186\n","56: 0.2602068893611431 0.18477297574281693 0.44497986510396004\n","57: 0.23282300680875778 0.21117940545082092 0.4440024122595787\n","58: 0.18101278692483902 0.18663730844855309 0.3676500953733921\n","59: 0.25110192969441414 0.1802782490849495 0.43138017877936363\n","60: 0.2622176259756088 0.1832623966038227 0.44548002257943153\n","61: 0.2853569760918617 0.17689943686127663 0.46225641295313835\n","62: 0.20838052034378052 0.18342242389917374 0.39180294424295425\n","63: 0.21905925497412682 0.1825026236474514 0.4015618786215782\n","64: 0.21791903674602509 0.1925285905599594 0.4104476273059845\n","65: 0.21729125082492828 0.19706135615706444 0.4143526069819927\n","66: 0.24705538153648376 0.1811739280819893 0.42822930961847305\n","67: 0.27644141763448715 0.16711740009486675 0.4435588177293539\n","68: 0.21738237887620926 0.17028163373470306 0.3876640126109123\n","69: 0.2677270621061325 0.18962695449590683 0.45735401660203934\n","70: 0.2763562723994255 0.17767301946878433 0.45402929186820984\n","71: 0.22770965099334717 0.1658359356224537 0.39354558661580086\n","72: 0.30613745748996735 0.1685108244419098 0.47464828193187714\n","73: 0.23890666663646698 0.1686742678284645 0.4075809344649315\n","74: 0.22908227890729904 0.167202390730381 0.39628466963768005\n","75: 0.14609304070472717 0.16557462513446808 0.31166766583919525\n","76: 0.23118145391345024 0.1624794490635395 0.39366090297698975\n","77: 0.21496275439858437 0.16478871554136276 0.37975146993994713\n","78: 0.21313155442476273 0.157378900796175 0.37051045522093773\n","79: 0.24885406345129013 0.15674574300646782 0.40559980645775795\n","80: 0.244073323905468 0.15743860602378845 0.40151192992925644\n","81: 0.19161060452461243 0.18147815018892288 0.3730887547135353\n","82: 0.274153508245945 0.16309558972716331 0.4372490979731083\n","83: 0.2956558167934418 0.15846728906035423 0.454123105853796\n","84: 0.26008912175893784 0.1607796512544155 0.42086877301335335\n","85: 0.22160426154732704 0.15418210998177528 0.3757863715291023\n","86: 0.20633868128061295 0.17428703606128693 0.38062571734189987\n","87: 0.2313987985253334 0.15707632899284363 0.38847512751817703\n","88: 0.22482432797551155 0.1742616854608059 0.39908601343631744\n","89: 0.29175325855612755 0.17183013446629047 0.463583393022418\n","90: 0.2647410221397877 0.18265953846275806 0.44740056060254574\n","91: 0.2512415796518326 0.17447722516953945 0.42571880482137203\n","92: 0.2988913469016552 0.14387565106153488 0.4427669979631901\n","93: 0.22766386717557907 0.15164533257484436 0.37930919975042343\n","94: 0.24060706049203873 0.1475954782217741 0.38820253871381283\n","95: 0.26026030257344246 0.14707682467997074 0.4073371272534132\n","96: 0.2548948898911476 0.14390564523637295 0.39880053512752056\n","97: 0.22111143544316292 0.1471559777855873 0.36826741322875023\n","98: 0.22927190363407135 0.1583012007176876 0.38757310435175896\n","99: 0.20836955308914185 0.14650901407003403 0.3548785671591759\n","100: 0.22804610803723335 0.16089242696762085 0.3889385350048542\n","101: 0.22334172390401363 0.14497246220707893 0.36831418611109257\n","102: 0.15549790114164352 0.16803088039159775 0.32352878153324127\n","103: 0.19807783886790276 0.14358743466436863 0.3416652735322714\n","104: 0.1752631701529026 0.14552763104438782 0.3207908011972904\n","105: 0.24225833266973495 0.1472400426864624 0.38949837535619736\n","106: 0.17049770802259445 0.13963714614510536 0.3101348541676998\n","107: 0.24044597148895264 0.17216315492987633 0.41260912641882896\n","108: 0.27443210780620575 0.16147782653570175 0.4359099343419075\n","109: 0.3518778532743454 0.140258451923728 0.4921363051980734\n","110: 0.13587713614106178 0.14408033154904842 0.2799574676901102\n","111: 0.23852445930242538 0.13594220951199532 0.3744666688144207\n","112: 0.21370982378721237 0.1423179879784584 0.3560278117656708\n","113: 0.24731796234846115 0.1441710814833641 0.39148904383182526\n","114: 0.2354340460151434 0.1496733110398054 0.3851073570549488\n","115: 0.30870433896780014 0.144050482660532 0.45275482162833214\n","116: 0.22085047140717506 0.13259209226816893 0.353442563675344\n","117: 0.22114552929997444 0.15454744547605515 0.3756929747760296\n","118: 0.23618211224675179 0.1339407879859209 0.3701229002326727\n","119: 0.2850594110786915 0.13824391551315784 0.4233033265918493\n","120: 0.16428252309560776 0.13694357685744762 0.3012260999530554\n","121: 0.21178723871707916 0.1630843859165907 0.37487162463366985\n","122: 0.14224876835942268 0.13243683613836765 0.27468560449779034\n","123: 0.19106906838715076 0.13697084598243237 0.32803991436958313\n","124: 0.29078028723597527 0.1316936444491148 0.42247393168509007\n","125: 0.31318444944918156 0.14103636518120766 0.4542208146303892\n","126: 0.18292826041579247 0.14238768070936203 0.3253159411251545\n","127: 0.19191867113113403 0.15432950481772423 0.34624817594885826\n","128: 0.24583209678530693 0.14525660127401352 0.39108869805932045\n","129: 0.2658971883356571 0.15197923220694065 0.41787642054259777\n","130: 0.22831303626298904 0.13234550692141056 0.3606585431843996\n","131: 0.2803011015057564 0.16161923296749592 0.4419203344732523\n","132: 0.24168825149536133 0.134409599006176 0.3760978505015373\n","133: 0.27836059406399727 0.15926739014685154 0.4376279842108488\n","134: 0.2530091926455498 0.13237654604017735 0.3853857386857271\n","135: 0.3279886394739151 0.15830961801111698 0.4862982574850321\n","136: 0.19210932031273842 0.15030007250607014 0.34240939281880856\n","137: 0.2524573914706707 0.160515652038157 0.4129730435088277\n","138: 0.13814214430749416 0.1582650225609541 0.29640716686844826\n","139: 0.3082668259739876 0.13944732025265694 0.4477141462266445\n","140: 0.21195552498102188 0.12853906117379665 0.34049458615481853\n","141: 0.28666335344314575 0.13054236955940723 0.417205723002553\n","142: 0.21734251454472542 0.12896156683564186 0.3463040813803673\n","143: 0.18899234756827354 0.1293471772223711 0.31833952479064465\n","144: 0.2540011890232563 0.1501060239970684 0.4041072130203247\n","145: 0.20430687069892883 0.1560343112796545 0.36034118197858334\n","146: 0.23657619580626488 0.15688766166567802 0.3934638574719429\n","147: 0.25536442175507545 0.15602010861039162 0.41138453036546707\n","148: 0.20743687264621258 0.12450631149113178 0.33194318413734436\n","149: 0.23227540403604507 0.13721568509936333 0.3694910891354084\n","150: 0.21234023571014404 0.1526593081653118 0.36499954387545586\n","151: 0.20194778218865395 0.13224752061069012 0.33419530279934406\n","152: 0.3310091495513916 0.1262489315122366 0.4572580810636282\n","153: 0.22987785935401917 0.13121771439909935 0.3610955737531185\n","154: 0.2629507929086685 0.12756254523992538 0.3905133381485939\n","155: 0.23243245482444763 0.15959835052490234 0.39203080534935\n","156: 0.2339250147342682 0.13208403438329697 0.36600904911756516\n","157: 0.2969261296093464 0.14633395709097385 0.44326008670032024\n","158: 0.24955898709595203 0.12840323336422443 0.37796222046017647\n","159: 0.22482829168438911 0.12958847172558308 0.3544167634099722\n","160: 0.21087650954723358 0.1319001354277134 0.342776644974947\n","161: 0.1972690112888813 0.13115838915109634 0.32842740043997765\n","162: 0.23087207227945328 0.14514241740107536 0.37601448968052864\n","163: 0.27011082135140896 0.12578309513628483 0.3958939164876938\n","164: 0.24392854794859886 0.1486344989389181 0.392563046887517\n","165: 0.2847169041633606 0.14734364673495293 0.4320605508983135\n","166: 0.21600551158189774 0.12385343015193939 0.3398589417338371\n","167: 0.2116481065750122 0.13627239502966404 0.34792050160467625\n","168: 0.20330652594566345 0.12603899836540222 0.3293455243110657\n","169: 0.21781796216964722 0.12724300287663937 0.3450609650462866\n","170: 0.16855183988809586 0.15075741708278656 0.3193092569708824\n","171: 0.20458394661545753 0.15216470882296562 0.35674865543842316\n","172: 0.23537562415003777 0.12476333323866129 0.36013895738869905\n","173: 0.3126356303691864 0.13732216507196426 0.44995779544115067\n","174: 0.2382124960422516 0.1266553569585085 0.3648678530007601\n","175: 0.3304620832204819 0.13123542815446854 0.4616975113749504\n","176: 0.215464748442173 0.1462260577827692 0.3616908062249422\n","177: 0.18174710124731064 0.12586499191820621 0.30761209316551685\n","178: 0.3410656973719597 0.1265660785138607 0.4676317758858204\n","179: 0.2377558797597885 0.15254963003098965 0.39030550979077816\n","180: 0.1812058798968792 0.13576623611152172 0.3169721160084009\n","181: 0.258124016225338 0.1264143493026495 0.3845383655279875\n","182: 0.2275100164115429 0.13487081974744797 0.36238083615899086\n","183: 0.2719694674015045 0.12799524888396263 0.39996471628546715\n","184: 0.15683656930923462 0.13015578873455524 0.28699235804378986\n","185: 0.2548868991434574 0.13162038289010525 0.38650728203356266\n","186: 0.28051579743623734 0.1229747636243701 0.40349056106060743\n","187: 0.23804159089922905 0.14768406003713608 0.3857256509363651\n","188: 0.08872116543352604 0.1503893230110407 0.23911048844456673\n","189: 0.22457719594240189 0.1251740325242281 0.34975122846663\n","190: 0.18460670113563538 0.12623296305537224 0.3108396641910076\n","191: 0.2821952626109123 0.13158269971609116 0.4137779623270035\n","192: 0.17727301642298698 0.12809370830655098 0.30536672472953796\n","193: 0.2444445639848709 0.12561825662851334 0.37006282061338425\n","194: 0.3369344621896744 0.14464443176984787 0.48157889395952225\n","195: 0.20645837485790253 0.1462798323482275 0.35273820720613003\n","196: 0.2373027801513672 0.13963675871491432 0.3769395388662815\n","197: 0.2083563320338726 0.13788530603051186 0.34624163806438446\n","198: 0.2161286473274231 0.1261328086256981 0.3422614559531212\n","199: 0.16984188929200172 0.12496365793049335 0.2948055472224951\n","200: 0.1419365182518959 0.15125742182135582 0.2931939400732517\n","201: 0.20151996240019798 0.12653088755905628 0.32805084995925426\n","202: 0.21320869401097298 0.1376532670110464 0.3508619610220194\n","203: 0.23323778808116913 0.13008666411042213 0.36332445219159126\n","204: 0.25874628871679306 0.1486114002764225 0.40735768899321556\n","205: 0.2118629887700081 0.12863032706081867 0.34049331583082676\n","206: 0.19606824219226837 0.12495754472911358 0.32102578692138195\n","207: 0.28688162565231323 0.14596351608633995 0.4328451417386532\n","208: 0.2819897457957268 0.12218527868390083 0.4041750244796276\n","209: 0.2623311933130026 0.1265831459313631 0.3889143392443657\n","210: 0.21044546738266945 0.12874941527843475 0.3391948826611042\n","211: 0.15478549152612686 0.1331078466027975 0.28789333812892437\n","212: 0.23830898106098175 0.15058814361691475 0.3888971246778965\n","213: 0.14085180684924126 0.14939765259623528 0.29024945944547653\n","214: 0.30161909759044647 0.12981510162353516 0.43143419921398163\n","215: 0.1923239752650261 0.1263189185410738 0.3186428938060999\n","216: 0.22479328513145447 0.1300351247191429 0.3548284098505974\n","217: 0.24393073469400406 0.14754433929920197 0.391475073993206\n","218: 0.16600962728261948 0.127656577154994 0.2936662044376135\n","219: 0.20582884550094604 0.1265087816864252 0.33233762718737125\n","220: 0.2120954878628254 0.12234428711235523 0.3344397749751806\n","221: 0.2606770694255829 0.13336533680558205 0.39404240623116493\n","222: 0.21911892294883728 0.12626533396542072 0.345384256914258\n","223: 0.22728373110294342 0.1453939639031887 0.3726776950061321\n","224: 0.2389010712504387 0.12621959671378136 0.36512066796422005\n","225: 0.16168799623847008 0.12395397759974003 0.2856419738382101\n","226: 0.20892396569252014 0.13359862379729748 0.3425225894898176\n","227: 0.26218704879283905 0.12499172054231167 0.3871787693351507\n","228: 0.2589621767401695 0.14432512409985065 0.4032873008400202\n","229: 0.3011421971023083 0.13067522458732128 0.43181742168962955\n","230: 0.278751939535141 0.13280699215829372 0.4115589316934347\n","231: 0.18332137167453766 0.14730761013925076 0.3306289818137884\n","232: 0.18653295561671257 0.12737001292407513 0.3139029685407877\n","233: 0.2520056553184986 0.12820135802030563 0.38020701333880424\n","234: 0.17495404556393623 0.1349678561091423 0.30992190167307854\n","235: 0.14775565639138222 0.12891347706317902 0.27666913345456123\n","236: 0.23250373266637325 0.1267106831073761 0.35921441577374935\n","237: 0.23028668016195297 0.12643704377114773 0.3567237239331007\n","238: 0.19257629290223122 0.12241120729595423 0.31498750019818544\n","239: 0.23048143461346626 0.13056612946093082 0.3610475640743971\n","240: 0.23585517704486847 0.12459189631044865 0.3604470733553171\n","241: 0.3257913626730442 0.14327760599553585 0.46906896866858006\n","242: 0.20980369858443737 0.13399014621973038 0.34379384480416775\n","243: 0.18884678557515144 0.12240150664001703 0.3112482922151685\n","244: 0.2322484664618969 0.1376700010150671 0.369918467476964\n","245: 0.17856907099485397 0.12558626383543015 0.3041553348302841\n","246: 0.16404486820101738 0.1347210593521595 0.2987659275531769\n","247: 0.33116644620895386 0.12300231866538525 0.4541687648743391\n","248: 0.25883054733276367 0.12495675310492516 0.3837873004376888\n","249: 0.279133178293705 0.15348614566028118 0.43261932395398617\n","250: 0.22439675778150558 0.12599210068583488 0.35038885846734047\n","251: 0.26135239750146866 0.12257999647408724 0.3839323939755559\n","252: 0.254343718290329 0.13201534748077393 0.3863590657711029\n","253: 0.17408034950494766 0.1349660623818636 0.30904641188681126\n","254: 0.2015904113650322 0.15394818782806396 0.35553859919309616\n","255: 0.32100261747837067 0.13455330207943916 0.45555591955780983\n","256: 0.21969519183039665 0.1259953510016203 0.34569054283201694\n","257: 0.2181316651403904 0.1240030974149704 0.3421347625553608\n","258: 0.21034860238432884 0.1204058462753892 0.33075444865971804\n","259: 0.20195794850587845 0.13063909485936165 0.3325970433652401\n","260: 0.3220592774450779 0.1548927016556263 0.4769519791007042\n","261: 0.14433124661445618 0.12347569316625595 0.2678069397807121\n","262: 0.2357601523399353 0.12657123617827892 0.3623313885182142\n","263: 0.21022944524884224 0.1451474018394947 0.35537684708833694\n","264: 0.14311876147985458 0.1241444107145071 0.2672631721943617\n","265: 0.19253501296043396 0.12580150738358498 0.31833652034401894\n","266: 0.23848066851496696 0.12350921332836151 0.3619898818433285\n","267: 0.17277598194777966 0.1257649715989828 0.29854095354676247\n","268: 0.2948080822825432 0.1458931714296341 0.4407012537121773\n","269: 0.2774818427860737 0.12264730595052242 0.4001291487365961\n","270: 0.27449436485767365 0.12605211324989796 0.4005464781075716\n","271: 0.33840223401784897 0.13363034464418888 0.47203257866203785\n","272: 0.24804266169667244 0.12879498302936554 0.376837644726038\n","273: 0.19438767805695534 0.12685772590339184 0.3212454039603472\n","274: 0.2735481560230255 0.14279880188405514 0.41634695790708065\n","275: 0.22563683986663818 0.14537810906767845 0.37101494893431664\n","276: 0.22641154006123543 0.12853439524769783 0.35494593530893326\n","277: 0.14845559187233448 0.14745766296982765 0.29591325484216213\n","278: 0.25937635637819767 0.14762305282056332 0.406999409198761\n","279: 0.3320258781313896 0.12166489940136671 0.45369077753275633\n","280: 0.16712675150483847 0.12141622044146061 0.2885429719462991\n","281: 0.10615610331296921 0.12291944958269596 0.22907555289566517\n","282: 0.18962663784623146 0.12454576604068279 0.31417240388691425\n","283: 0.20907030627131462 0.12353809550404549 0.3326084017753601\n","284: 0.22133544459939003 0.13701889850199223 0.35835434310138226\n","285: 0.28761791810393333 0.14542202278971672 0.43303994089365005\n","286: 0.2034727856516838 0.14484289661049843 0.34831568226218224\n","287: 0.18164124712347984 0.12972690165042877 0.3113681487739086\n","288: 0.22405541315674782 0.1230868473649025 0.3471422605216503\n","289: 0.1802911013364792 0.1308642067015171 0.3111553080379963\n","290: 0.28436364233493805 0.1304296925663948 0.41479333490133286\n","291: 0.14375123754143715 0.13028622418642044 0.2740374617278576\n","292: 0.30932118743658066 0.14343813806772232 0.452759325504303\n","293: 0.24472377821803093 0.15233357436954975 0.3970573525875807\n","294: 0.2272278480231762 0.12326118536293507 0.35048903338611126\n","295: 0.18203772231936455 0.15163524635136127 0.3336729686707258\n","296: 0.13635532185435295 0.14407732710242271 0.28043264895677567\n","297: 0.31351398676633835 0.12706689536571503 0.4405808821320534\n","298: 0.23591076955199242 0.12365388125181198 0.3595646508038044\n","299: 0.20626281946897507 0.14594725333154202 0.3522100728005171\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625274468949,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625274468950,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"819e2afd-7319-4ac9-fd28-001876ef55ed"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9649122807017544\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625274469303,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"613902e8-aff4-41de-e420-ddeae0dbe210"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9941348973607038\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625274469303,"user_tz":-60,"elapsed":2,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":12,"outputs":[]}]}
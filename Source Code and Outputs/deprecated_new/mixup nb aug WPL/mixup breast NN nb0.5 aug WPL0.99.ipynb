{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN nb0.5 aug WPL0.99.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625274635624,"user_tz":-60,"elapsed":27478,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"557cd0c5-989f-4b1d-b322-c90175677b35"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625274636094,"user_tz":-60,"elapsed":472,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625274636375,"user_tz":-60,"elapsed":290,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625274636375,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"09d8f485-57ea-44dd-f27f-9ce20c0e9007"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","geometric_param = 0.5\n","perturb_loss_weight = 0.99\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f94f2671a50>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625274636376,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625274643135,"user_tz":-60,"elapsed":6762,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"329f67e1-7aed-4b27-de90-1048e31bf22a"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1625274643136,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"id":"quiet-module","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1625274643136,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625274646786,"user_tz":-60,"elapsed":3653,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"5e8091ad-1f2f-4f28-9bb1-a36f660dd0bc"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup with random neighbour perturbation #\n","        mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_breast_nb(inputs, labels, geometric_param, alpha)\n","        \n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_nb))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_nb = augment_outputs[original_num:]\n","        mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_nb + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_nb.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_nb.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print decomposed losses #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:151: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n","  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.1368529200553894 2.13209730386734 4.2689502239227295\n","1: 2.105747938156128 2.0972559452056885 4.203003883361816\n","2: 2.0433067679405212 2.0422208309173584 4.08552759885788\n","3: 1.9695347547531128 1.9674239754676819 3.9369587302207947\n","4: 1.8879019021987915 1.8843719959259033 3.772273898124695\n","5: 1.8002915382385254 1.7954825162887573 3.5957740545272827\n","6: 1.6983575224876404 1.699646532535553 3.3980040550231934\n","7: 1.587097406387329 1.5938002467155457 3.1808976531028748\n","8: 1.4782319962978363 1.4861015975475311 2.9643335938453674\n","9: 1.3790322840213776 1.3805201053619385 2.759552389383316\n","10: 1.2448890209197998 1.2556446492671967 2.5005336701869965\n","11: 1.135371446609497 1.1508201956748962 2.2861916422843933\n","12: 1.0139789283275604 1.0361024141311646 2.050081342458725\n","13: 0.8971425294876099 0.928803414106369 1.8259459435939789\n","14: 0.8514749705791473 0.8466672897338867 1.698142260313034\n","15: 0.765037477016449 0.7733787894248962 1.5384162664413452\n","16: 0.7103687971830368 0.6845524162054062 1.394921213388443\n","17: 0.6831871271133423 0.6577804237604141 1.3409675508737564\n","18: 0.5816211551427841 0.5819125473499298 1.163533702492714\n","19: 0.5789188593626022 0.5484681874513626 1.1273870468139648\n","20: 0.5655409097671509 0.5305448025465012 1.096085712313652\n","21: 0.4749699980020523 0.48333127796649933 0.9583012759685516\n","22: 0.4843905419111252 0.4622781276702881 0.9466686695814133\n","23: 0.47670722752809525 0.4453456252813339 0.9220528528094292\n","24: 0.4364433139562607 0.4093119949102402 0.8457553088665009\n","25: 0.42379137873649597 0.39837566763162613 0.8221670463681221\n","26: 0.39340896904468536 0.36140182614326477 0.7548107951879501\n","27: 0.4721810072660446 0.3676878958940506 0.8398689031600952\n","28: 0.39095621556043625 0.35460763424634933 0.7455638498067856\n","29: 0.3595926836133003 0.3333006426692009 0.6928933262825012\n","30: 0.3398837149143219 0.31595128774642944 0.6558350026607513\n","31: 0.45160598307847977 0.3176376223564148 0.7692436054348946\n","32: 0.3080681562423706 0.2997099906206131 0.6077781468629837\n","33: 0.32592252641916275 0.28533561527729034 0.6112581416964531\n","34: 0.39325011521577835 0.29475080221891403 0.6880009174346924\n","35: 0.32279857248067856 0.2761111408472061 0.5989097133278847\n","36: 0.4014480784535408 0.28676800057291985 0.6882160790264606\n","37: 0.27275049313902855 0.2603499963879585 0.5331004895269871\n","38: 0.34261439740657806 0.255429245531559 0.598043642938137\n","39: 0.2920795977115631 0.24644480645656586 0.538524404168129\n","40: 0.3011363446712494 0.242623221129179 0.5437595658004284\n","41: 0.2956560403108597 0.25155191123485565 0.5472079515457153\n","42: 0.28492340445518494 0.24855772405862808 0.533481128513813\n","43: 0.2937525361776352 0.2337050512433052 0.5274575874209404\n","44: 0.295868881046772 0.23004719614982605 0.525916077196598\n","45: 0.2651057206094265 0.22742098942399025 0.49252671003341675\n","46: 0.29587795585393906 0.22788289934396744 0.5237608551979065\n","47: 0.26487185806035995 0.2156684212386608 0.48054027929902077\n","48: 0.24604105204343796 0.22846489399671555 0.4745059460401535\n","49: 0.2694014087319374 0.21717193350195885 0.48657334223389626\n","50: 0.30111951753497124 0.22615164518356323 0.5272711627185345\n","51: 0.358476422727108 0.21895789355039597 0.577434316277504\n","52: 0.2771153301000595 0.2096366137266159 0.4867519438266754\n","53: 0.24131395667791367 0.19832776486873627 0.43964172154664993\n","54: 0.2249114029109478 0.2077127806842327 0.4326241835951805\n","55: 0.3077542185783386 0.20284650847315788 0.5106007270514965\n","56: 0.31732165813446045 0.20668230950832367 0.5240039676427841\n","57: 0.2620772570371628 0.19661573320627213 0.4586929902434349\n","58: 0.23008384555578232 0.1924683302640915 0.4225521758198738\n","59: 0.32468343526124954 0.19282716512680054 0.5175106003880501\n","60: 0.28052277863025665 0.2063501663506031 0.48687294498085976\n","61: 0.26591163128614426 0.18397174403071404 0.4498833753168583\n","62: 0.27236098796129227 0.18760519474744797 0.45996618270874023\n","63: 0.24037759751081467 0.19136305153369904 0.4317406490445137\n","64: 0.2188139334321022 0.19185327365994453 0.41066720709204674\n","65: 0.24122577160596848 0.19093561172485352 0.432161383330822\n","66: 0.23449593037366867 0.1983565092086792 0.43285243958234787\n","67: 0.2995915934443474 0.18254544585943222 0.4821370393037796\n","68: 0.2868863642215729 0.17999419942498207 0.46688056364655495\n","69: 0.28636781871318817 0.18808520957827568 0.47445302829146385\n","70: 0.21917875297367573 0.17745090648531914 0.39662965945899487\n","71: 0.33804626762866974 0.1942981630563736 0.5323444306850433\n","72: 0.2987951785326004 0.19950493797659874 0.49830011650919914\n","73: 0.23146716132760048 0.1752592697739601 0.4067264311015606\n","74: 0.3434399962425232 0.1848626472055912 0.5283026434481144\n","75: 0.23345788940787315 0.18324198946356773 0.4166998788714409\n","76: 0.2813560180366039 0.18304014950990677 0.4643961675465107\n","77: 0.23856376484036446 0.17292342334985733 0.4114871881902218\n","78: 0.20155754312872887 0.17768514901399612 0.379242692142725\n","79: 0.2619764357805252 0.17049767449498177 0.432474110275507\n","80: 0.2688852772116661 0.19345993548631668 0.4623452126979828\n","81: 0.22789964079856873 0.1748143695294857 0.40271401032805443\n","82: 0.2934773936867714 0.1745770387351513 0.4680544324219227\n","83: 0.21766522526741028 0.17556514218449593 0.3932303674519062\n","84: 0.25846749544143677 0.16942061111330986 0.42788810655474663\n","85: 0.27331362664699554 0.17739558964967728 0.4507092162966728\n","86: 0.202379047870636 0.16171494871377945 0.36409399658441544\n","87: 0.2530573010444641 0.17429760098457336 0.4273549020290375\n","88: 0.18110213056206703 0.17056526988744736 0.3516674004495144\n","89: 0.2819655165076256 0.18819619715213776 0.47016171365976334\n","90: 0.26073458045721054 0.18966581672430038 0.4504003971815109\n","91: 0.20848775655031204 0.17118040844798088 0.3796681649982929\n","92: 0.23863907158374786 0.17517440393567085 0.4138134755194187\n","93: 0.2196657359600067 0.1565524935722351 0.3762182295322418\n","94: 0.21711164340376854 0.18132281675934792 0.39843446016311646\n","95: 0.32177572697401047 0.16090526431798935 0.4826809912919998\n","96: 0.26892081648111343 0.1643107272684574 0.43323154374957085\n","97: 0.21437492221593857 0.16808726638555527 0.38246218860149384\n","98: 0.2535821981728077 0.16125956177711487 0.41484175994992256\n","99: 0.21385573595762253 0.15453725308179855 0.3683929890394211\n","100: 0.22576402872800827 0.1640913113951683 0.3898553401231766\n","101: 0.28120157122612 0.16162951290607452 0.4428310841321945\n","102: 0.27676960825920105 0.16196878254413605 0.4387383908033371\n","103: 0.2191990353167057 0.16340163722634315 0.38260067254304886\n","104: 0.20064947754144669 0.15828970447182655 0.35893918201327324\n","105: 0.24932307749986649 0.15586373955011368 0.40518681704998016\n","106: 0.20038596540689468 0.14907141961157322 0.3494573850184679\n","107: 0.29262296855449677 0.15148259326815605 0.4441055618226528\n","108: 0.27283429354429245 0.15122700855135918 0.4240613020956516\n","109: 0.24903931096196175 0.14728879369795322 0.39632810465991497\n","110: 0.25261105597019196 0.15932511538267136 0.4119361713528633\n","111: 0.25263744592666626 0.14553417079150677 0.398171616718173\n","112: 0.23522193729877472 0.15726053714752197 0.3924824744462967\n","113: 0.22250260412693024 0.14948320016264915 0.3719858042895794\n","114: 0.18263095617294312 0.14599082618951797 0.3286217823624611\n","115: 0.2822644114494324 0.1517994999885559 0.4340639114379883\n","116: 0.2685058116912842 0.15213696658611298 0.42064277827739716\n","117: 0.22624697536230087 0.1457965038716793 0.3720434792339802\n","118: 0.2817959487438202 0.1487795189023018 0.430575467646122\n","119: 0.2346152663230896 0.15156671032309532 0.3861819766461849\n","120: 0.14981000870466232 0.15094643086194992 0.30075643956661224\n","121: 0.1759381741285324 0.14715132862329483 0.32308950275182724\n","122: 0.21422189474105835 0.15111690759658813 0.3653388023376465\n","123: 0.20231374725699425 0.1558019407093525 0.35811568796634674\n","124: 0.3093593046069145 0.14146757684648037 0.4508268814533949\n","125: 0.2729981541633606 0.14788567274808884 0.42088382691144943\n","126: 0.23923665285110474 0.15357722342014313 0.39281387627124786\n","127: 0.2695133686065674 0.15543058887124062 0.424943957477808\n","128: 0.2868231013417244 0.1584316622465849 0.4452547635883093\n","129: 0.2977900207042694 0.14327875152230263 0.44106877222657204\n","130: 0.19946832582354546 0.14858603477478027 0.34805436059832573\n","131: 0.23080061376094818 0.1494046077132225 0.3802052214741707\n","132: 0.2571452260017395 0.14597899839282036 0.40312422439455986\n","133: 0.25053711980581284 0.1491314209997654 0.39966854080557823\n","134: 0.2742828540503979 0.13886196538805962 0.4131448194384575\n","135: 0.2669341340661049 0.15030126087367535 0.41723539493978024\n","136: 0.1581299565732479 0.14199437201023102 0.3001243285834789\n","137: 0.3148830384016037 0.13657207787036896 0.45145511627197266\n","138: 0.19509738311171532 0.13879051804542542 0.33388790115714073\n","139: 0.3151164725422859 0.14599798247218132 0.46111445501446724\n","140: 0.2500230595469475 0.15727506950497627 0.40729812905192375\n","141: 0.21006685122847557 0.14400575309991837 0.35407260432839394\n","142: 0.22582832723855972 0.14822755381464958 0.3740558810532093\n","143: 0.25529085472226143 0.15172450989484787 0.4070153646171093\n","144: 0.23932794481515884 0.1423833779990673 0.38171132281422615\n","145: 0.2567407116293907 0.14153597503900528 0.398276686668396\n","146: 0.24527883902192116 0.1356770135462284 0.38095585256814957\n","147: 0.25471940264105797 0.14098094403743744 0.3957003466784954\n","148: 0.19531840458512306 0.14530694112181664 0.3406253457069397\n","149: 0.1926334872841835 0.14156736806035042 0.3342008553445339\n","150: 0.27723846584558487 0.13517259061336517 0.41241105645895004\n","151: 0.21641534566879272 0.1358565129339695 0.3522718586027622\n","152: 0.3071533292531967 0.13670245185494423 0.44385578110814095\n","153: 0.25704648345708847 0.14520074054598808 0.40224722400307655\n","154: 0.23402487486600876 0.1516693364828825 0.38569421134889126\n","155: 0.23681677877902985 0.1362483948469162 0.37306517362594604\n","156: 0.27433785796165466 0.1317239124327898 0.40606177039444447\n","157: 0.23392967879772186 0.1316593587398529 0.36558903753757477\n","158: 0.2045866921544075 0.1484014242887497 0.3529881164431572\n","159: 0.2637818902730942 0.1372247040271759 0.4010065943002701\n","160: 0.22066029533743858 0.13061329536139965 0.35127359069883823\n","161: 0.15870088711380959 0.1444391906261444 0.303140077739954\n","162: 0.2975260466337204 0.13863879069685936 0.43616483733057976\n","163: 0.2659776844084263 0.1354721561074257 0.401449840515852\n","164: 0.17849314212799072 0.1332833357155323 0.311776477843523\n","165: 0.24549824371933937 0.13904253765940666 0.38454078137874603\n","166: 0.23532909527420998 0.13839595392346382 0.3737250491976738\n","167: 0.16414113715291023 0.13003516755998135 0.2941763047128916\n","168: 0.21296002715826035 0.14523738250136375 0.3581974096596241\n","169: 0.277432844042778 0.1482461765408516 0.4256790205836296\n","170: 0.25716833770275116 0.13796204701066017 0.39513038471341133\n","171: 0.24461275339126587 0.14152332581579685 0.3861360792070627\n","172: 0.26658338867127895 0.14464582689106464 0.4112292155623436\n","173: 0.24989168345928192 0.13714192807674408 0.387033611536026\n","174: 0.22913703322410583 0.13647551089525223 0.36561254411935806\n","175: 0.2829366773366928 0.14734715037047863 0.43028382770717144\n","176: 0.24377057701349258 0.13441776670515537 0.37818834371864796\n","177: 0.23758961632847786 0.13358094915747643 0.3711705654859543\n","178: 0.3009692281484604 0.14612431451678276 0.44709354266524315\n","179: 0.3090245872735977 0.1397971473634243 0.448821734637022\n","180: 0.199588343501091 0.13456977158784866 0.33415811508893967\n","181: 0.24449605494737625 0.1389564387500286 0.38345249369740486\n","182: 0.2565349191427231 0.14259209856390953 0.3991270177066326\n","183: 0.2495594248175621 0.14294781163334846 0.39250723645091057\n","184: 0.1661580726504326 0.14329798892140388 0.30945606157183647\n","185: 0.20668112114071846 0.1352139599621296 0.34189508110284805\n","186: 0.24726255238056183 0.14124586805701256 0.3885084204375744\n","187: 0.21352827921509743 0.13718261942267418 0.3507108986377716\n","188: 0.15241931565105915 0.1323419325053692 0.28476124815642834\n","189: 0.1982368752360344 0.13620033115148544 0.33443720638751984\n","190: 0.3061980605125427 0.1415388211607933 0.44773688167333603\n","191: 0.19934198260307312 0.13488872349262238 0.3342307060956955\n","192: 0.3093332126736641 0.13691943138837814 0.44625264406204224\n","193: 0.2622629553079605 0.13567082956433296 0.3979337848722935\n","194: 0.23451920598745346 0.13491275161504745 0.3694319576025009\n","195: 0.27973872423171997 0.13617969304323196 0.41591841727495193\n","196: 0.217682383954525 0.14463604427874088 0.3623184282332659\n","197: 0.31142251938581467 0.15044380724430084 0.4618663266301155\n","198: 0.2876644805073738 0.14544200152158737 0.4331064820289612\n","199: 0.24882462620735168 0.1436646617949009 0.3924892880022526\n","200: 0.2825637720525265 0.1319732815027237 0.41453705355525017\n","201: 0.31640879809856415 0.15259381383657455 0.4690026119351387\n","202: 0.2553313449025154 0.1324867121875286 0.387818057090044\n","203: 0.2581949271261692 0.13329272717237473 0.39148765429854393\n","204: 0.22258205711841583 0.13803719356656075 0.3606192506849766\n","205: 0.234308160841465 0.13991667702794075 0.37422483786940575\n","206: 0.23764550685882568 0.14531565457582474 0.3829611614346504\n","207: 0.22246643155813217 0.14253956824541092 0.3650059998035431\n","208: 0.2879311814904213 0.13656651973724365 0.42449770122766495\n","209: 0.16877897456288338 0.13824118673801422 0.3070201613008976\n","210: 0.2294275052845478 0.13307924568653107 0.3625067509710789\n","211: 0.20516493171453476 0.13129046373069286 0.3364553954452276\n","212: 0.2019333466887474 0.15036851353943348 0.3523018602281809\n","213: 0.19786759838461876 0.1328133437782526 0.33068094216287136\n","214: 0.1945389248430729 0.1289661768823862 0.3235051017254591\n","215: 0.227449931204319 0.14128036051988602 0.368730291724205\n","216: 0.2474520206451416 0.13997064903378487 0.38742266967892647\n","217: 0.2666344419121742 0.1392248198390007 0.4058592617511749\n","218: 0.17079967632889748 0.13423149287700653 0.305031169205904\n","219: 0.2285975180566311 0.13485319912433624 0.36345071718096733\n","220: 0.1841764897108078 0.13459838181734085 0.31877487152814865\n","221: 0.29441162198781967 0.1364692784845829 0.4308809004724026\n","222: 0.1619272418320179 0.1347796805202961 0.296706922352314\n","223: 0.24188067391514778 0.13712810724973679 0.37900878116488457\n","224: 0.26529738306999207 0.14996855333447456 0.41526593640446663\n","225: 0.2216607667505741 0.1452265866100788 0.3668873533606529\n","226: 0.19489727169275284 0.136624988168478 0.33152225986123085\n","227: 0.20707572251558304 0.13069725967943668 0.3377729821950197\n","228: 0.3088444173336029 0.13557547330856323 0.44441989064216614\n","229: 0.28689488768577576 0.13945422321558 0.42634911090135574\n","230: 0.23688442260026932 0.13203800097107887 0.3689224235713482\n","231: 0.2059684507548809 0.1291837841272354 0.3351522348821163\n","232: 0.218677319586277 0.13755527138710022 0.35623259097337723\n","233: 0.27350418269634247 0.13842012360692024 0.4119243063032627\n","234: 0.20241952314972878 0.13394791260361671 0.3363674357533455\n","235: 0.21208062022924423 0.14783908985555172 0.35991971008479595\n","236: 0.23135925456881523 0.13568099588155746 0.3670402504503727\n","237: 0.19973643124103546 0.14836423471570015 0.3481006659567356\n","238: 0.264065720140934 0.1406501606106758 0.4047158807516098\n","239: 0.20146458968520164 0.13926278799772263 0.34072737768292427\n","240: 0.3174406662583351 0.13680706545710564 0.45424773171544075\n","241: 0.2610541731119156 0.14098674803972244 0.40204092115163803\n","242: 0.16679418459534645 0.12947938591241837 0.2962735705077648\n","243: 0.26056747511029243 0.13514181599020958 0.395709291100502\n","244: 0.25139120221138 0.1422966942191124 0.3936878964304924\n","245: 0.2636575438082218 0.14208880811929703 0.40574635192751884\n","246: 0.17621295154094696 0.13343197107315063 0.3096449226140976\n","247: 0.2896071895956993 0.13340606540441513 0.42301325500011444\n","248: 0.26281819492578506 0.14097986184060574 0.4037980567663908\n","249: 0.24078912287950516 0.14124425500631332 0.3820333778858185\n","250: 0.24775175005197525 0.13492228090763092 0.38267403095960617\n","251: 0.2462926134467125 0.1406891867518425 0.386981800198555\n","252: 0.2615409567952156 0.14522051997482777 0.4067614767700434\n","253: 0.20050156116485596 0.15239175036549568 0.35289331153035164\n","254: 0.3003943897783756 0.14348803088068962 0.44388242065906525\n","255: 0.37409014999866486 0.14089863747358322 0.5149887874722481\n","256: 0.21720583364367485 0.12815508246421814 0.345360916107893\n","257: 0.21484282240271568 0.13721982762217522 0.3520626500248909\n","258: 0.23204821348190308 0.13378780335187912 0.3658360168337822\n","259: 0.1932353489100933 0.14088501408696175 0.33412036299705505\n","260: 0.19541802629828453 0.13094509579241276 0.3263631220906973\n","261: 0.16973206400871277 0.14028426632285118 0.31001633033156395\n","262: 0.2727917954325676 0.14327865093946457 0.41607044637203217\n","263: 0.24405965954065323 0.1284047607332468 0.37246442027390003\n","264: 0.19224992021918297 0.13476122915744781 0.3270111493766308\n","265: 0.245937030762434 0.14224906638264656 0.38818609714508057\n","266: 0.28195230662822723 0.14278944581747055 0.4247417524456978\n","267: 0.3197070471942425 0.13911425694823265 0.45882130414247513\n","268: 0.2769014462828636 0.133508138358593 0.4104095846414566\n","269: 0.2434246726334095 0.1330382451415062 0.3764629177749157\n","270: 0.28627432882785797 0.14082018099725246 0.42709450982511044\n","271: 0.3222610577940941 0.14018302783370018 0.46244408562779427\n","272: 0.25320589169859886 0.1348266564309597 0.38803254812955856\n","273: 0.24167023971676826 0.14089372754096985 0.3825639672577381\n","274: 0.2898033708333969 0.14646676182746887 0.4362701326608658\n","275: 0.2645115628838539 0.13687792792916298 0.4013894908130169\n","276: 0.20122402533888817 0.13265109062194824 0.3338751159608364\n","277: 0.22215629741549492 0.1366784106940031 0.358834708109498\n","278: 0.2243301048874855 0.1309497319161892 0.3552798368036747\n","279: 0.2136789672076702 0.13627231307327747 0.3499512802809477\n","280: 0.204411119222641 0.14825157821178436 0.35266269743442535\n","281: 0.21977368742227554 0.14315605536103249 0.36292974278330803\n","282: 0.21681880205869675 0.14046910032629967 0.3572879023849964\n","283: 0.24503852054476738 0.13102221488952637 0.37606073543429375\n","284: 0.24506549909710884 0.12988044880330563 0.37494594790041447\n","285: 0.24262575060129166 0.13268772885203362 0.37531347945332527\n","286: 0.2571307569742203 0.13548289611935616 0.39261365309357643\n","287: 0.19770271703600883 0.13527267053723335 0.3329753875732422\n","288: 0.18323375284671783 0.12729714065790176 0.3105308935046196\n","289: 0.2560797184705734 0.13742540776729584 0.39350512623786926\n","290: 0.29843395203351974 0.14494255930185318 0.4433765113353729\n","291: 0.20753725618124008 0.13517733290791512 0.3427145890891552\n","292: 0.2604665383696556 0.1322641856968403 0.3927307240664959\n","293: 0.2486347034573555 0.1421390250325203 0.3907737284898758\n","294: 0.2814425379037857 0.135070588439703 0.4165131263434887\n","295: 0.16750053688883781 0.12743345089256763 0.29493398778140545\n","296: 0.2817036062479019 0.14226334542036057 0.4239669516682625\n","297: 0.2050083689391613 0.13841277733445168 0.343421146273613\n","298: 0.3335620388388634 0.1436508260667324 0.4772128649055958\n","299: 0.23194317147135735 0.14039110764861107 0.3723342791199684\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625274646786,"user_tz":-60,"elapsed":13,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625274646787,"user_tz":-60,"elapsed":13,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"5c0a9d5b-3a05-41d3-80d0-21b1506468f6"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":11,"outputs":[{"output_type":"stream","text":["0.956140350877193\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625274646787,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"c2d1738e-6117-4fe0-9fb1-67144f644183"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9853372434017595\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625274646787,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":12,"outputs":[]}]}
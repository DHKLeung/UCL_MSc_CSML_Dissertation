{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN nb0.75 aug WPL0.99.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625275079731,"user_tz":-60,"elapsed":21011,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"b8401229-88fd-458d-ae11-0c394f418572"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625275080335,"user_tz":-60,"elapsed":606,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625275080961,"user_tz":-60,"elapsed":315,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625275080961,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"4112459b-abfc-4159-fd2d-9a3c479ddc13"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","geometric_param = 0.75\n","perturb_loss_weight = 0.99\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f23f4296a50>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625275080962,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625275087596,"user_tz":-60,"elapsed":6637,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"b9a033c7-4e9d-4bfa-bf42-624e94113a75"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1625275087598,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"id":"quiet-module","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1625275087599,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625275091282,"user_tz":-60,"elapsed":3688,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"c48c8736-e43f-41cd-ba24-93fe24c7e7a1"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup with random neighbour perturbation #\n","        mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_breast_nb(inputs, labels, geometric_param, alpha)\n","        \n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_nb))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_nb = augment_outputs[original_num:]\n","        mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_nb + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_nb.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_nb.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print decomposed losses #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:151: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n","  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.1503087878227234 2.1445900797843933 4.294898867607117\n","1: 2.1129508018493652 2.1092678904533386 4.222218692302704\n","2: 2.0520946979522705 2.0478619933128357 4.099956691265106\n","3: 1.976877748966217 1.9756619334220886 3.9525396823883057\n","4: 1.8914054036140442 1.8898784518241882 3.7812838554382324\n","5: 1.7975106835365295 1.800435483455658 3.5979461669921875\n","6: 1.7036454677581787 1.7057110667228699 3.4093565344810486\n","7: 1.5889176726341248 1.5970364809036255 3.1859541535377502\n","8: 1.4964825809001923 1.4950112402439117 2.991493821144104\n","9: 1.376957505941391 1.3819993734359741 2.758956879377365\n","10: 1.2685454487800598 1.2849865555763245 2.5535320043563843\n","11: 1.1744561195373535 1.1646944880485535 2.339150607585907\n","12: 1.0606946349143982 1.0634656250476837 2.124160259962082\n","13: 0.9048055410385132 0.962763249874115 1.8675687909126282\n","14: 0.8914060592651367 0.8747180700302124 1.7661241292953491\n","15: 0.7351260185241699 0.7818605750799179 1.5169865936040878\n","16: 0.7367209196090698 0.735628068447113 1.4723489880561829\n","17: 0.6300280690193176 0.6807173639535904 1.310745432972908\n","18: 0.6300380825996399 0.6256421208381653 1.2556802034378052\n","19: 0.5902618020772934 0.5809005349874496 1.171162337064743\n","20: 0.5227681398391724 0.5240240395069122 1.0467921793460846\n","21: 0.4922448843717575 0.49618110060691833 0.9884259849786758\n","22: 0.5026826858520508 0.4816812425851822 0.984363928437233\n","23: 0.44885097444057465 0.45831283181905746 0.9071638062596321\n","24: 0.4467179626226425 0.42935100197792053 0.876068964600563\n","25: 0.452620692551136 0.4189218282699585 0.8715425208210945\n","26: 0.4066536873579025 0.3887845128774643 0.7954382002353668\n","27: 0.39635907113552094 0.39101485162973404 0.787373922765255\n","28: 0.37036172300577164 0.36149749904870987 0.7318592220544815\n","29: 0.36231814324855804 0.3437100797891617 0.7060282230377197\n","30: 0.38104813545942307 0.34036320447921753 0.7214113399386406\n","31: 0.3152092546224594 0.3251916244626045 0.6404008790850639\n","32: 0.3429713398218155 0.31208404898643494 0.6550553888082504\n","33: 0.37399624288082123 0.30131497979164124 0.6753112226724625\n","34: 0.3031390309333801 0.30079206079244614 0.6039310917258263\n","35: 0.2692798227071762 0.2854112535715103 0.5546910762786865\n","36: 0.2957388460636139 0.279792957007885 0.5755318030714989\n","37: 0.285408653318882 0.28594377636909485 0.5713524296879768\n","38: 0.33198754489421844 0.2691991329193115 0.60118667781353\n","39: 0.3264264464378357 0.26505059003829956 0.5914770364761353\n","40: 0.28614772856235504 0.25735127180814743 0.5434990003705025\n","41: 0.2832862436771393 0.25680433958768845 0.5400905832648277\n","42: 0.28315360099077225 0.25847743451595306 0.5416310355067253\n","43: 0.3085814490914345 0.25061528384685516 0.5591967329382896\n","44: 0.297783762216568 0.24004791676998138 0.5378316789865494\n","45: 0.2788347378373146 0.24998380616307259 0.5288185440003872\n","46: 0.27644816786050797 0.23818715661764145 0.5146353244781494\n","47: 0.27045826613903046 0.2331075333058834 0.5035657994449139\n","48: 0.2633124478161335 0.2297205850481987 0.4930330328643322\n","49: 0.3088679537177086 0.23446976393461227 0.5433377176523209\n","50: 0.19890526682138443 0.24023723974823952 0.43914250656962395\n","51: 0.2977735325694084 0.23349831998348236 0.5312718525528908\n","52: 0.2760908156633377 0.2184843234717846 0.4945751391351223\n","53: 0.2628501430153847 0.2291572242975235 0.4920073673129082\n","54: 0.22028662264347076 0.2205105945467949 0.44079721719026566\n","55: 0.353359367698431 0.2293921485543251 0.5827515162527561\n","56: 0.2764247804880142 0.23451276496052742 0.5109375454485416\n","57: 0.26351838558912277 0.2153766192495823 0.47889500483870506\n","58: 0.2583005130290985 0.21732895448803902 0.4756294675171375\n","59: 0.27537986636161804 0.21411433815956116 0.4894942045211792\n","60: 0.2550354599952698 0.2134437933564186 0.4684792533516884\n","61: 0.27619871497154236 0.20639966055750847 0.4825983755290508\n","62: 0.2354527823626995 0.20857297256588936 0.44402575492858887\n","63: 0.28419504687190056 0.22825604304671288 0.5124510899186134\n","64: 0.26501578837633133 0.2143164575099945 0.47933224588632584\n","65: 0.22155052423477173 0.20063743740320206 0.4221879616379738\n","66: 0.2713461071252823 0.21785333007574081 0.4891994372010231\n","67: 0.28751564025878906 0.19899910688400269 0.48651474714279175\n","68: 0.19696605578064919 0.19774775952100754 0.3947138153016567\n","69: 0.2326124683022499 0.1954858899116516 0.4280983582139015\n","70: 0.25134584307670593 0.21117747575044632 0.46252331882715225\n","71: 0.25310754030942917 0.20376519858837128 0.45687273889780045\n","72: 0.3256895840167999 0.18972809985280037 0.5154176838696003\n","73: 0.20912740752100945 0.19054266810417175 0.3996700756251812\n","74: 0.24969420582056046 0.20154710114002228 0.45124130696058273\n","75: 0.18678460270166397 0.19887064024806023 0.3856552429497242\n","76: 0.274764746427536 0.19140831753611565 0.46617306396365166\n","77: 0.30837681144475937 0.20798882842063904 0.5163656398653984\n","78: 0.2950097545981407 0.1941935457289219 0.4892033003270626\n","79: 0.31905647367239 0.20036990568041801 0.519426379352808\n","80: 0.2722374498844147 0.1897381767630577 0.4619756266474724\n","81: 0.2261681631207466 0.19831449538469315 0.42448265850543976\n","82: 0.23190540075302124 0.19874747097492218 0.4306528717279434\n","83: 0.19845552742481232 0.1882217712700367 0.386677298694849\n","84: 0.27904683351516724 0.18638325110077858 0.4654300846159458\n","85: 0.22488384321331978 0.19235312938690186 0.41723697260022163\n","86: 0.2572140023112297 0.17958873510360718 0.4368027374148369\n","87: 0.22262072563171387 0.17969072237610817 0.40231144800782204\n","88: 0.1723027378320694 0.18418706580996513 0.35648980364203453\n","89: 0.24371080845594406 0.1828102357685566 0.42652104422450066\n","90: 0.26837994903326035 0.17712218686938286 0.4455021359026432\n","91: 0.21349913999438286 0.18516741320490837 0.39866655319929123\n","92: 0.2634813264012337 0.17501841858029366 0.43849974498152733\n","93: 0.1777113825082779 0.18484872952103615 0.36256011202931404\n","94: 0.14995069988071918 0.19845695421099663 0.3484076540917158\n","95: 0.28256965428590775 0.18963170796632767 0.4722013622522354\n","96: 0.23654089868068695 0.1775970607995987 0.41413795948028564\n","97: 0.2379927635192871 0.17068687453866005 0.40867963805794716\n","98: 0.21676542237401009 0.17711293324828148 0.39387835562229156\n","99: 0.3118118494749069 0.17256029695272446 0.4843721464276314\n","100: 0.265010304749012 0.17077584192156792 0.4357861466705799\n","101: 0.21044501662254333 0.18471268191933632 0.39515769854187965\n","102: 0.2443999946117401 0.17259981110692024 0.41699980571866035\n","103: 0.29642830789089203 0.19224433600902557 0.4886726438999176\n","104: 0.2176667284220457 0.18473975732922554 0.40240648575127125\n","105: 0.2974610701203346 0.1957878228276968 0.4932488929480314\n","106: 0.24454521387815475 0.17832831293344498 0.42287352681159973\n","107: 0.2133278101682663 0.16693241707980633 0.3802602272480726\n","108: 0.33483364433050156 0.17153308913111687 0.5063667334616184\n","109: 0.32342732697725296 0.18010620772838593 0.5035335347056389\n","110: 0.20434504002332687 0.18491550534963608 0.38926054537296295\n","111: 0.1897737868130207 0.16667705588042736 0.35645084269344807\n","112: 0.20249168574810028 0.16354930587112904 0.3660409916192293\n","113: 0.21532911993563175 0.16255776770412922 0.37788688763976097\n","114: 0.21601808071136475 0.1711057759821415 0.38712385669350624\n","115: 0.2757628411054611 0.1794034130871296 0.4551662541925907\n","116: 0.22367997467517853 0.17704333364963531 0.40072330832481384\n","117: 0.22638682276010513 0.18174520134925842 0.40813202410936356\n","118: 0.2790442742407322 0.18311593681573868 0.46216021105647087\n","119: 0.2450978085398674 0.16297010518610477 0.4080679137259722\n","120: 0.20053432881832123 0.19092227891087532 0.39145660772919655\n","121: 0.1605820097029209 0.17160892114043236 0.33219093084335327\n","122: 0.1813405081629753 0.17639194056391716 0.35773244872689247\n","123: 0.15599749609827995 0.16019101813435555 0.3161885142326355\n","124: 0.30124641209840775 0.17912627011537552 0.48037268221378326\n","125: 0.30157771706581116 0.1790929101407528 0.48067062720656395\n","126: 0.18929989635944366 0.16964296624064445 0.3589428626000881\n","127: 0.27512264996767044 0.17259375005960464 0.4477164000272751\n","128: 0.23396071791648865 0.16364524513483047 0.3976059630513191\n","129: 0.32384752482175827 0.15920303761959076 0.48305056244134903\n","130: 0.3218536488711834 0.16907216981053352 0.4909258186817169\n","131: 0.23124604672193527 0.16229217872023582 0.3935382254421711\n","132: 0.23040714859962463 0.17873578518629074 0.4091429337859154\n","133: 0.2970530241727829 0.17552531510591507 0.47257833927869797\n","134: 0.23039759695529938 0.1767335906624794 0.4071311876177788\n","135: 0.21808496490120888 0.17326036095619202 0.3913453258574009\n","136: 0.17747937142848969 0.1579976212233305 0.3354769926518202\n","137: 0.257893867790699 0.16293323040008545 0.42082709819078445\n","138: 0.26661616936326027 0.1763804666697979 0.44299663603305817\n","139: 0.2581447083503008 0.1656474433839321 0.4237921517342329\n","140: 0.21178583055734634 0.16887115128338337 0.3806569818407297\n","141: 0.21352362632751465 0.16471928358078003 0.3782429099082947\n","142: 0.20465195178985596 0.16818344593048096 0.3728353977203369\n","143: 0.20056413859128952 0.1687271185219288 0.3692912571132183\n","144: 0.2451244294643402 0.15872009471058846 0.40384452417492867\n","145: 0.23573272675275803 0.17677665501832962 0.41250938177108765\n","146: 0.2166604157537222 0.16276150941848755 0.37942192517220974\n","147: 0.23070859909057617 0.16454485803842545 0.3952534571290016\n","148: 0.21831970661878586 0.16978660598397255 0.3881063126027584\n","149: 0.2226249985396862 0.15427566692233086 0.37690066546201706\n","150: 0.21967199444770813 0.15573538467288017 0.3754073791205883\n","151: 0.20017126947641373 0.15634918585419655 0.3565204553306103\n","152: 0.26264434307813644 0.1622370295226574 0.42488137260079384\n","153: 0.23103932291269302 0.16411352157592773 0.39515284448862076\n","154: 0.22783175483345985 0.15716689825057983 0.3849986530840397\n","155: 0.29916973412036896 0.15345657616853714 0.4526263102889061\n","156: 0.2118549421429634 0.15096057206392288 0.3628155142068863\n","157: 0.20574169605970383 0.15176130086183548 0.3575029969215393\n","158: 0.2037203684449196 0.16218117624521255 0.36590154469013214\n","159: 0.12555541843175888 0.15058697573840618 0.27614239417016506\n","160: 0.20651379227638245 0.15795652195811272 0.36447031423449516\n","161: 0.19579586759209633 0.15099910832941532 0.34679497592151165\n","162: 0.25949661433696747 0.1656651757657528 0.42516179010272026\n","163: 0.20920364558696747 0.15776138752698898 0.36696503311395645\n","164: 0.18991803005337715 0.14644061401486397 0.3363586440682411\n","165: 0.19713738560676575 0.1575828194618225 0.35472020506858826\n","166: 0.21858948469161987 0.16370680183172226 0.38229628652334213\n","167: 0.20752070099115372 0.16028409078717232 0.36780479177832603\n","168: 0.20352868735790253 0.16475777328014374 0.36828646063804626\n","169: 0.2573898658156395 0.16756803914904594 0.42495790496468544\n","170: 0.27244044840335846 0.16020971536636353 0.432650163769722\n","171: 0.2850189879536629 0.16046183183789253 0.4454808197915554\n","172: 0.24672891944646835 0.155911760404706 0.40264067985117435\n","173: 0.19348981976509094 0.15459542721509933 0.3480852469801903\n","174: 0.22006472200155258 0.15200390294194221 0.3720686249434948\n","175: 0.2865036353468895 0.16834203153848648 0.454845666885376\n","176: 0.2512671798467636 0.15296932309865952 0.4042365029454231\n","177: 0.24175090342760086 0.15499991923570633 0.3967508226633072\n","178: 0.3103479780256748 0.1796210054308176 0.4899689834564924\n","179: 0.18291779980063438 0.1483478918671608 0.3312656916677952\n","180: 0.1888943687081337 0.16534606367349625 0.35424043238162994\n","181: 0.22626939415931702 0.156426839530468 0.382696233689785\n","182: 0.24468620494008064 0.15716876089572906 0.4018549658358097\n","183: 0.21689550206065178 0.160762470215559 0.3776579722762108\n","184: 0.17360682040452957 0.14942324720323086 0.32303006760776043\n","185: 0.21974783390760422 0.15496591851115227 0.3747137524187565\n","186: 0.2943793684244156 0.16425537317991257 0.45863474160432816\n","187: 0.24533271044492722 0.15980099141597748 0.4051337018609047\n","188: 0.12317235395312309 0.15114834532141685 0.27432069927453995\n","189: 0.1970823034644127 0.15217259526252747 0.34925489872694016\n","190: 0.19694427773356438 0.15966441482305527 0.35660869255661964\n","191: 0.2272530384361744 0.1600160002708435 0.3872690387070179\n","192: 0.27531613409519196 0.16021807864308357 0.43553421273827553\n","193: 0.2372000589966774 0.1671808771789074 0.4043809361755848\n","194: 0.2664646003395319 0.14574809931218624 0.41221269965171814\n","195: 0.22050216048955917 0.15255868434906006 0.37306084483861923\n","196: 0.21477576717734337 0.16229654848575592 0.3770723156630993\n","197: 0.2111343964934349 0.15729448199272156 0.36842887848615646\n","198: 0.23638591170310974 0.15059231221675873 0.38697822391986847\n","199: 0.20453119277954102 0.14952421747148037 0.3540554102510214\n","200: 0.2556680515408516 0.15045029297471046 0.40611834451556206\n","201: 0.2416472751647234 0.1437555029988289 0.3854027781635523\n","202: 0.1609136015176773 0.14975945465266705 0.31067305617034435\n","203: 0.2412065714597702 0.1633300520479679 0.4045366235077381\n","204: 0.23987465351819992 0.15939150378108025 0.39926615729928017\n","205: 0.20305870845913887 0.1541973352432251 0.35725604370236397\n","206: 0.2583795115351677 0.16482871770858765 0.42320822924375534\n","207: 0.2761727273464203 0.17990226112306118 0.45607498846948147\n","208: 0.21268124505877495 0.16311730444431305 0.375798549503088\n","209: 0.2350577525794506 0.1671016812324524 0.402159433811903\n","210: 0.21250119805335999 0.15822644904255867 0.37072764709591866\n","211: 0.23140029422938824 0.15352456271648407 0.3849248569458723\n","212: 0.21353093162178993 0.14729577861726284 0.3608267102390528\n","213: 0.2377500906586647 0.1582116074860096 0.3959616981446743\n","214: 0.18923378735780716 0.14758852310478687 0.33682231046259403\n","215: 0.21203284710645676 0.15069158002734184 0.3627244271337986\n","216: 0.2524353712797165 0.14933492615818977 0.40177029743790627\n","217: 0.2645600587129593 0.16667907685041428 0.43123913556337357\n","218: 0.20167271047830582 0.14727109484374523 0.34894380532205105\n","219: 0.2795383967459202 0.16972431913018227 0.44926271587610245\n","220: 0.22673475742340088 0.15246838703751564 0.3792031444609165\n","221: 0.2689770385622978 0.14652361162006855 0.41550065018236637\n","222: 0.2239673063158989 0.15957576781511307 0.38354307413101196\n","223: 0.24884761683642864 0.17224601283669472 0.42109362967312336\n","224: 0.22425121441483498 0.14941290393471718 0.37366411834955215\n","225: 0.2353380247950554 0.16111142560839653 0.3964494504034519\n","226: 0.2140711024403572 0.15344370901584625 0.36751481145620346\n","227: 0.2341076359152794 0.1569821499288082 0.3910897858440876\n","228: 0.27423249185085297 0.15672435611486435 0.4309568479657173\n","229: 0.27713990584015846 0.15591337159276009 0.43305327743291855\n","230: 0.2064451351761818 0.15291187167167664 0.35935700684785843\n","231: 0.23659183084964752 0.15018975734710693 0.38678158819675446\n","232: 0.24835189431905746 0.15529204905033112 0.4036439433693886\n","233: 0.27267203480005264 0.16343531385064125 0.4361073486506939\n","234: 0.21042343601584435 0.14952325448393822 0.35994669049978256\n","235: 0.2151356376707554 0.15471259504556656 0.36984823271632195\n","236: 0.20173977315425873 0.1679186299443245 0.3696584030985832\n","237: 0.28478172421455383 0.16617219150066376 0.4509539157152176\n","238: 0.27221740782260895 0.16249872744083405 0.434716135263443\n","239: 0.18576354905962944 0.146781288087368 0.33254483714699745\n","240: 0.2769891731441021 0.16957903653383255 0.44656820967793465\n","241: 0.18178169801831245 0.15002092719078064 0.3318026252090931\n","242: 0.24826981127262115 0.14729860052466393 0.3955684117972851\n","243: 0.22761841118335724 0.16034132800996304 0.3879597391933203\n","244: 0.21883543208241463 0.15873421728610992 0.37756964936852455\n","245: 0.2469269260764122 0.15128952637314796 0.39821645244956017\n","246: 0.17977620288729668 0.14786548167467117 0.32764168456196785\n","247: 0.22272133827209473 0.14840638637542725 0.371127724647522\n","248: 0.2011340968310833 0.14480257034301758 0.3459366671741009\n","249: 0.2657052353024483 0.15482047945261002 0.4205257147550583\n","250: 0.2024918682873249 0.150346077978611 0.3528379462659359\n","251: 0.27225814014673233 0.16103956289589405 0.4332977030426264\n","252: 0.2532758116722107 0.15039440244436264 0.40367021411657333\n","253: 0.1812092438340187 0.1537879966199398 0.3349972404539585\n","254: 0.2570013254880905 0.1515941247344017 0.4085954502224922\n","255: 0.2260277159512043 0.1510530523955822 0.3770807683467865\n","256: 0.21931324154138565 0.15846700593829155 0.3777802474796772\n","257: 0.21312297880649567 0.15499793365597725 0.3681209124624729\n","258: 0.23182128742337227 0.16811060719192028 0.39993189461529255\n","259: 0.1647530496120453 0.1580279916524887 0.322781041264534\n","260: 0.2299751490354538 0.14397204481065273 0.37394719384610653\n","261: 0.23331088200211525 0.1595686450600624 0.39287952706217766\n","262: 0.24672647938132286 0.15514561533927917 0.40187209472060204\n","263: 0.21330007910728455 0.15618912875652313 0.3694892078638077\n","264: 0.19956115633249283 0.15282824262976646 0.3523893989622593\n","265: 0.2122088149189949 0.15501998737454414 0.36722880229353905\n","266: 0.25453685969114304 0.1518690176308155 0.40640587732195854\n","267: 0.20885870978236198 0.14402658492326736 0.35288529470562935\n","268: 0.21686357259750366 0.1578877456486225 0.3747513182461262\n","269: 0.27984110079705715 0.1567237712442875 0.43656487204134464\n","270: 0.20684584230184555 0.1499783918261528 0.35682423412799835\n","271: 0.27755042910575867 0.1588982231914997 0.4364486522972584\n","272: 0.2012895904481411 0.15157752484083176 0.35286711528897285\n","273: 0.17440858483314514 0.15192396193742752 0.32633254677057266\n","274: 0.22814393043518066 0.14682413451373577 0.37496806494891644\n","275: 0.22426199913024902 0.152814120054245 0.377076119184494\n","276: 0.22823423892259598 0.1575741171836853 0.3858083561062813\n","277: 0.23019450902938843 0.15499956160783768 0.3851940706372261\n","278: 0.18351951986551285 0.14529951848089695 0.3288190383464098\n","279: 0.25144119560718536 0.16650474444031715 0.4179459400475025\n","280: 0.17853810265660286 0.14922043308615685 0.3277585357427597\n","281: 0.25090865790843964 0.16088006645441055 0.4117887243628502\n","282: 0.27572278678417206 0.15058216080069542 0.4263049475848675\n","283: 0.19252648204565048 0.15843044221401215 0.35095692425966263\n","284: 0.2543281801044941 0.15070312656462193 0.405031306669116\n","285: 0.26299159601330757 0.14498893171548843 0.407980527728796\n","286: 0.22281521558761597 0.15050124377012253 0.3733164593577385\n","287: 0.17310502752661705 0.14981473609805107 0.3229197636246681\n","288: 0.22222348302602768 0.15787611529231071 0.3800995983183384\n","289: 0.17037581279873848 0.14542332105338573 0.3157991338521242\n","290: 0.27553437650203705 0.15215621143579483 0.4276905879378319\n","291: 0.18878883495926857 0.16790474951267242 0.356693584471941\n","292: 0.27316615730524063 0.1457295548170805 0.41889571212232113\n","293: 0.18494617566466331 0.14510311000049114 0.33004928566515446\n","294: 0.24576936662197113 0.14693396165966988 0.392703328281641\n","295: 0.1634592618793249 0.1451185718178749 0.3085778336971998\n","296: 0.2164441980421543 0.16231661662459373 0.37876081466674805\n","297: 0.28201134875416756 0.16219734400510788 0.44420869275927544\n","298: 0.22812964022159576 0.16004912927746773 0.3881787694990635\n","299: 0.23279771208763123 0.1501239500939846 0.38292166218161583\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625275091282,"user_tz":-60,"elapsed":11,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625275091282,"user_tz":-60,"elapsed":11,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"bfe31ba2-96c1-4a6d-b911-72b6ba64e08c"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9780701754385965\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625275091283,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"8dd72465-9355-4b89-94f6-012a6f69ae0a"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9853372434017595\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625275091283,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":12,"outputs":[]}]}
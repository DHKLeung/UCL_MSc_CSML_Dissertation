{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN nb1.0 aug.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625275348393,"user_tz":-60,"elapsed":24304,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"ad862f24-5af1-4fbe-bab5-00c455457afb"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5"},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W"},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625275350012,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"27f07f90-f6bc-4786-f56e-175bde16554a"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","geometric_param = 1.\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f7d7b0b5a50>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule"},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625275356417,"user_tz":-60,"elapsed":6407,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"e0a0d863-cda7-4d96-e6b7-7523af90b7dd"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module"},"source":["def mixup_breast_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"id":"quiet-module","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions"},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625275360229,"user_tz":-60,"elapsed":3814,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"dbff3a7d-fe34-4d6e-c60c-136b25a925ed"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup with random neighbour perturbation #\n","        mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_breast_nb(inputs, labels, geometric_param, alpha)\n","        \n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_nb))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_nb = augment_outputs[original_num:]\n","        mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        augment_loss = mixup_loss_nb + loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_nb.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += augment_loss.item()\n","\n","        # Gradient Calculation & Optimisation #\n","        augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print decomposed losses #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:151: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n","  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.13731449842453 2.129429817199707 4.266744136810303\n","1: 2.0623812079429626 2.061479866504669 4.123861074447632\n","2: 1.9493603110313416 1.9506045579910278 3.8999650478363037\n","3: 1.8006634712219238 1.8032172322273254 3.6038806438446045\n","4: 1.6261621713638306 1.6376991868019104 3.2638614177703857\n","5: 1.4390183985233307 1.4519588649272919 2.8909772634506226\n","6: 1.255480170249939 1.258717030286789 2.5141972303390503\n","7: 1.0595659613609314 1.0708217322826385 2.1303876638412476\n","8: 0.8499622344970703 0.8674925416707993 1.7174547910690308\n","9: 0.6790866851806641 0.724028691649437 1.4031153619289398\n","10: 0.6309217661619186 0.6305419206619263 1.261463701725006\n","11: 0.5138242840766907 0.5391129404306412 1.0529372096061707\n","12: 0.4676665812730789 0.4594270884990692 0.9270936846733093\n","13: 0.3315367326140404 0.3982413411140442 0.7297780811786652\n","14: 0.37671254575252533 0.37316112220287323 0.7498736679553986\n","15: 0.3036835417151451 0.31830158829689026 0.621985137462616\n","16: 0.2406318597495556 0.29061369970440865 0.5312455594539642\n","17: 0.22563546895980835 0.27436942607164383 0.500004880130291\n","18: 0.2580820135772228 0.28174618631601334 0.5398281961679459\n","19: 0.2365228682756424 0.24927844107151031 0.4858013093471527\n","20: 0.25491461157798767 0.24066384881734848 0.49557846784591675\n","21: 0.1785120852291584 0.22425469756126404 0.40276677161455154\n","22: 0.2280903458595276 0.23522033542394638 0.46331068128347397\n","23: 0.26742541790008545 0.20928555727005005 0.4767109751701355\n","24: 0.25213567167520523 0.213770791888237 0.46590644866228104\n","25: 0.22208546847105026 0.20474878326058388 0.42683425545692444\n","26: 0.22223938815295696 0.18803750723600388 0.4102768823504448\n","27: 0.1691715121269226 0.19013097882270813 0.35930247604846954\n","28: 0.22287820279598236 0.1890757493674755 0.41195395588874817\n","29: 0.2629593685269356 0.19338316470384598 0.45634252578020096\n","30: 0.1945755071938038 0.18339215219020844 0.3779676631093025\n","31: 0.19144559651613235 0.20118261501193047 0.3926282078027725\n","32: 0.24720178544521332 0.19155658781528473 0.43875837326049805\n","33: 0.21636086329817772 0.17204001545906067 0.3884008824825287\n","34: 0.17814446985721588 0.19387518987059593 0.3720196560025215\n","35: 0.21489856764674187 0.1694461777806282 0.3843447342514992\n","36: 0.21670030802488327 0.16244950890541077 0.37914982438087463\n","37: 0.21429871208965778 0.17962606251239777 0.3939247652888298\n","38: 0.22242607176303864 0.16056865081191063 0.38299472630023956\n","39: 0.20087876915931702 0.18252144753932953 0.38340020924806595\n","40: 0.21876297518610954 0.19486157782375813 0.4136245548725128\n","41: 0.18382836505770683 0.16124553978443146 0.3450738936662674\n","42: 0.13229206763207912 0.16482607647776604 0.2971181347966194\n","43: 0.2332708090543747 0.16802128590643406 0.4012920930981636\n","44: 0.1723821423947811 0.15631350874900818 0.328695647418499\n","45: 0.15073850378394127 0.18942249938845634 0.3401610031723976\n","46: 0.16335727088153362 0.1515772957354784 0.3149345740675926\n","47: 0.24479661881923676 0.18853053450584412 0.4333271384239197\n","48: 0.16567817702889442 0.15351026505231857 0.3191884458065033\n","49: 0.25580311566591263 0.15304603800177574 0.40884914994239807\n","50: 0.21262803860008717 0.17004209198057652 0.3826701231300831\n","51: 0.22789005935192108 0.14356152527034283 0.37145158648490906\n","52: 0.23747479170560837 0.14973758906126022 0.3872123658657074\n","53: 0.20951830595731735 0.16662543639540672 0.3761437386274338\n","54: 0.13132662884891033 0.146532841026783 0.27785947918891907\n","55: 0.21603853441774845 0.16367079131305218 0.37970932200551033\n","56: 0.24104800075292587 0.18022934347391129 0.42127732560038567\n","57: 0.22841684147715569 0.1577289216220379 0.3861457630991936\n","58: 0.11702921614050865 0.1440113764256239 0.2610405832529068\n","59: 0.1980750635266304 0.14768337458372116 0.34575844556093216\n","60: 0.1798180639743805 0.17610783874988556 0.35592590272426605\n","61: 0.17971773073077202 0.1504063680768013 0.3301241025328636\n","62: 0.22421524673700333 0.1620542872697115 0.38626954704523087\n","63: 0.1806767014786601 0.13660643342882395 0.31728313490748405\n","64: 0.14949134178459644 0.14362729340791702 0.2931186407804489\n","65: 0.18971589021384716 0.14438171591609716 0.33409760147333145\n","66: 0.22974121943116188 0.16114148497581482 0.3908827006816864\n","67: 0.21819709241390228 0.13941525295376778 0.35761234164237976\n","68: 0.1542419120669365 0.13362161442637444 0.28786352649331093\n","69: 0.19351080805063248 0.13919978961348534 0.3327106013894081\n","70: 0.13534102588891983 0.13996058329939842 0.27530160546302795\n","71: 0.18069021962583065 0.13315470330417156 0.3138449266552925\n","72: 0.2176981195807457 0.13434188440442085 0.35204000025987625\n","73: 0.149531539529562 0.13832706585526466 0.28785860538482666\n","74: 0.18783864751458168 0.13438340090215206 0.3222220465540886\n","75: 0.15353007894009352 0.12979350611567497 0.28332358971238136\n","76: 0.18745047226548195 0.13083036988973618 0.3182808496057987\n","77: 0.16118546575307846 0.13868049532175064 0.2998659722507\n","78: 0.14781276881694794 0.13436383567750454 0.2821766138076782\n","79: 0.2927119992673397 0.152975520119071 0.44568751752376556\n","80: 0.1875276416540146 0.12704936880618334 0.31457700207829475\n","81: 0.17699845880270004 0.12601915653795004 0.30301761627197266\n","82: 0.20021740347146988 0.12795190513134003 0.3281693086028099\n","83: 0.19194455072283745 0.13689100556075573 0.32883555442094803\n","84: 0.1847735047340393 0.13101936504244804 0.31579287350177765\n","85: 0.13127772323787212 0.13171235099434853 0.2629900723695755\n","86: 0.1749436194077134 0.15577670373022556 0.33072032406926155\n","87: 0.16111393831670284 0.12673603743314743 0.287849985063076\n","88: 0.17072634026408195 0.12699815351516008 0.29772448912262917\n","89: 0.159218305721879 0.13496310636401176 0.2941814064979553\n","90: 0.23805291205644608 0.14858656655997038 0.38663947582244873\n","91: 0.1380804143846035 0.12273653596639633 0.2608169615268707\n","92: 0.19418315961956978 0.12637578509747982 0.32055893540382385\n","93: 0.1521895956248045 0.13113660365343094 0.2833262011408806\n","94: 0.13046531565487385 0.12284393422305584 0.2533092498779297\n","95: 0.19526559486985207 0.15857512969523668 0.35384073853492737\n","96: 0.21142112091183662 0.12448686547577381 0.335907980799675\n","97: 0.15779387764632702 0.12424216605722904 0.28203604742884636\n","98: 0.1722552627325058 0.155174033716321 0.32742930203676224\n","99: 0.15700170025229454 0.12900971621274948 0.286011416465044\n","100: 0.173378212377429 0.12174042221158743 0.29511863365769386\n","101: 0.1262511294335127 0.15896074753254652 0.28521187976002693\n","102: 0.14438930619508028 0.14537514187395573 0.2897644490003586\n","103: 0.13704697415232658 0.1549115814268589 0.29195854626595974\n","104: 0.09178339876234531 0.12144618202000856 0.21322957798838615\n","105: 0.22425181418657303 0.12688142899423838 0.35113323107361794\n","106: 0.18311773985624313 0.11854403652250767 0.3016617726534605\n","107: 0.20034174993634224 0.1202569268643856 0.32059866935014725\n","108: 0.1720288060605526 0.11592568643391132 0.2879544869065285\n","109: 0.17710100673139095 0.14781970903277397 0.3249207213521004\n","110: 0.14222318679094315 0.11893025040626526 0.2611534371972084\n","111: 0.19445715844631195 0.14348100870847702 0.33793816715478897\n","112: 0.16881438344717026 0.11723685264587402 0.2860512360930443\n","113: 0.15681852586567402 0.12011977285146713 0.2769382931292057\n","114: 0.086881123483181 0.11442019883543253 0.20130132511258125\n","115: 0.20665251091122627 0.11663064733147621 0.3232831619679928\n","116: 0.19258087128400803 0.11841658130288124 0.31099744141101837\n","117: 0.13898857310414314 0.11431418731808662 0.25330276414752007\n","118: 0.23886652663350105 0.1391710415482521 0.37803755700588226\n","119: 0.1580282598733902 0.13763311132788658 0.2956613600254059\n","120: 0.09729066118597984 0.13885503634810448 0.23614570498466492\n","121: 0.10346861556172371 0.1153975073248148 0.21886612474918365\n","122: 0.1473820060491562 0.13697480410337448 0.28435680642724037\n","123: 0.15041919611394405 0.11628808453679085 0.26670727133750916\n","124: 0.27423661202192307 0.13676315546035767 0.41099976003170013\n","125: 0.17551999911665916 0.11586346291005611 0.29138346016407013\n","126: 0.14427076652646065 0.11287261638790369 0.25714338198304176\n","127: 0.12173664197325706 0.12003579922020435 0.24177244305610657\n","128: 0.21127591468393803 0.11404791660606861 0.32532382011413574\n","129: 0.2332621067762375 0.13858110271394253 0.37184321135282516\n","130: 0.18379715271294117 0.11149095185101032 0.2952881008386612\n","131: 0.16188033670186996 0.1461442494764924 0.3080245852470398\n","132: 0.16819916665554047 0.11066605057567358 0.2788652181625366\n","133: 0.2253095656633377 0.11030283849686384 0.3356124013662338\n","134: 0.24020333588123322 0.1436669249087572 0.3838702440261841\n","135: 0.16760605573654175 0.11069605126976967 0.2783020995557308\n","136: 0.19062747433781624 0.11096663866192102 0.3015941083431244\n","137: 0.1801974792033434 0.11938849836587906 0.2995859757065773\n","138: 0.12450854294002056 0.11625948920845985 0.24076802656054497\n","139: 0.20255102962255478 0.1330755203962326 0.3356265425682068\n","140: 0.185088150203228 0.11145894415676594 0.2965470850467682\n","141: 0.1963985413312912 0.11207744479179382 0.3084759898483753\n","142: 0.19537803530693054 0.11789018101990223 0.3132682144641876\n","143: 0.14708705991506577 0.11556891351938248 0.26265597715973854\n","144: 0.15222781151533127 0.10985696874558926 0.2620847746729851\n","145: 0.18238655105233192 0.10616327077150345 0.2885498218238354\n","146: 0.12173562683165073 0.10987285524606705 0.23160849139094353\n","147: 0.12525119818747044 0.10940679721534252 0.23465799540281296\n","148: 0.19008238427340984 0.11572437454015017 0.3058067597448826\n","149: 0.25014325976371765 0.1325747575610876 0.3827180229127407\n","150: 0.16026572696864605 0.10560856480151415 0.26587430015206337\n","151: 0.21468108519911766 0.13269679620862007 0.34737787768244743\n","152: 0.23063569515943527 0.13578165974467993 0.3664173558354378\n","153: 0.14900081977248192 0.10696714092046022 0.25596796721220016\n","154: 0.1834418922662735 0.11336619593203068 0.296808086335659\n","155: 0.1932312473654747 0.13900952227413654 0.3322407752275467\n","156: 0.2199838012456894 0.11647641845047474 0.336460217833519\n","157: 0.18601546622812748 0.11279501859098673 0.2988104782998562\n","158: 0.2012050673365593 0.13579741306602955 0.3370024785399437\n","159: 0.1660398580133915 0.10722536779940128 0.27326522022485733\n","160: 0.14120617136359215 0.10724140889942646 0.24844758212566376\n","161: 0.14194003865122795 0.1052605900913477 0.24720062129199505\n","162: 0.2700512558221817 0.14358084462583065 0.4136320911347866\n","163: 0.09930835291743279 0.10613260604441166 0.20544096268713474\n","164: 0.12001907080411911 0.11005604360252619 0.23007511347532272\n","165: 0.15947981551289558 0.11326378211379051 0.2727436050772667\n","166: 0.2509447857737541 0.10761637892574072 0.3585611656308174\n","167: 0.12833930272608995 0.10572570748627186 0.2340650111436844\n","168: 0.13330045714974403 0.10638466663658619 0.23968512564897537\n","169: 0.18189185857772827 0.106428905390203 0.28832076489925385\n","170: 0.16768886521458626 0.1125329677015543 0.2802218310534954\n","171: 0.2223830297589302 0.10781744495034218 0.3302004747092724\n","172: 0.22040148824453354 0.10559442639350891 0.32599590718746185\n","173: 0.1612718403339386 0.1116661112755537 0.27293794974684715\n","174: 0.1967393234372139 0.10478447657078505 0.3015238046646118\n","175: 0.23043888062238693 0.11178759671747684 0.34222647547721863\n","176: 0.16802048683166504 0.11135951057076454 0.2793800011277199\n","177: 0.13684547320008278 0.10448637790977955 0.24133186042308807\n","178: 0.15268704667687416 0.10455823643133044 0.2572452798485756\n","179: 0.2235667109489441 0.10835333168506622 0.3319200351834297\n","180: 0.18344058468937874 0.13291669823229313 0.316357284784317\n","181: 0.14113236591219902 0.10509991180151701 0.2462322823703289\n","182: 0.15600857231765985 0.10661880671977997 0.26262736693024635\n","183: 0.14143095165491104 0.11213246919214725 0.25356341898441315\n","184: 0.12860863469541073 0.10840491205453873 0.2370135374367237\n","185: 0.16373810917139053 0.10766369290649891 0.271401796489954\n","186: 0.22823204472661018 0.1315386053174734 0.35977064818143845\n","187: 0.15719990245997906 0.11560743115842342 0.2728073336184025\n","188: 0.14438529312610626 0.11454254575073719 0.2589278370141983\n","189: 0.14612105302512646 0.10811392776668072 0.2542349845170975\n","190: 0.19416753202676773 0.10746194235980511 0.301629476249218\n","191: 0.17026376724243164 0.10479349829256535 0.27505725994706154\n","192: 0.19600702077150345 0.13348578661680222 0.32949281111359596\n","193: 0.255648884922266 0.13675476983189583 0.39240366220474243\n","194: 0.14868658781051636 0.1069151358678937 0.2556017190217972\n","195: 0.16313786059617996 0.10794269293546677 0.27108055353164673\n","196: 0.17900683730840683 0.10724704340100288 0.2862538918852806\n","197: 0.18149972707033157 0.10835569724440575 0.2898554354906082\n","198: 0.20416264049708843 0.10479370877146721 0.3089563474059105\n","199: 0.16136214509606361 0.10412563430145383 0.26548778638243675\n","200: 0.14057113975286484 0.13321074470877647 0.2737818844616413\n","201: 0.17793485149741173 0.10676944628357887 0.2847042977809906\n","202: 0.14020162168890238 0.11264132335782051 0.2528429329395294\n","203: 0.1582517772912979 0.11414339765906334 0.27239517122507095\n","204: 0.17922454327344894 0.10754602216184139 0.2867705598473549\n","205: 0.20009075850248337 0.11133674532175064 0.311427503824234\n","206: 0.1314350590109825 0.10585036408156157 0.23728542402386665\n","207: 0.15368683822453022 0.111079853028059 0.2647666893899441\n","208: 0.23893873766064644 0.11125423200428486 0.35019296407699585\n","209: 0.09890193864703178 0.10678859986364841 0.20569054409861565\n","210: 0.13155993074178696 0.11202460899949074 0.243584543466568\n","211: 0.09116264432668686 0.10643643792718649 0.19759908318519592\n","212: 0.1741706896573305 0.10577991046011448 0.27995059825479984\n","213: 0.12197187729179859 0.1136296484619379 0.2356015294790268\n","214: 0.13910875469446182 0.10799039341509342 0.2470991462469101\n","215: 0.16298606619238853 0.10370770748704672 0.2666937820613384\n","216: 0.15475327894091606 0.10739946272224188 0.2621527500450611\n","217: 0.13905519619584084 0.10589580703526735 0.2449510060250759\n","218: 0.1344001181423664 0.1080397367477417 0.2424398548901081\n","219: 0.16565291583538055 0.11071890220046043 0.2763718217611313\n","220: 0.14188534021377563 0.10623871628195047 0.24812405556440353\n","221: 0.23820610716938972 0.12980720214545727 0.36801330745220184\n","222: 0.13200195506215096 0.131667154841125 0.26366911455988884\n","223: 0.12280979007482529 0.11074345000088215 0.23355324566364288\n","224: 0.17930909618735313 0.10661321133375168 0.28592229820787907\n","225: 0.11792301945388317 0.1337945619598031 0.25171758234500885\n","226: 0.12674527056515217 0.10431721992790699 0.23106249794363976\n","227: 0.2118803933262825 0.11237494833767414 0.3242553249001503\n","228: 0.26516612619161606 0.10582655761390924 0.37099268287420273\n","229: 0.1124400906264782 0.10494980402290821 0.21738989651203156\n","230: 0.19980166852474213 0.13168857712298632 0.33149025589227676\n","231: 0.17315901815891266 0.1087401881814003 0.28189920261502266\n","232: 0.1578374095261097 0.1062723696231842 0.2641097791492939\n","233: 0.13302253186702728 0.10511023085564375 0.2381327636539936\n","234: 0.14864855259656906 0.10698323883116245 0.25563178211450577\n","235: 0.16545747220516205 0.13148682564496994 0.296944297850132\n","236: 0.20144938863813877 0.10468813963234425 0.306137528270483\n","237: 0.16117367148399353 0.10999230714514852 0.27116598188877106\n","238: 0.14957366697490215 0.10739853046834469 0.25697219371795654\n","239: 0.12346963956952095 0.1111942920833826 0.2346639335155487\n","240: 0.2176240012049675 0.10595483053475618 0.3235788345336914\n","241: 0.12966890260577202 0.13081164844334126 0.2604805454611778\n","242: 0.18523674830794334 0.10624919272959232 0.2914859429001808\n","243: 0.1567849963903427 0.11369812395423651 0.2704831138253212\n","244: 0.15991274639964104 0.10793204419314861 0.2678447887301445\n","245: 0.1578066535294056 0.10602566041052341 0.26383231580257416\n","246: 0.14321241155266762 0.11108038388192654 0.2542928010225296\n","247: 0.20836567878723145 0.11295507475733757 0.3213207572698593\n","248: 0.2403800766915083 0.1331058880314231 0.3734859563410282\n","249: 0.2068600058555603 0.12935385666787624 0.3362138569355011\n","250: 0.1326457690447569 0.1047245729714632 0.2373703420162201\n","251: 0.15433301776647568 0.10311859147623181 0.2574516087770462\n","252: 0.14608672633767128 0.10295265121385455 0.24903936870396137\n","253: 0.13049038127064705 0.10929960384964943 0.23978998512029648\n","254: 0.18172748014330864 0.10432563629001379 0.2860531210899353\n","255: 0.18952438235282898 0.10711749456822872 0.29664187878370285\n","256: 0.17220044322311878 0.13153109699487686 0.3037315532565117\n","257: 0.13914434611797333 0.10579910315573215 0.24494345113635063\n","258: 0.15888850204646587 0.13170748855918646 0.29059599712491035\n","259: 0.17174604535102844 0.11128037609159946 0.28302642703056335\n","260: 0.1916647721081972 0.10665624961256981 0.29832103103399277\n","261: 0.1386551782488823 0.11121197510510683 0.24986715614795685\n","262: 0.12992717325687408 0.10601680353283882 0.2359439842402935\n","263: 0.15019442327320576 0.11276662163436413 0.2629610449075699\n","264: 0.12021731585264206 0.10495114512741566 0.22516846656799316\n","265: 0.15808647125959396 0.10483592562377453 0.26292238757014275\n","266: 0.22893666476011276 0.13009174540638924 0.3590283989906311\n","267: 0.0883982446976006 0.1346909385174513 0.22308918461203575\n","268: 0.20684127509593964 0.13278464041650295 0.33962590992450714\n","269: 0.23187213763594627 0.11416345648467541 0.3460356071591377\n","270: 0.2151956409215927 0.10622624680399895 0.32142189145088196\n","271: 0.20597612112760544 0.11140656657516956 0.31738268584012985\n","272: 0.16128594428300858 0.11139086075127125 0.2726768031716347\n","273: 0.14538539201021194 0.10340148396790028 0.24878688342869282\n","274: 0.23087970912456512 0.13060829043388367 0.3614879995584488\n","275: 0.1505722003057599 0.10330915823578835 0.2538813576102257\n","276: 0.2501758933067322 0.12909269519150257 0.3792686015367508\n","277: 0.12756481021642685 0.1066999826580286 0.2342647910118103\n","278: 0.21494631096720695 0.105358955450356 0.3203052654862404\n","279: 0.19788221269845963 0.11140919476747513 0.30929140746593475\n","280: 0.15693771559745073 0.10480191931128502 0.2617396377027035\n","281: 0.12135512754321098 0.13088547252118587 0.2522406131029129\n","282: 0.16320255398750305 0.12875971570611 0.29196227341890335\n","283: 0.20048978738486767 0.11274265684187412 0.3132324479520321\n","284: 0.12604405172169209 0.10652836598455906 0.23257241398096085\n","285: 0.20045646652579308 0.12849145382642746 0.32894792407751083\n","286: 0.1931753922253847 0.13722359761595726 0.3303989842534065\n","287: 0.13856404833495617 0.11031253263354301 0.24887658469378948\n","288: 0.17932270281016827 0.11195322126150131 0.2912759203463793\n","289: 0.09288314171135426 0.1360126556828618 0.22889580205082893\n","290: 0.21866372227668762 0.1102877352386713 0.3289514556527138\n","291: 0.10754639655351639 0.13020088337361813 0.23774727806448936\n","292: 0.20205923914909363 0.10428436938673258 0.30634361132979393\n","293: 0.2018243968486786 0.13340336177498102 0.33522776514291763\n","294: 0.20039942115545273 0.10919913649559021 0.30959855020046234\n","295: 0.19223972409963608 0.13026722986251116 0.32250694930553436\n","296: 0.11230079457163811 0.1362309749238193 0.2485317699611187\n","297: 0.16355164349079132 0.10642893239855766 0.2699805833399296\n","298: 0.1601390428841114 0.10555296577513218 0.26569201797246933\n","299: 0.16277535818517208 0.10333139728754759 0.2661067508161068\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details"},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625275360230,"user_tz":-60,"elapsed":17,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"2655c87f-08ce-4b60-dade-491db6b7bcf5"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":null,"outputs":[{"output_type":"stream","text":["0.9692982456140351\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625275360230,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"c8337441-f700-4c0d-c6fc-dbedc00d3e2e"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":null,"outputs":[{"output_type":"stream","text":["0.9941348973607038\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy"},"source":[""],"id":"preceding-galaxy","execution_count":null,"outputs":[]}]}
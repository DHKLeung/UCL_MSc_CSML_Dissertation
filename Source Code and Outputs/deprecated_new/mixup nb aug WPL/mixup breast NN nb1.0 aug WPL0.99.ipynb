{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN nb1.0 aug WPL0.99.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625275258244,"user_tz":-60,"elapsed":55831,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"a720b68b-5bb2-428a-bf39-725f84aa57eb"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625275259061,"user_tz":-60,"elapsed":819,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625275259374,"user_tz":-60,"elapsed":314,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625275259374,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"09101762-67e4-42e5-ea69-2afc8cd73632"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","geometric_param = 1.\n","perturb_loss_weight = 0.99\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f6c416cfa50>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625275259375,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625275265645,"user_tz":-60,"elapsed":5925,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"e480f29f-e94c-4f1f-c08b-007ebcf6a2e2"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1625275265645,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"id":"quiet-module","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1625275265646,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625275268889,"user_tz":-60,"elapsed":3246,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"9ecf5fea-1a1e-4c8e-8060-2e8c9740797c"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup with random neighbour perturbation #\n","        mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_breast_nb(inputs, labels, geometric_param, alpha)\n","        \n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_nb))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_nb = augment_outputs[original_num:]\n","        mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_nb + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_nb.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_nb.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print decomposed losses #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:151: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n","  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.1474971175193787 2.1428107619285583 4.290307879447937\n","1: 2.109291732311249 2.1062490940093994 4.215540826320648\n","2: 2.050751745700836 2.047065019607544 4.09781676530838\n","3: 1.9735172986984253 1.973512589931488 3.9470298886299133\n","4: 1.8905183672904968 1.8862571716308594 3.776775538921356\n","5: 1.785868763923645 1.78936767578125 3.575236439704895\n","6: 1.6924107670783997 1.692635416984558 3.3850461840629578\n","7: 1.5790077447891235 1.590003788471222 3.1690115332603455\n","8: 1.4653834104537964 1.473097324371338 2.9384807348251343\n","9: 1.3483508825302124 1.3591899871826172 2.7075408697128296\n","10: 1.2205640375614166 1.2471187710762024 2.467682808637619\n","11: 1.1025945842266083 1.1387452483177185 2.241339832544327\n","12: 1.0156057178974152 1.037030279636383 2.052635997533798\n","13: 0.8677406907081604 0.9205180406570435 1.7882587313652039\n","14: 0.8375106453895569 0.822622075676918 1.660132721066475\n","15: 0.7352421134710312 0.7402984350919724 1.4755405485630035\n","16: 0.6482524573802948 0.6911188066005707 1.3393712639808655\n","17: 0.5888474881649017 0.6291857659816742 1.218033254146576\n","18: 0.5784259140491486 0.5805638134479523 1.1589897274971008\n","19: 0.5395637154579163 0.5496018826961517 1.089165598154068\n","20: 0.48256370425224304 0.48856180161237717 0.9711255058646202\n","21: 0.41537898033857346 0.48004424571990967 0.8954232260584831\n","22: 0.45133569836616516 0.45575572550296783 0.907091423869133\n","23: 0.3810475617647171 0.4268081784248352 0.8078557401895523\n","24: 0.44186627864837646 0.4167972803115845 0.8586635589599609\n","25: 0.4017089456319809 0.3873228132724762 0.7890317589044571\n","26: 0.3771946653723717 0.38056471198797226 0.7577593773603439\n","27: 0.3921079710125923 0.3820563852787018 0.7741643562912941\n","28: 0.3714989870786667 0.36387137323617935 0.735370360314846\n","29: 0.30629371106624603 0.3332969769835472 0.6395906880497932\n","30: 0.33542051911354065 0.3263564631342888 0.6617769822478294\n","31: 0.3214968740940094 0.3147215396165848 0.6362184137105942\n","32: 0.3278411254286766 0.3055146858096123 0.6333558112382889\n","33: 0.37875185906887054 0.3145214766263962 0.6932733356952667\n","34: 0.29671555757522583 0.2855404280126095 0.5822559855878353\n","35: 0.29473496973514557 0.29374033212661743 0.588475301861763\n","36: 0.33401400595903397 0.29207655787467957 0.6260905638337135\n","37: 0.26435886323451996 0.2811898812651634 0.5455487444996834\n","38: 0.2977524474263191 0.2748700901865959 0.572622537612915\n","39: 0.30177561193704605 0.2874489203095436 0.5892245322465897\n","40: 0.28886518627405167 0.26257894933223724 0.5514441356062889\n","41: 0.29814983904361725 0.25877708196640015 0.5569269210100174\n","42: 0.2641078345477581 0.24990642815828323 0.5140142627060413\n","43: 0.3093502074480057 0.25137780606746674 0.5607280135154724\n","44: 0.31847362220287323 0.2550567761063576 0.5735303983092308\n","45: 0.22960984706878662 0.24417716264724731 0.47378700971603394\n","46: 0.2733779773116112 0.2505520209670067 0.5239299982786179\n","47: 0.2647627219557762 0.24933408200740814 0.5140968039631844\n","48: 0.28016236424446106 0.2500533014535904 0.5302156656980515\n","49: 0.2484739124774933 0.22882571071386337 0.47729962319135666\n","50: 0.21054570004343987 0.22961785644292831 0.4401635564863682\n","51: 0.2593107894062996 0.22741758450865746 0.48672837391495705\n","52: 0.2710561081767082 0.2308334782719612 0.5018895864486694\n","53: 0.27692989259958267 0.24042587727308273 0.5173557698726654\n","54: 0.28745800256729126 0.21922962740063667 0.5066876299679279\n","55: 0.2373916357755661 0.22578056156635284 0.46317219734191895\n","56: 0.2536921128630638 0.21521058678627014 0.46890269964933395\n","57: 0.26463746652007103 0.22054995968937874 0.48518742620944977\n","58: 0.23737987130880356 0.21645040810108185 0.4538302794098854\n","59: 0.2880179360508919 0.22804345563054085 0.5160613916814327\n","60: 0.2364097386598587 0.21669605374336243 0.45310579240322113\n","61: 0.2620011419057846 0.20938421040773392 0.4713853523135185\n","62: 0.21904587000608444 0.21170539036393166 0.4307512603700161\n","63: 0.25963930040597916 0.21602186560630798 0.47566116601228714\n","64: 0.24255160242319107 0.21666757762432098 0.45921918004751205\n","65: 0.23921265453100204 0.22974681109189987 0.4689594656229019\n","66: 0.2375081330537796 0.21954390406608582 0.4570520371198654\n","67: 0.2980346605181694 0.21266142278909683 0.5106960833072662\n","68: 0.220182865858078 0.21782391890883446 0.43800678476691246\n","69: 0.27277252823114395 0.2111099734902382 0.48388250172138214\n","70: 0.16895100846886635 0.21105942502617836 0.3800104334950447\n","71: 0.25928767025470734 0.20324645936489105 0.4625341296195984\n","72: 0.252001628279686 0.2178788036108017 0.46988043189048767\n","73: 0.2104809433221817 0.22101988270878792 0.4315008260309696\n","74: 0.24164192378520966 0.19971338286995888 0.44135530665516853\n","75: 0.22514241188764572 0.20320988819003105 0.4283523000776768\n","76: 0.2676645591855049 0.20688974484801292 0.47455430403351784\n","77: 0.22045434266328812 0.19946658238768578 0.4199209250509739\n","78: 0.22422267496585846 0.2015925981104374 0.42581527307629585\n","79: 0.31470078602433205 0.22695103287696838 0.5416518189013004\n","80: 0.23557202145457268 0.19033298641443253 0.4259050078690052\n","81: 0.23619085550308228 0.18815048038959503 0.4243413358926773\n","82: 0.2355349436402321 0.19038120657205582 0.4259161502122879\n","83: 0.28029878437519073 0.20839166827499866 0.4886904526501894\n","84: 0.21295364201068878 0.21015213429927826 0.42310577630996704\n","85: 0.24486331455409527 0.22082819789648056 0.46569151245057583\n","86: 0.1750926449894905 0.2057056911289692 0.3807983361184597\n","87: 0.2505834251642227 0.19519789144396782 0.44578131660819054\n","88: 0.27155859768390656 0.1925140656530857 0.46407266333699226\n","89: 0.23566563054919243 0.20413055270910263 0.43979618325829506\n","90: 0.2491399049758911 0.21501575037837029 0.4641556553542614\n","91: 0.21634666621685028 0.1824692115187645 0.3988158777356148\n","92: 0.2616308405995369 0.18915286287665367 0.45078370347619057\n","93: 0.22967779263854027 0.19145621731877327 0.42113400995731354\n","94: 0.1927093304693699 0.18279356323182583 0.3755028937011957\n","95: 0.23591791465878487 0.19954293966293335 0.4354608543217182\n","96: 0.3025215342640877 0.18313851207494736 0.48566004633903503\n","97: 0.22550234198570251 0.18558740988373756 0.4110897518694401\n","98: 0.22030796855688095 0.1895425021648407 0.40985047072172165\n","99: 0.2592983990907669 0.18759829550981522 0.4468966946005821\n","100: 0.2490449994802475 0.1853254698216915 0.434370469301939\n","101: 0.21428735926747322 0.1871308833360672 0.4014182426035404\n","102: 0.2214164212346077 0.19360781461000443 0.4150242358446121\n","103: 0.16930949687957764 0.17731122113764286 0.3466207180172205\n","104: 0.25062815845012665 0.1818356141448021 0.43246377259492874\n","105: 0.2508958727121353 0.189928088337183 0.4408239610493183\n","106: 0.22917335852980614 0.18235358968377113 0.41152694821357727\n","107: 0.2738151475787163 0.1790863275527954 0.4529014751315117\n","108: 0.23182816803455353 0.1886005401611328 0.42042870819568634\n","109: 0.25692857429385185 0.19266719743609428 0.44959577172994614\n","110: 0.29356903210282326 0.2125401794910431 0.5061092115938663\n","111: 0.22804899141192436 0.19739288464188576 0.4254418760538101\n","112: 0.21909908950328827 0.18510878458619118 0.40420787408947945\n","113: 0.23226068913936615 0.1735725086182356 0.40583319775760174\n","114: 0.2315451130270958 0.18238918110728264 0.41393429413437843\n","115: 0.23470884934067726 0.19048935174942017 0.4251982010900974\n","116: 0.22328870370984077 0.1710353773087263 0.3943240810185671\n","117: 0.24347936734557152 0.17201093398034573 0.41549030132591724\n","118: 0.22025995701551437 0.1857004314661026 0.405960388481617\n","119: 0.2124910205602646 0.19074905663728714 0.4032400771975517\n","120: 0.20684625580906868 0.16918085515499115 0.37602711096405983\n","121: 0.15422362089157104 0.18424046784639359 0.33846408873796463\n","122: 0.13610408268868923 0.17218885570764542 0.30829293839633465\n","123: 0.1974523663520813 0.17401331663131714 0.37146568298339844\n","124: 0.23349232599139214 0.17438166961073875 0.4078739956021309\n","125: 0.1470026969909668 0.17607155069708824 0.32307424768805504\n","126: 0.2433004155755043 0.18301541730761528 0.4263158328831196\n","127: 0.186961367726326 0.1683737188577652 0.3553350865840912\n","128: 0.18580037727952003 0.16825056076049805 0.3540509380400181\n","129: 0.2577941566705704 0.18218577653169632 0.4399799332022667\n","130: 0.27550794929265976 0.17299649864435196 0.4485044479370117\n","131: 0.2658719941973686 0.1812787540256977 0.44715074822306633\n","132: 0.21939726173877716 0.18069805949926376 0.4000953212380409\n","133: 0.24298866093158722 0.16574029065668583 0.40872895158827305\n","134: 0.23676880449056625 0.17353614419698715 0.4103049486875534\n","135: 0.21532434970140457 0.16498589143157005 0.3803102411329746\n","136: 0.2286180593073368 0.17673923820257187 0.4053572975099087\n","137: 0.28066663444042206 0.17574969120323658 0.45641632564365864\n","138: 0.23357199877500534 0.1610214151442051 0.39459341391921043\n","139: 0.18396279215812683 0.16309218853712082 0.34705498069524765\n","140: 0.18818861059844494 0.16368264332413673 0.3518712539225817\n","141: 0.2436613216996193 0.1683039702475071 0.4119652919471264\n","142: 0.22387995570898056 0.17464539408683777 0.39852534979581833\n","143: 0.20814666897058487 0.17015795782208443 0.3783046267926693\n","144: 0.22720395028591156 0.16861090809106827 0.39581485837697983\n","145: 0.13076069205999374 0.17322063446044922 0.30398132652044296\n","146: 0.16377639397978783 0.17095506191253662 0.33473145589232445\n","147: 0.22477388754487038 0.16382384300231934 0.3885977305471897\n","148: 0.2087549977004528 0.17090873420238495 0.37966373190283775\n","149: 0.23313889279961586 0.16494492441415787 0.3980838172137737\n","150: 0.2231326475739479 0.1786129865795374 0.4017456341534853\n","151: 0.1865425556898117 0.16249467432498932 0.349037230014801\n","152: 0.25979073345661163 0.16137460246682167 0.4211653359234333\n","153: 0.1881282813847065 0.16812745109200478 0.3562557324767113\n","154: 0.23878241702914238 0.1752167008817196 0.41399911791086197\n","155: 0.2941413074731827 0.1700737290084362 0.4642150364816189\n","156: 0.18859584257006645 0.16527513414621353 0.35387097671628\n","157: 0.21824946254491806 0.16950000822544098 0.38774947077035904\n","158: 0.21376415342092514 0.1874961368739605 0.40126029029488564\n","159: 0.2426551729440689 0.17056217789649963 0.41321735084056854\n","160: 0.17499285377562046 0.1761097051203251 0.35110255889594555\n","161: 0.22430521622300148 0.18880273029208183 0.4131079465150833\n","162: 0.3072489872574806 0.18332077376544476 0.4905697610229254\n","163: 0.18187087029218674 0.15973135270178318 0.3416022229939699\n","164: 0.2394297607243061 0.17825878411531448 0.4176885448396206\n","165: 0.20158181339502335 0.16390525922179222 0.36548707261681557\n","166: 0.21483591198921204 0.1718815490603447 0.38671746104955673\n","167: 0.27021075040102005 0.17698580957949162 0.44719655998051167\n","168: 0.24109621345996857 0.1650841049849987 0.40618031844496727\n","169: 0.2317645326256752 0.16587620973587036 0.39764074236154556\n","170: 0.18216896802186966 0.17749258503317833 0.359661553055048\n","171: 0.22244025394320488 0.15809972770512104 0.3805399816483259\n","172: 0.2093234397470951 0.17107023298740387 0.380393672734499\n","173: 0.18857527524232864 0.1654016710817814 0.35397694632411003\n","174: 0.21224941313266754 0.16595052927732468 0.3781999424099922\n","175: 0.22622708976268768 0.1626160964369774 0.38884318619966507\n","176: 0.22755150124430656 0.17495080828666687 0.40250230953097343\n","177: 0.20197322219610214 0.17008913680911064 0.3720623590052128\n","178: 0.2588410899043083 0.16220221668481827 0.4210433065891266\n","179: 0.24656236916780472 0.17282556369900703 0.41938793286681175\n","180: 0.18349400162696838 0.17773382365703583 0.3612278252840042\n","181: 0.22616185247898102 0.15871192514896393 0.38487377762794495\n","182: 0.18352869153022766 0.16347794979810715 0.3470066413283348\n","183: 0.22285713255405426 0.16090160608291626 0.3837587386369705\n","184: 0.19405831769108772 0.1746697761118412 0.3687280938029289\n","185: 0.30676840245723724 0.1812029518187046 0.48797135427594185\n","186: 0.26352909952402115 0.16127312555909157 0.4248022250831127\n","187: 0.208163533359766 0.1728430539369583 0.3810065872967243\n","188: 0.1200016438961029 0.16126689687371254 0.28126854076981544\n","189: 0.21863963454961777 0.16286202520132065 0.3815016597509384\n","190: 0.2544764578342438 0.17274200543761253 0.4272184632718563\n","191: 0.22767751663923264 0.17014002427458763 0.39781754091382027\n","192: 0.22670510783791542 0.1708657518029213 0.3975708596408367\n","193: 0.24906618893146515 0.16311082616448402 0.4121770150959492\n","194: 0.19029846042394638 0.17366839945316315 0.3639668598771095\n","195: 0.21118861064314842 0.16285080090165138 0.3740394115447998\n","196: 0.17852131091058254 0.1574020627886057 0.33592337369918823\n","197: 0.25652237609028816 0.1614481620490551 0.41797053813934326\n","198: 0.2295657843351364 0.17875723913311958 0.408323023468256\n","199: 0.20293214544653893 0.16322754323482513 0.36615968868136406\n","200: 0.27159786969423294 0.1741402968764305 0.44573816657066345\n","201: 0.1805901937186718 0.1617470569908619 0.3423372507095337\n","202: 0.2158820778131485 0.1744242087006569 0.3903062865138054\n","203: 0.22632737457752228 0.17444276437163353 0.4007701389491558\n","204: 0.22646725922822952 0.17447805777192116 0.4009453170001507\n","205: 0.2517027333378792 0.1678459793329239 0.41954871267080307\n","206: 0.202901192009449 0.17013590037822723 0.37303709238767624\n","207: 0.27813128754496574 0.16277502849698067 0.4409063160419464\n","208: 0.2338193766772747 0.17488376423716545 0.40870314091444016\n","209: 0.23021427169442177 0.17831334099173546 0.4085276126861572\n","210: 0.21154042333364487 0.16237041354179382 0.3739108368754387\n","211: 0.19160940498113632 0.16126558184623718 0.3528749868273735\n","212: 0.24966853111982346 0.18153388798236847 0.4312024191021919\n","213: 0.15180202014744282 0.1623968444764614 0.31419886462390423\n","214: 0.17094385623931885 0.15939398854970932 0.33033784478902817\n","215: 0.22542093694210052 0.16275037080049515 0.3881713077425957\n","216: 0.2524107024073601 0.16834428533911705 0.4207549877464771\n","217: 0.2119477055966854 0.16366949118673801 0.3756171967834234\n","218: 0.311224140226841 0.17723162099719048 0.48845576122403145\n","219: 0.17588273622095585 0.17360152676701546 0.3494842629879713\n","220: 0.1934014856815338 0.1607035957276821 0.3541050814092159\n","221: 0.24257348477840424 0.1696312427520752 0.41220472753047943\n","222: 0.1884872131049633 0.16135871037840843 0.34984592348337173\n","223: 0.23478662967681885 0.1604638658463955 0.39525049552321434\n","224: 0.2502918764948845 0.174268938601017 0.4245608150959015\n","225: 0.19643202796578407 0.1558600701391697 0.35229209810495377\n","226: 0.20263875648379326 0.16230280697345734 0.3649415634572506\n","227: 0.2427145056426525 0.17337696999311447 0.416091475635767\n","228: 0.19457443803548813 0.17283757403492928 0.3674120120704174\n","229: 0.2462172955274582 0.16810665279626846 0.41432394832372665\n","230: 0.2745431438088417 0.1881859079003334 0.4627290517091751\n","231: 0.23440753854811192 0.15812695026397705 0.39253448881208897\n","232: 0.2195778526365757 0.16010794416069984 0.37968579679727554\n","233: 0.2077173888683319 0.17174942418932915 0.37946681305766106\n","234: 0.19041791185736656 0.16074064373970032 0.3511585555970669\n","235: 0.18269401788711548 0.1687941737473011 0.3514881916344166\n","236: 0.18354887887835503 0.1753539852797985 0.35890286415815353\n","237: 0.2465978041291237 0.16343248635530472 0.4100302904844284\n","238: 0.22936413809657097 0.16727204620838165 0.3966361843049526\n","239: 0.20799253694713116 0.1783804502338171 0.38637298718094826\n","240: 0.2099819928407669 0.1688777171075344 0.3788597099483013\n","241: 0.20977497845888138 0.17311904951930046 0.38289402797818184\n","242: 0.2240467257797718 0.17336833104491234 0.39741505682468414\n","243: 0.21002864092588425 0.15545327961444855 0.3654819205403328\n","244: 0.21625973656773567 0.15988999977707863 0.3761497363448143\n","245: 0.2266220524907112 0.1719500944018364 0.3985721468925476\n","246: 0.1815529614686966 0.15958397835493088 0.34113693982362747\n","247: 0.2301991544663906 0.17750215530395508 0.4077013097703457\n","248: 0.2336518056690693 0.15470287017524242 0.3883546758443117\n","249: 0.26311736553907394 0.17266250401735306 0.435779869556427\n","250: 0.21315167844295502 0.15618648752570152 0.36933816596865654\n","251: 0.21130798757076263 0.159976776689291 0.37128476426005363\n","252: 0.29580941796302795 0.18002056889235973 0.4758299868553877\n","253: 0.16739719733595848 0.15515134669840336 0.32254854403436184\n","254: 0.23851220309734344 0.16899055242538452 0.40750275552272797\n","255: 0.2657066360116005 0.16831080242991447 0.43401743844151497\n","256: 0.22446762397885323 0.16349093616008759 0.3879585601389408\n","257: 0.22174771688878536 0.15869027376174927 0.38043799065053463\n","258: 0.19365829229354858 0.1583438701927662 0.3520021624863148\n","259: 0.20509517937898636 0.15659371577203274 0.3616888951510191\n","260: 0.2874601185321808 0.17207812517881393 0.4595382437109947\n","261: 0.19244002178311348 0.16716787591576576 0.35960789769887924\n","262: 0.1857745163142681 0.17130279541015625 0.35707731172442436\n","263: 0.20775265246629715 0.17751503735780716 0.3852676898241043\n","264: 0.18458621576428413 0.1600584276020527 0.3446446433663368\n","265: 0.2570282258093357 0.17568612843751907 0.4327143542468548\n","266: 0.22676944732666016 0.16392283514142036 0.3906922824680805\n","267: 0.21304266899824142 0.16435475274920464 0.37739742174744606\n","268: 0.24121542274951935 0.1780410259962082 0.41925644874572754\n","269: 0.19624077528715134 0.16137443482875824 0.3576152101159096\n","270: 0.19947942718863487 0.17256540060043335 0.3720448277890682\n","271: 0.2568582072854042 0.1617077961564064 0.4185660034418106\n","272: 0.21576690673828125 0.16489508748054504 0.3806619942188263\n","273: 0.19651034101843834 0.163715161383152 0.36022550240159035\n","274: 0.25112008303403854 0.17750399000942707 0.4286240730434656\n","275: 0.19942275807261467 0.16167208179831505 0.3610948398709297\n","276: 0.21831367909908295 0.1566001269966364 0.37491380609571934\n","277: 0.20558585226535797 0.17339494824409485 0.3789808005094528\n","278: 0.27396569959819317 0.16533894464373589 0.43930464424192905\n","279: 0.27219250053167343 0.18527955934405327 0.4574720598757267\n","280: 0.23043643683195114 0.16334970481693745 0.3937861416488886\n","281: 0.19546426460146904 0.16222578659653664 0.3576900511980057\n","282: 0.19938664138317108 0.16694379970431328 0.36633044108748436\n","283: 0.23739633709192276 0.15940932743251324 0.396805664524436\n","284: 0.1826912760734558 0.16843962669372559 0.3511309027671814\n","285: 0.2201726883649826 0.1576426848769188 0.3778153732419014\n","286: 0.2308504283428192 0.16084673255681992 0.39169716089963913\n","287: 0.21013182029128075 0.16189097985625267 0.3720228001475334\n","288: 0.2064693346619606 0.17052292078733444 0.37699225544929504\n","289: 0.19793394580483437 0.1666061393916607 0.36454008519649506\n","290: 0.24900589138269424 0.15729015693068504 0.4062960483133793\n","291: 0.19212115928530693 0.15269493125379086 0.3448160905390978\n","292: 0.24171433597803116 0.16941161826252937 0.41112595424056053\n","293: 0.1961400806903839 0.17291594669222832 0.36905602738261223\n","294: 0.17715734988451004 0.15485216118395329 0.3320095110684633\n","295: 0.21958579309284687 0.15505236573517323 0.3746381588280201\n","296: 0.22371910326182842 0.15766477212309837 0.3813838753849268\n","297: 0.25571849197149277 0.17425242066383362 0.4299709126353264\n","298: 0.22688127681612968 0.17160335928201675 0.39848463609814644\n","299: 0.22642478346824646 0.16415813006460667 0.3905829135328531\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625275268890,"user_tz":-60,"elapsed":10,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625275268890,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"6c2debf4-008a-4ccb-d181-281686359bae"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9824561403508771\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625275268891,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"29531f40-1f5c-4f06-be7f-c702d9cae82d"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9853372434017595\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625275269156,"user_tz":-60,"elapsed":269,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":12,"outputs":[]}]}
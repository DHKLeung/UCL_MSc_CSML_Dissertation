{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN nb1.0 aug WPL0.25.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625275218169,"user_tz":-60,"elapsed":20761,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"4f5a1ce9-77c0-4c29-9dac-9fae5d1b6041"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625275218991,"user_tz":-60,"elapsed":824,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625275219243,"user_tz":-60,"elapsed":253,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625275219244,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"c5670d11-9202-44e7-f1b8-6544fea02e09"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","geometric_param = 1.\n","perturb_loss_weight = 0.25\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f2fccfeda70>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625275219901,"user_tz":-60,"elapsed":659,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625275226187,"user_tz":-60,"elapsed":6288,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"e492ff27-fb30-49b5-a652-2f99c3821f83"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1625275226188,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"id":"quiet-module","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1625275226188,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625275229925,"user_tz":-60,"elapsed":3739,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"a6a1c496-3d7f-4a66-c9c8-8a32bdc79850"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup with random neighbour perturbation #\n","        mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_breast_nb(inputs, labels, geometric_param, alpha)\n","        \n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_nb))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_nb = augment_outputs[original_num:]\n","        mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_nb + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_nb.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_nb.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print decomposed losses #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:151: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n","  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.1554341316223145 2.149895131587982 4.305329263210297\n","1: 2.1121495366096497 2.1106187105178833 4.222768247127533\n","2: 2.0540689826011658 2.050315499305725 4.104384481906891\n","3: 1.973490834236145 1.9733039140701294 3.9467947483062744\n","4: 1.8849290609359741 1.8785014748573303 3.7634305357933044\n","5: 1.7802568078041077 1.7814823389053345 3.561739146709442\n","6: 1.6757473945617676 1.6789100170135498 3.3546574115753174\n","7: 1.5627792477607727 1.565669298171997 3.1284485459327698\n","8: 1.4436377584934235 1.4510639011859894 2.894701659679413\n","9: 1.323561817407608 1.3295493125915527 2.6531111299991608\n","10: 1.183613508939743 1.213371753692627 2.39698526263237\n","11: 1.0802726447582245 1.0966347754001617 2.1769074201583862\n","12: 0.9713384807109833 0.9875749349594116 1.958913415670395\n","13: 0.8383261263370514 0.871190994977951 1.7095171213150024\n","14: 0.7655259370803833 0.777961015701294 1.5434869527816772\n","15: 0.6669168323278427 0.6935901343822479 1.3605069667100906\n","16: 0.6260523051023483 0.6322561353445053 1.2583084404468536\n","17: 0.5538985729217529 0.5730926245450974 1.1269911974668503\n","18: 0.5107143670320511 0.5187184810638428 1.0294328480958939\n","19: 0.4607042372226715 0.47496266663074493 0.9356669038534164\n","20: 0.46615883708000183 0.43996237218379974 0.9061212092638016\n","21: 0.44725871831178665 0.4183986485004425 0.8656573668122292\n","22: 0.3917553499341011 0.40414954721927643 0.7959048971533775\n","23: 0.3594152703881264 0.37044278532266617 0.7298580557107925\n","24: 0.35654016584157944 0.3557950034737587 0.7123351693153381\n","25: 0.33061134070158005 0.3224584385752678 0.6530697792768478\n","26: 0.31441008299589157 0.31719905883073807 0.6316091418266296\n","27: 0.2838018909096718 0.30615297704935074 0.5899548679590225\n","28: 0.30202680081129074 0.293022058904171 0.5950488597154617\n","29: 0.2536962255835533 0.2787218317389488 0.5324180573225021\n","30: 0.250813752412796 0.2626490034162998 0.5134627558290958\n","31: 0.30272579193115234 0.25565341487526894 0.5583792068064213\n","32: 0.29237081855535507 0.25578632950782776 0.5481571480631828\n","33: 0.28494448959827423 0.2510944530367851 0.5360389426350594\n","34: 0.23758982867002487 0.23762303963303566 0.47521286830306053\n","35: 0.28324470669031143 0.24913578480482101 0.5323804914951324\n","36: 0.2462790384888649 0.2206004410982132 0.4668794795870781\n","37: 0.2021898776292801 0.22656931728124619 0.4287591949105263\n","38: 0.2599877566099167 0.2312181070446968 0.4912058636546135\n","39: 0.20268166065216064 0.2101149708032608 0.41279663145542145\n","40: 0.2413252666592598 0.2131609097123146 0.4544861763715744\n","41: 0.256451740860939 0.2006724402308464 0.45712418109178543\n","42: 0.1965247504413128 0.19711101800203323 0.393635768443346\n","43: 0.258459709584713 0.2028660885989666 0.4613257981836796\n","44: 0.2606990784406662 0.20099228620529175 0.46169136464595795\n","45: 0.20502710342407227 0.19269516319036484 0.3977222666144371\n","46: 0.20551618933677673 0.20113414898514748 0.4066503383219242\n","47: 0.2046792432665825 0.18291420117020607 0.38759344443678856\n","48: 0.22897547483444214 0.19934232160449028 0.4283177964389324\n","49: 0.21943160891532898 0.18705657497048378 0.40648818388581276\n","50: 0.22995589673519135 0.17960695177316666 0.409562848508358\n","51: 0.20034565776586533 0.16756457835435867 0.367910236120224\n","52: 0.20871349051594734 0.18087618052959442 0.38958967104554176\n","53: 0.2251264974474907 0.17984963580965996 0.40497613325715065\n","54: 0.22590141743421555 0.1692325584590435 0.39513397589325905\n","55: 0.23382069915533066 0.19008957594633102 0.4239102751016617\n","56: 0.21949511766433716 0.17342406883835793 0.3929191865026951\n","57: 0.2009783461689949 0.15629252046346664 0.35727086663246155\n","58: 0.1946653351187706 0.15584706515073776 0.35051240026950836\n","59: 0.20851421356201172 0.16622734814882278 0.3747415617108345\n","60: 0.20494602620601654 0.1631212681531906 0.36806729435920715\n","61: 0.203140489757061 0.15411294996738434 0.35725343972444534\n","62: 0.19772472605109215 0.14946625009179115 0.3471909761428833\n","63: 0.2588813975453377 0.14640608057379723 0.4052874781191349\n","64: 0.1917925514280796 0.15888653695583344 0.35067908838391304\n","65: 0.1745787300169468 0.1507927030324936 0.3253714330494404\n","66: 0.2067178562283516 0.152377150952816 0.3590950071811676\n","67: 0.23155590891838074 0.14206433296203613 0.37362024188041687\n","68: 0.1638236977159977 0.1481935903429985 0.3120172880589962\n","69: 0.19235852360725403 0.14743410050868988 0.3397926241159439\n","70: 0.20508763194084167 0.14793985709547997 0.35302748903632164\n","71: 0.2181529998779297 0.1427612006664276 0.3609142005443573\n","72: 0.20728185772895813 0.14300477504730225 0.3502866327762604\n","73: 0.1826539896428585 0.13777994737029076 0.32043393701314926\n","74: 0.17585937678813934 0.13149229623377323 0.3073516730219126\n","75: 0.13319425284862518 0.12902166321873665 0.26221591606736183\n","76: 0.19008320569992065 0.12826533243060112 0.3183485381305218\n","77: 0.1408037282526493 0.12792743183672428 0.2687311600893736\n","78: 0.17229440063238144 0.13186562806367874 0.3041600286960602\n","79: 0.23765311762690544 0.14313016273081303 0.38078328035771847\n","80: 0.15447108447551727 0.12304679863154888 0.27751788310706615\n","81: 0.15471238270401955 0.12232637591660023 0.2770387586206198\n","82: 0.19541536644101143 0.12675758078694344 0.32217294722795486\n","83: 0.1378842405974865 0.12276196479797363 0.26064620539546013\n","84: 0.17041417583823204 0.1301971673965454 0.30061134323477745\n","85: 0.15048976987600327 0.11994826979935169 0.27043803967535496\n","86: 0.1754874736070633 0.12225926294922829 0.2977467365562916\n","87: 0.17464431002736092 0.13261860981583595 0.30726291984319687\n","88: 0.13595865294337273 0.1257822960615158 0.26174094900488853\n","89: 0.2274823561310768 0.12459093146026134 0.35207328759133816\n","90: 0.2162861004471779 0.11526763439178467 0.33155373483896255\n","91: 0.13166562840342522 0.11506647802889347 0.2467321064323187\n","92: 0.1867986135184765 0.11302619613707066 0.29982480965554714\n","93: 0.15098601207137108 0.11034920066595078 0.26133521273732185\n","94: 0.12711305171251297 0.11626759544014931 0.24338064715266228\n","95: 0.1516702063381672 0.10933875478804111 0.2610089611262083\n","96: 0.2590405344963074 0.10935892537236214 0.3683994598686695\n","97: 0.15728678554296494 0.11355920508503914 0.2708459906280041\n","98: 0.24385501444339752 0.11794715002179146 0.361802164465189\n","99: 0.186648927628994 0.10800968110561371 0.2946586087346077\n","100: 0.1606845110654831 0.10471881926059723 0.2654033303260803\n","101: 0.20100882276892662 0.10716831125319004 0.30817713402211666\n","102: 0.15373678877949715 0.10530515946447849 0.25904194824397564\n","103: 0.2006840519607067 0.1024331133812666 0.3031171653419733\n","104: 0.1284376084804535 0.10893112793564796 0.23736873641610146\n","105: 0.1799585409462452 0.10508588515222073 0.2850444260984659\n","106: 0.20713284611701965 0.11998319625854492 0.3271160423755646\n","107: 0.18356356769800186 0.10574595630168915 0.289309523999691\n","108: 0.1843009702861309 0.10333649069070816 0.28763746097683907\n","109: 0.2365996167063713 0.10855431854724884 0.34515393525362015\n","110: 0.1316813062876463 0.11166608333587646 0.24334738962352276\n","111: 0.19342635571956635 0.11026961728930473 0.3036959730088711\n","112: 0.12997845001518726 0.09866764396429062 0.22864609397947788\n","113: 0.19245220348238945 0.0970744639635086 0.28952666744589806\n","114: 0.13654515147209167 0.09794892184436321 0.2344940733164549\n","115: 0.1672658994793892 0.0976891852915287 0.2649550847709179\n","116: 0.20157203078269958 0.09669432230293751 0.2982663530856371\n","117: 0.19538378715515137 0.10916468501091003 0.3045484721660614\n","118: 0.2344398982822895 0.09637372195720673 0.33081362023949623\n","119: 0.17821092903614044 0.10344459488987923 0.28165552392601967\n","120: 0.13814174756407738 0.10113822482526302 0.2392799723893404\n","121: 0.12725862860679626 0.09955355804413557 0.22681218665093184\n","122: 0.1399909146130085 0.0954467672854662 0.2354376818984747\n","123: 0.13478239625692368 0.08905315399169922 0.2238355502486229\n","124: 0.1469261683523655 0.09753558784723282 0.2444617561995983\n","125: 0.2002926729619503 0.09172433242201805 0.29201700538396835\n","126: 0.15278762392699718 0.09497567638754845 0.24776330031454563\n","127: 0.1694091409444809 0.09123476315289736 0.26064390409737825\n","128: 0.17674874141812325 0.08953451178967953 0.2662832532078028\n","129: 0.1673619318753481 0.08775617741048336 0.25511810928583145\n","130: 0.1810213066637516 0.08695495873689651 0.2679762654006481\n","131: 0.1973024606704712 0.09007063321769238 0.28737309388816357\n","132: 0.24100577272474766 0.103599414229393 0.34460518695414066\n","133: 0.2219318524003029 0.0877985879778862 0.3097304403781891\n","134: 0.1671098954975605 0.0975518561899662 0.2646617516875267\n","135: 0.24370383098721504 0.09246513061225414 0.3361689615994692\n","136: 0.1443237103521824 0.08413160312920809 0.22845531348139048\n","137: 0.19482863694429398 0.08741728588938713 0.2822459228336811\n","138: 0.18977847322821617 0.0855116918683052 0.2752901650965214\n","139: 0.15397246927022934 0.0853387750685215 0.23931124433875084\n","140: 0.1982683390378952 0.08651216700673103 0.28478050604462624\n","141: 0.19861747696995735 0.09150602668523788 0.29012350365519524\n","142: 0.1435067355632782 0.08201947808265686 0.22552621364593506\n","143: 0.11304636485874653 0.08531485684216022 0.19836122170090675\n","144: 0.14774741511791945 0.08246085047721863 0.23020826559513807\n","145: 0.17488318774849176 0.08307054173201323 0.257953729480505\n","146: 0.15002577286213636 0.09326080046594143 0.2432865733280778\n","147: 0.1422629039734602 0.09122483339160681 0.233487737365067\n","148: 0.1383359506726265 0.08316044695675373 0.22149639762938023\n","149: 0.17730653285980225 0.0928563941270113 0.27016292698681355\n","150: 0.19155246950685978 0.09259521216154099 0.28414768166840076\n","151: 0.18773087114095688 0.08040593937039375 0.26813681051135063\n","152: 0.15531433373689651 0.08638262748718262 0.24169696122407913\n","153: 0.14017830044031143 0.07782627642154694 0.21800457686185837\n","154: 0.15749423578381538 0.09262899123132229 0.2501232270151377\n","155: 0.22193803638219833 0.0810722392052412 0.30301027558743954\n","156: 0.20281889289617538 0.07942136749625206 0.28224026039242744\n","157: 0.13613413460552692 0.0824139453470707 0.21854807995259762\n","158: 0.15896495059132576 0.0891667464748025 0.24813169706612825\n","159: 0.1968923732638359 0.07918944209814072 0.2760818153619766\n","160: 0.1718474105000496 0.09024108946323395 0.26208849996328354\n","161: 0.1269413586705923 0.0826835073530674 0.2096248660236597\n","162: 0.25029586255550385 0.08797962218523026 0.3382754847407341\n","163: 0.17122745886445045 0.07986526750028133 0.2510927263647318\n","164: 0.20617805048823357 0.09090711176395416 0.29708516225218773\n","165: 0.16015474870800972 0.08946927450597286 0.24962402321398258\n","166: 0.17529612593352795 0.08179606311023235 0.2570921890437603\n","167: 0.15933452919125557 0.08320760913193226 0.24254213832318783\n","168: 0.14514882117509842 0.08201182261109352 0.22716064378619194\n","169: 0.16673165187239647 0.07774903904646635 0.24448069091886282\n","170: 0.13142748922109604 0.07723880093544722 0.20866629015654325\n","171: 0.20596574805676937 0.08196048717945814 0.2879262352362275\n","172: 0.13729367777705193 0.0888450313359499 0.22613870911300182\n","173: 0.13697930052876472 0.07763627264648676 0.21461557317525148\n","174: 0.15388034842908382 0.09091331250965595 0.24479366093873978\n","175: 0.24446697533130646 0.07976578548550606 0.3242327608168125\n","176: 0.16745653003454208 0.07696706242859364 0.24442359246313572\n","177: 0.10428357496857643 0.07999470457434654 0.18427827954292297\n","178: 0.2644680514931679 0.084443723782897 0.3489117752760649\n","179: 0.17466897517442703 0.08100935444235802 0.25567832961678505\n","180: 0.11778444331139326 0.08019422180950642 0.19797866512089968\n","181: 0.1676761470735073 0.08193940669298172 0.24961555376648903\n","182: 0.19598610885441303 0.08888197224587202 0.28486808110028505\n","183: 0.13857748359441757 0.08641912788152695 0.22499661147594452\n","184: 0.13937813229858875 0.09402409195899963 0.2334022242575884\n","185: 0.21499153226613998 0.08940238133072853 0.3043939135968685\n","186: 0.17484081536531448 0.07923579216003418 0.25407660752534866\n","187: 0.1352084055542946 0.08208348229527473 0.21729188784956932\n","188: 0.10944540984928608 0.07949146628379822 0.1889368761330843\n","189: 0.16116183064877987 0.0839077029377222 0.24506953358650208\n","190: 0.21975936740636826 0.08077607117593288 0.30053543858230114\n","191: 0.1195654608309269 0.07925394736230373 0.19881940819323063\n","192: 0.19598452746868134 0.07919009774923325 0.2751746252179146\n","193: 0.17914487421512604 0.07838683854788542 0.25753171276301146\n","194: 0.15380621515214443 0.08143929857760668 0.2352455137297511\n","195: 0.10475037712603807 0.07856612280011177 0.18331649992614985\n","196: 0.14832482859492302 0.08181273099035025 0.23013755958527327\n","197: 0.180520661175251 0.0858453307300806 0.2663659919053316\n","198: 0.174498088657856 0.07841632980853319 0.2529144184663892\n","199: 0.1584179624915123 0.07989139668643475 0.23830935917794704\n","200: 0.15507589280605316 0.09156650863587856 0.24664240144193172\n","201: 0.193015418946743 0.07806225400418043 0.27107767295092344\n","202: 0.15884535014629364 0.07873917184770107 0.2375845219939947\n","203: 0.17201174888759851 0.0885718371719122 0.2605835860595107\n","204: 0.17617850378155708 0.07896725460886955 0.25514575839042664\n","205: 0.11734915897250175 0.08107485622167587 0.19842401519417763\n","206: 0.1466357298195362 0.07851543836295605 0.22515116818249226\n","207: 0.13614422548562288 0.07651772536337376 0.21266195084899664\n","208: 0.22745117917656898 0.08449726644903421 0.3119484456256032\n","209: 0.13334347866475582 0.08858798444271088 0.2219314631074667\n","210: 0.14941724762320518 0.08736405614763498 0.23678130377084017\n","211: 0.18076936528086662 0.08254982344806194 0.26331918872892857\n","212: 0.1402042619884014 0.08156810142099857 0.2217723634094\n","213: 0.09691281244158745 0.08041264116764069 0.17732545360922813\n","214: 0.13684970140457153 0.08194432966411114 0.21879403106868267\n","215: 0.16611207462847233 0.08764309529215097 0.2537551699206233\n","216: 0.17276863008737564 0.07917840220034122 0.25194703228771687\n","217: 0.12991589680314064 0.09205628372728825 0.2219721805304289\n","218: 0.14930038526654243 0.08834579773247242 0.23764618299901485\n","219: 0.2002086965367198 0.0811432283371687 0.2813519248738885\n","220: 0.17526008188724518 0.07741758320480585 0.25267766509205103\n","221: 0.1926344744861126 0.08136958070099354 0.27400405518710613\n","222: 0.2570995520800352 0.07745658792555332 0.33455614000558853\n","223: 0.13331793248653412 0.09219812601804733 0.22551605850458145\n","224: 0.1961989514529705 0.08820690028369427 0.28440585173666477\n","225: 0.14502883702516556 0.08007434476166964 0.2251031817868352\n","226: 0.1688367472961545 0.07527258340269327 0.24410933069884777\n","227: 0.15085472911596298 0.07696014735847712 0.2278148764744401\n","228: 0.19483833014965057 0.07663685455918312 0.2714751847088337\n","229: 0.14359256997704506 0.07864450849592686 0.22223707847297192\n","230: 0.1621304415166378 0.08746746741235256 0.24959790892899036\n","231: 0.16599969007074833 0.0775014366954565 0.24350112676620483\n","232: 0.0876105148345232 0.08935938775539398 0.17696990258991718\n","233: 0.21246061101555824 0.07933893986046314 0.2917995508760214\n","234: 0.13697129115462303 0.08241941407322884 0.21939070522785187\n","235: 0.11048877611756325 0.07520600873976946 0.1856947848573327\n","236: 0.10729450825601816 0.07502877898514271 0.18232328724116087\n","237: 0.18823406845331192 0.08956864289939404 0.27780271135270596\n","238: 0.18026233185082674 0.09181268326938152 0.27207501512020826\n","239: 0.13140081614255905 0.08074911497533321 0.21214993111789227\n","240: 0.23922326415777206 0.08697791397571564 0.3262011781334877\n","241: 0.18826374784111977 0.0783845093101263 0.26664825715124607\n","242: 0.1684700958430767 0.07887480407953262 0.24734489992260933\n","243: 0.1375071257352829 0.07737772725522518 0.21488485299050808\n","244: 0.16552109457552433 0.07399760512635112 0.23951869970187545\n","245: 0.14457513950765133 0.07648678962141275 0.22106192912906408\n","246: 0.1142423264682293 0.08252355083823204 0.19676587730646133\n","247: 0.19260308519005775 0.08992120902985334 0.2825242942199111\n","248: 0.1478234827518463 0.07994943112134933 0.22777291387319565\n","249: 0.15468283370137215 0.08693497441709042 0.24161780811846256\n","250: 0.17591341584920883 0.07452217116951942 0.25043558701872826\n","251: 0.22124119848012924 0.0822137463837862 0.30345494486391544\n","252: 0.19591336324810982 0.0790993981063366 0.2750127613544464\n","253: 0.09553450532257557 0.07458693534135818 0.17012144066393375\n","254: 0.24281398952007294 0.07898512110114098 0.3217991106212139\n","255: 0.24287205934524536 0.07636827137321234 0.3192403307184577\n","256: 0.1744309403002262 0.0870776828378439 0.2615086231380701\n","257: 0.1752384826540947 0.07550410740077496 0.25074259005486965\n","258: 0.12686168029904366 0.07617055252194405 0.2030322328209877\n","259: 0.12254087813198566 0.07758013810962439 0.20012101624161005\n","260: 0.19622752256691456 0.09092923905700445 0.287156761623919\n","261: 0.1379585564136505 0.0873776376247406 0.2253361940383911\n","262: 0.16751138865947723 0.08135111629962921 0.24886250495910645\n","263: 0.17463402077555656 0.09148312732577324 0.2661171481013298\n","264: 0.10787559673190117 0.07696693763136864 0.1848425343632698\n","265: 0.12558435089886189 0.08925377577543259 0.21483812667429447\n","266: 0.17863930016756058 0.07806121185421944 0.25670051202178\n","267: 0.09186539053916931 0.07448411360383034 0.16634950414299965\n","268: 0.1630486249923706 0.08598718978464603 0.24903581477701664\n","269: 0.20766842179000378 0.07789415307343006 0.28556257486343384\n","270: 0.11820372752845287 0.07737479917705059 0.19557852670550346\n","271: 0.22709226980805397 0.07741032354533672 0.3045025933533907\n","272: 0.1503649540245533 0.08839535713195801 0.2387603111565113\n","273: 0.1346414629369974 0.0779083538800478 0.2125498168170452\n","274: 0.1721331849694252 0.07727834582328796 0.24941153079271317\n","275: 0.13272585719823837 0.0782576221972704 0.21098347939550877\n","276: 0.17709306627511978 0.0782591225579381 0.2553521888330579\n","277: 0.15209905803203583 0.07897291239351034 0.23107197042554617\n","278: 0.169286142103374 0.0877884291112423 0.2570745712146163\n","279: 0.19474804401397705 0.07715089619159698 0.27189894020557404\n","280: 0.1670103557407856 0.08673097938299179 0.2537413351237774\n","281: 0.17510506510734558 0.07527818065136671 0.2503832457587123\n","282: 0.17405440285801888 0.07527292799204588 0.24932733085006475\n","283: 0.1788138933479786 0.07820094842463732 0.2570148417726159\n","284: 0.192096509039402 0.07640309259295464 0.26849960163235664\n","285: 0.22337907552719116 0.07608277350664139 0.29946184903383255\n","286: 0.13100677356123924 0.07504279259592295 0.2060495661571622\n","287: 0.14848366752266884 0.08486996218562126 0.2333536297082901\n","288: 0.16952048055827618 0.0750935897231102 0.24461407028138638\n","289: 0.17547681741416454 0.07385644596070051 0.24933326337486506\n","290: 0.2660824917256832 0.0800546258687973 0.3461371175944805\n","291: 0.1088704876601696 0.07535974122583866 0.18423022888600826\n","292: 0.24255451187491417 0.07288437150418758 0.31543888337910175\n","293: 0.17303097248077393 0.07945806346833706 0.252489035949111\n","294: 0.20376181975007057 0.07750847004354 0.2812702897936106\n","295: 0.10899706184864044 0.07523153070360422 0.18422859255224466\n","296: 0.1654965616762638 0.08783696964383125 0.25333353132009506\n","297: 0.20340874791145325 0.08411900326609612 0.28752775117754936\n","298: 0.14148472994565964 0.08727921918034554 0.22876394912600517\n","299: 0.17835788056254387 0.08896012231707573 0.2673180028796196\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625275229926,"user_tz":-60,"elapsed":10,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625275229926,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"952a821e-f604-4eee-bffd-f5d6de6ee157"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9736842105263158\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625275230276,"user_tz":-60,"elapsed":354,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"7b18b241-e617-4118-ec13-640709351471"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9970674486803519\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625275230276,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":12,"outputs":[]}]}
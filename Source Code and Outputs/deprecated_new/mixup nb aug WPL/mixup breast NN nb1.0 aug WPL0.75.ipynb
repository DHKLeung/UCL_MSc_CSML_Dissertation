{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN nb1.0 aug WPL0.75.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625275232718,"user_tz":-60,"elapsed":33982,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"e4ebdb0f-e407-4b12-a3b1-68601b84d407"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625275233608,"user_tz":-60,"elapsed":892,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625275234144,"user_tz":-60,"elapsed":537,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625275234144,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"a54b76cf-6021-450d-fea7-48ae5735f18a"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","geometric_param = 1.\n","perturb_loss_weight = 0.75\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fa184c04a50>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625275234440,"user_tz":-60,"elapsed":298,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625275240759,"user_tz":-60,"elapsed":6320,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"e6c376dd-9e35-4408-bb99-10cd3e93ae55"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1625275240759,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"id":"quiet-module","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1625275240760,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625275245768,"user_tz":-60,"elapsed":5011,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"a6292250-a6be-4e0d-e9b9-21eec39061f5"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup with random neighbour perturbation #\n","        mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_breast_nb(inputs, labels, geometric_param, alpha)\n","        \n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_nb))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_nb = augment_outputs[original_num:]\n","        mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_nb + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_nb.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_nb.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print decomposed losses #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:151: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n","  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.1490453481674194 2.144520103931427 4.293565452098846\n","1: 2.1112778186798096 2.1085758805274963 4.219853699207306\n","2: 2.052563786506653 2.048723042011261 4.101286828517914\n","3: 1.97309410572052 1.9728952050209045 3.9459893107414246\n","4: 1.8807622194290161 1.8837562799453735 3.7645184993743896\n","5: 1.7857717871665955 1.7920429110527039 3.5778146982192993\n","6: 1.684672772884369 1.6881643533706665 3.3728371262550354\n","7: 1.5795400738716125 1.586699903011322 3.1662399768829346\n","8: 1.4634199440479279 1.4672513902187347 2.9306713342666626\n","9: 1.3443845212459564 1.3622526228427887 2.706637144088745\n","10: 1.206708699464798 1.2350686490535736 2.4417773485183716\n","11: 1.10326486825943 1.1327637732028961 2.236028641462326\n","12: 1.0025906562805176 1.0168292820453644 2.019419938325882\n","13: 0.8634504973888397 0.9097862839698792 1.7732367813587189\n","14: 0.8049543797969818 0.8324391841888428 1.6373935639858246\n","15: 0.7126833945512772 0.7348804175853729 1.44756381213665\n","16: 0.642917200922966 0.6840369701385498 1.3269541710615158\n","17: 0.6143821179866791 0.6296361684799194 1.2440182864665985\n","18: 0.5840394049882889 0.5788717418909073 1.1629111468791962\n","19: 0.5027101188898087 0.5346122533082962 1.0373223721981049\n","20: 0.5048991143703461 0.5167000740766525 1.0215991884469986\n","21: 0.4404726028442383 0.46961046755313873 0.910083070397377\n","22: 0.4622451812028885 0.443616159260273 0.9058613404631615\n","23: 0.46919524669647217 0.45452672243118286 0.923721969127655\n","24: 0.4184165894985199 0.43046604096889496 0.8488826304674149\n","25: 0.38745198398828506 0.39912983775138855 0.7865818217396736\n","26: 0.36668162792921066 0.3717353790998459 0.7384170070290565\n","27: 0.3016050010919571 0.3566995933651924 0.6583045944571495\n","28: 0.35742176324129105 0.3604222238063812 0.7178439870476723\n","29: 0.3287312537431717 0.3321133479475975 0.6608446016907692\n","30: 0.3244803547859192 0.33509907871484756 0.6595794335007668\n","31: 0.38081901520490646 0.33061157912015915 0.7114305943250656\n","32: 0.30679961293935776 0.29583531245589256 0.6026349253952503\n","33: 0.3264138624072075 0.30115216225385666 0.6275660246610641\n","34: 0.28907517343759537 0.3002583011984825 0.5893334746360779\n","35: 0.36547984927892685 0.29975786805152893 0.6652377173304558\n","36: 0.28499504923820496 0.3008875772356987 0.5858826264739037\n","37: 0.29232922196388245 0.28720515221357346 0.5795343741774559\n","38: 0.28934812545776367 0.2724212259054184 0.5617693513631821\n","39: 0.30387435480952263 0.2977400906383991 0.6016144454479218\n","40: 0.2984856367111206 0.27365827560424805 0.5721439123153687\n","41: 0.28223414346575737 0.29165084287524223 0.5738849863409996\n","42: 0.2328784540295601 0.26489243656396866 0.49777089059352875\n","43: 0.3010592423379421 0.2918379455804825 0.5928971879184246\n","44: 0.2904411479830742 0.25389404594898224 0.5443351939320564\n","45: 0.24526416137814522 0.2511930726468563 0.4964572340250015\n","46: 0.2789459228515625 0.24926990270614624 0.5282158255577087\n","47: 0.2704551964998245 0.2501988857984543 0.5206540822982788\n","48: 0.2383335828781128 0.24754874408245087 0.48588232696056366\n","49: 0.2830672785639763 0.24631323665380478 0.5293805152177811\n","50: 0.259285643696785 0.2667783610522747 0.5260640047490597\n","51: 0.32698075100779533 0.2672002278268337 0.5941809788346291\n","52: 0.2424325793981552 0.2314029522240162 0.4738355316221714\n","53: 0.33872056007385254 0.24367301166057587 0.5823935717344284\n","54: 0.20577320083975792 0.24281060695648193 0.44858380779623985\n","55: 0.22412415966391563 0.23788310214877129 0.4620072618126869\n","56: 0.2684889957308769 0.2276143953204155 0.4961033910512924\n","57: 0.3034852221608162 0.25533101335167885 0.558816235512495\n","58: 0.37233997881412506 0.24295317381620407 0.6152931526303291\n","59: 0.3381313234567642 0.2326267696917057 0.5707580931484699\n","60: 0.22973191738128662 0.22303390502929688 0.4527658224105835\n","61: 0.21890050545334816 0.22471511363983154 0.4436156190931797\n","62: 0.2797708809375763 0.22402352094650269 0.503794401884079\n","63: 0.3090754598379135 0.23761386424303055 0.5466893240809441\n","64: 0.23664460331201553 0.22693075984716415 0.4635753631591797\n","65: 0.2397465631365776 0.23077622801065445 0.47052279114723206\n","66: 0.2725251317024231 0.23295015841722488 0.505475290119648\n","67: 0.26237331330776215 0.23925703018903732 0.5016303434967995\n","68: 0.23336387425661087 0.20802497118711472 0.4413888454437256\n","69: 0.22712400555610657 0.2134866826236248 0.44061068817973137\n","70: 0.21118802577257156 0.21288534998893738 0.42407337576150894\n","71: 0.28018398582935333 0.205661378800869 0.4858453646302223\n","72: 0.2806430980563164 0.2033371962606907 0.48398029431700706\n","73: 0.3036285862326622 0.2125444859266281 0.5161730721592903\n","74: 0.27861568331718445 0.2031955011188984 0.48181118443608284\n","75: 0.1731739528477192 0.1980032715946436 0.3711772244423628\n","76: 0.23286914080381393 0.19879654794931412 0.43166568875312805\n","77: 0.21165183931589127 0.20020341873168945 0.4118552580475807\n","78: 0.19280382245779037 0.2141597680747509 0.4069635905325413\n","79: 0.24567146599292755 0.19935841113328934 0.4450298771262169\n","80: 0.2202945053577423 0.21671124547719955 0.43700575083494186\n","81: 0.17141962423920631 0.21408062800765038 0.3855002522468567\n","82: 0.2750691846013069 0.20750582590699196 0.4825750105082989\n","83: 0.24741239100694656 0.19022849015891552 0.4376408811658621\n","84: 0.22838333994150162 0.20472058653831482 0.43310392647981644\n","85: 0.2335834875702858 0.20694393292069435 0.44052742049098015\n","86: 0.19601882994174957 0.19016512855887413 0.3861839585006237\n","87: 0.253777077421546 0.19792960211634636 0.45170667953789234\n","88: 0.2279253527522087 0.1965474747121334 0.4244728274643421\n","89: 0.24059071391820908 0.18632684834301472 0.4269175622612238\n","90: 0.2541264593601227 0.20041139796376228 0.45453785732388496\n","91: 0.2600015327334404 0.21276652812957764 0.47276806086301804\n","92: 0.27147623896598816 0.1970491111278534 0.46852535009384155\n","93: 0.24524186737835407 0.18497325107455254 0.4302151184529066\n","94: 0.16963111609220505 0.18668566644191742 0.35631678253412247\n","95: 0.24910374730825424 0.1942349560558796 0.44333870336413383\n","96: 0.2674527168273926 0.1845199055969715 0.4519726224243641\n","97: 0.2674781493842602 0.2049485668540001 0.47242671623826027\n","98: 0.15093683451414108 0.18439358472824097 0.33533041924238205\n","99: 0.20607735216617584 0.1810448206961155 0.38712217286229134\n","100: 0.2884051725268364 0.20485617220401764 0.49326134473085403\n","101: 0.2261291965842247 0.2002488411962986 0.4263780377805233\n","102: 0.2213200107216835 0.1836528778076172 0.4049728885293007\n","103: 0.17415305227041245 0.1878991723060608 0.36205222457647324\n","104: 0.2255459725856781 0.18133436888456345 0.40688034147024155\n","105: 0.26857107877731323 0.19793299585580826 0.4665040746331215\n","106: 0.2427445650100708 0.19883232191205025 0.44157688692212105\n","107: 0.2568998336791992 0.18337024562060833 0.44027007929980755\n","108: 0.2412196770310402 0.18033422157168388 0.4215538986027241\n","109: 0.19811557978391647 0.17929522693157196 0.37741080671548843\n","110: 0.24438059143722057 0.18640170618891716 0.43078229762613773\n","111: 0.24289031326770782 0.21145622432231903 0.45434653759002686\n","112: 0.20782020315527916 0.18156415969133377 0.38938436284661293\n","113: 0.26808422803878784 0.1987454853951931 0.46682971343398094\n","114: 0.2295403964817524 0.18301578983664513 0.4125561863183975\n","115: 0.203736811876297 0.1686735488474369 0.3724103607237339\n","116: 0.2860396057367325 0.18303586915135384 0.4690754748880863\n","117: 0.28622908145189285 0.18886512145400047 0.4750942029058933\n","118: 0.25298701971769333 0.18402983620762825 0.4370168559253216\n","119: 0.1863635666668415 0.17668014764785767 0.3630437143146992\n","120: 0.2803620845079422 0.18635967373847961 0.4667217582464218\n","121: 0.15708240494132042 0.1730736568570137 0.3301560617983341\n","122: 0.15062812715768814 0.1723236721009016 0.32295179925858974\n","123: 0.16947653889656067 0.1737246960401535 0.3432012349367142\n","124: 0.22084449976682663 0.17490766197443008 0.3957521617412567\n","125: 0.18437446281313896 0.16970841214060783 0.3540828749537468\n","126: 0.21874189376831055 0.20062199048697948 0.41936388425529003\n","127: 0.14971963316202164 0.1704278513789177 0.32014748454093933\n","128: 0.2631338909268379 0.18406178429722786 0.4471956752240658\n","129: 0.23689281940460205 0.17144770547747612 0.40834052488207817\n","130: 0.20405679754912853 0.17457513324916363 0.37863193079829216\n","131: 0.2152642346918583 0.16876913979649544 0.38403337448835373\n","132: 0.2301405742764473 0.18596800416707993 0.4161085784435272\n","133: 0.2587174102663994 0.17041851207613945 0.42913592234253883\n","134: 0.2504570111632347 0.16292248852550983 0.41337949968874454\n","135: 0.3200007230043411 0.19368158653378487 0.513682309538126\n","136: 0.16291295737028122 0.16806123778223991 0.33097419515252113\n","137: 0.2427326887845993 0.1793408878147602 0.4220735765993595\n","138: 0.21130748465657234 0.16361393220722675 0.3749214168637991\n","139: 0.1791061908006668 0.16102528758347034 0.34013147838413715\n","140: 0.26553256064653397 0.17852836847305298 0.44406092911958694\n","141: 0.2334398254752159 0.16580916941165924 0.39924899488687515\n","142: 0.24203507229685783 0.18010812625288963 0.42214319854974747\n","143: 0.19434605911374092 0.15955227427184582 0.35389833338558674\n","144: 0.2418916169553995 0.18853671103715897 0.4304283279925585\n","145: 0.16849682293832302 0.15708769112825394 0.32558451406657696\n","146: 0.21645117551088333 0.17700021341443062 0.39345138892531395\n","147: 0.16447873786091805 0.17742857336997986 0.3419073112308979\n","148: 0.1472577266395092 0.170564204454422 0.3178219310939312\n","149: 0.21683110855519772 0.16521688736975193 0.38204799592494965\n","150: 0.21353578194975853 0.15736441127955914 0.37090019322931767\n","151: 0.180223248898983 0.16169294342398643 0.34191619232296944\n","152: 0.3455513119697571 0.18094168603420258 0.5264929980039597\n","153: 0.20472177118062973 0.17850815504789352 0.38322992622852325\n","154: 0.21047630906105042 0.16127295047044754 0.37174925953149796\n","155: 0.3318381756544113 0.19030947797000408 0.5221476536244154\n","156: 0.21877216175198555 0.1761149950325489 0.39488715678453445\n","157: 0.20530464127659798 0.16550397500395775 0.3708086162805557\n","158: 0.13858231902122498 0.15505048632621765 0.2936328053474426\n","159: 0.16028262302279472 0.15964490920305252 0.31992753222584724\n","160: 0.18950388580560684 0.15689625963568687 0.3464001454412937\n","161: 0.19364162907004356 0.15720078721642494 0.3508424162864685\n","162: 0.2753859758377075 0.16313005983829498 0.4385160356760025\n","163: 0.20253601484000683 0.19212722405791283 0.39466323889791965\n","164: 0.1721845678985119 0.16012491285800934 0.3323094807565212\n","165: 0.2017066553235054 0.16164058074355125 0.36334723606705666\n","166: 0.2381078079342842 0.18538711220026016 0.4234949201345444\n","167: 0.18746530264616013 0.17650891840457916 0.3639742210507393\n","168: 0.2726995348930359 0.1769257578998804 0.4496252927929163\n","169: 0.19292860478162766 0.15410351380705833 0.347032118588686\n","170: 0.17723620310425758 0.15438324213027954 0.3316194452345371\n","171: 0.24400830641388893 0.15738762356340885 0.4013959299772978\n","172: 0.17423397861421108 0.1744451094418764 0.3486790880560875\n","173: 0.24520589783787727 0.15142153296619654 0.3966274308040738\n","174: 0.17455413937568665 0.16128096729516983 0.3358351066708565\n","175: 0.22055169939994812 0.1603187434375286 0.38087044283747673\n","176: 0.23091309517621994 0.15369592420756817 0.3846090193837881\n","177: 0.21565872430801392 0.17457341775298119 0.3902321420609951\n","178: 0.17481671646237373 0.15927881374955177 0.3340955302119255\n","179: 0.23732352443039417 0.16636347398161888 0.40368699841201305\n","180: 0.14373639971017838 0.15945108607411385 0.3031874857842922\n","181: 0.21423856914043427 0.16414899751544 0.37838756665587425\n","182: 0.2591431513428688 0.17276442050933838 0.4319075718522072\n","183: 0.21732063591480255 0.17766222544014454 0.3949828613549471\n","184: 0.18154099583625793 0.16452468186616898 0.3460656777024269\n","185: 0.2013534978032112 0.16302217915654182 0.36437567695975304\n","186: 0.19354713335633278 0.17928588017821312 0.3728330135345459\n","187: 0.18524364940822124 0.16941318660974503 0.35465683601796627\n","188: 0.14682351797819138 0.15940316393971443 0.3062266819179058\n","189: 0.21845971792936325 0.17542442493140697 0.3938841428607702\n","190: 0.25357476249337196 0.16351452097296715 0.4170892834663391\n","191: 0.21849530935287476 0.15947653725743294 0.3779718466103077\n","192: 0.25127315521240234 0.1560588777065277 0.40733203291893005\n","193: 0.2651136443018913 0.1755826324224472 0.44069627672433853\n","194: 0.19817690178751945 0.15717792510986328 0.35535482689738274\n","195: 0.2625843361020088 0.16089818999171257 0.4234825260937214\n","196: 0.1912164893001318 0.15458886325359344 0.34580535255372524\n","197: 0.1733378991484642 0.16188719868659973 0.33522509783506393\n","198: 0.16149640455842018 0.15602312050759792 0.3175195250660181\n","199: 0.21687629632651806 0.1756199300289154 0.39249622635543346\n","200: 0.2497541941702366 0.18676338344812393 0.4365175776183605\n","201: 0.22451133280992508 0.15576336905360222 0.3802747018635273\n","202: 0.28570688515901566 0.1640400066971779 0.44974689185619354\n","203: 0.24687085300683975 0.16788771376013756 0.4147585667669773\n","204: 0.17770098894834518 0.17077790200710297 0.34847889095544815\n","205: 0.2175895944237709 0.1535913571715355 0.3711809515953064\n","206: 0.19380513206124306 0.15686742961406708 0.35067256167531013\n","207: 0.23488719761371613 0.1700846590101719 0.404971856623888\n","208: 0.2214856930077076 0.15533184818923473 0.37681754119694233\n","209: 0.2541554346680641 0.16805516183376312 0.42221059650182724\n","210: 0.24236926808953285 0.16120323538780212 0.403572503477335\n","211: 0.20285577699542046 0.15847727470099926 0.3613330516964197\n","212: 0.20459537580609322 0.16350248456001282 0.36809786036610603\n","213: 0.0965463612228632 0.17508427612483501 0.2716306373476982\n","214: 0.17286739498376846 0.16987191140651703 0.3427393063902855\n","215: 0.16534586623311043 0.15613924898207188 0.3214851152151823\n","216: 0.20096547156572342 0.1535423006862402 0.3545077722519636\n","217: 0.16762611642479897 0.1649525761604309 0.3325786925852299\n","218: 0.20192730985581875 0.1542741023004055 0.35620141215622425\n","219: 0.17088604718446732 0.15467922948300838 0.3255652766674757\n","220: 0.2331364005804062 0.17087572813034058 0.40401212871074677\n","221: 0.282862339168787 0.1739509366452694 0.4568132758140564\n","222: 0.2084527462720871 0.18713470548391342 0.3955874517560005\n","223: 0.20416009426116943 0.1611659862101078 0.36532608047127724\n","224: 0.19898005202412605 0.158338887616992 0.35731893964111805\n","225: 0.14161017537117004 0.17164620384573936 0.3132563792169094\n","226: 0.20058883726596832 0.17492098733782768 0.375509824603796\n","227: 0.2555687874555588 0.16954609006643295 0.42511487752199173\n","228: 0.22736863046884537 0.1728345789015293 0.4002032093703747\n","229: 0.17051652818918228 0.15574314072728157 0.32625966891646385\n","230: 0.26240667700767517 0.158637385815382 0.4210440628230572\n","231: 0.22560559585690498 0.15489571169018745 0.38050130754709244\n","232: 0.2057410553097725 0.1672016829252243 0.3729427382349968\n","233: 0.24507371708750725 0.1721215359866619 0.41719525307416916\n","234: 0.19008085876703262 0.16725967079401016 0.3573405295610428\n","235: 0.2082277610898018 0.17123530432581902 0.3794630654156208\n","236: 0.2408849522471428 0.17151066288352013 0.4123956151306629\n","237: 0.2435159757733345 0.15331009030342102 0.3968260660767555\n","238: 0.28364113718271255 0.18475420959293842 0.468395346775651\n","239: 0.20897748693823814 0.18107013031840324 0.3900476172566414\n","240: 0.2301589958369732 0.16810628026723862 0.3982652761042118\n","241: 0.18196482211351395 0.1838020756840706 0.36576689779758453\n","242: 0.2220226414501667 0.15996020659804344 0.38198284804821014\n","243: 0.20767470262944698 0.17599447444081306 0.38366917707026005\n","244: 0.18101076781749725 0.15513991750776768 0.33615068532526493\n","245: 0.18634912371635437 0.1582917533814907 0.3446408770978451\n","246: 0.17303909361362457 0.15744534507393837 0.33048443868756294\n","247: 0.2979320362210274 0.16119315847754478 0.45912519469857216\n","248: 0.20381959155201912 0.15414323285222054 0.35796282440423965\n","249: 0.18421060219407082 0.15271269716322422 0.33692329935729504\n","250: 0.165468268096447 0.15438860282301903 0.319856870919466\n","251: 0.2577243223786354 0.1522078439593315 0.4099321663379669\n","252: 0.18214187026023865 0.1723121590912342 0.35445402935147285\n","253: 0.1861002929508686 0.15557553991675377 0.3416758328676224\n","254: 0.24456291273236275 0.16780133917927742 0.41236425191164017\n","255: 0.23325525969266891 0.16511953994631767 0.3983747996389866\n","256: 0.287054680287838 0.15617897734045982 0.4432336576282978\n","257: 0.19493808969855309 0.15733777359127998 0.35227586328983307\n","258: 0.20059721171855927 0.17599419131875038 0.37659140303730965\n","259: 0.19128011912107468 0.16193758323788643 0.3532177023589611\n","260: 0.21095994859933853 0.17367273196578026 0.3846326805651188\n","261: 0.22878780961036682 0.16902203112840652 0.39780984073877335\n","262: 0.17538642324507236 0.15125685185194016 0.3266432750970125\n","263: 0.18165428191423416 0.15565377846360207 0.3373080603778362\n","264: 0.20265087112784386 0.17588113248348236 0.3785320036113262\n","265: 0.18449990451335907 0.1526021584868431 0.3371020630002022\n","266: 0.2426692731678486 0.15305829606950283 0.3957275692373514\n","267: 0.14278458431363106 0.17136339843273163 0.3141479827463627\n","268: 0.19900093972682953 0.15029923245310783 0.34930017217993736\n","269: 0.2231838945299387 0.17221971228718758 0.3954036068171263\n","270: 0.19779501855373383 0.17395373806357384 0.37174875661730766\n","271: 0.23837655037641525 0.16916117072105408 0.40753772109746933\n","272: 0.21795422211289406 0.1535243671387434 0.37147858925163746\n","273: 0.2103717140853405 0.17405743524432182 0.3844291493296623\n","274: 0.2446421943604946 0.1762549877166748 0.4208971820771694\n","275: 0.2104688435792923 0.15550794824957848 0.3659767918288708\n","276: 0.2480047568678856 0.15876279771327972 0.4067675545811653\n","277: 0.27432770282030106 0.16132285445928574 0.4356505572795868\n","278: 0.19558477401733398 0.17118606716394424 0.36677084118127823\n","279: 0.18307636678218842 0.1495391298085451 0.33261549659073353\n","280: 0.17626363784074783 0.1676902361214161 0.3439538739621639\n","281: 0.19205277785658836 0.15248809568583965 0.344540873542428\n","282: 0.23526888713240623 0.15198611840605736 0.3872550055384636\n","283: 0.23357556015253067 0.1544797420501709 0.38805530220270157\n","284: 0.2600470818579197 0.15123519487679005 0.41128227673470974\n","285: 0.2415718212723732 0.18493952229619026 0.42651134356856346\n","286: 0.17681342363357544 0.16037564910948277 0.3371890727430582\n","287: 0.20596015453338623 0.16755566373467445 0.3735158182680607\n","288: 0.14293381944298744 0.15084227547049522 0.29377609491348267\n","289: 0.1538148783147335 0.15883182734251022 0.31264670565724373\n","290: 0.2503099627792835 0.16214163601398468 0.4124515987932682\n","291: 0.15774033591151237 0.15449965000152588 0.31223998591303825\n","292: 0.20608822256326675 0.16034069657325745 0.3664289191365242\n","293: 0.19582964293658733 0.17021949402987957 0.3660491369664669\n","294: 0.23593255877494812 0.15922316908836365 0.39515572786331177\n","295: 0.19587790966033936 0.15858985856175423 0.3544677682220936\n","296: 0.16940613463521004 0.15250222571194172 0.32190836034715176\n","297: 0.19875796139240265 0.1519805733114481 0.35073853470385075\n","298: 0.19938315078616142 0.1657218374311924 0.3651049882173538\n","299: 0.20316732861101627 0.1553226336836815 0.35848996229469776\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625275245769,"user_tz":-60,"elapsed":19,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625275245769,"user_tz":-60,"elapsed":19,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"ad2bf779-e2c2-4a60-c44c-1cf8aa45987a"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9736842105263158\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625275245770,"user_tz":-60,"elapsed":14,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"54f269ac-3a6e-46b2-87d3-2095fab6aff0"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9912023460410557\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625275245770,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":12,"outputs":[]}]}
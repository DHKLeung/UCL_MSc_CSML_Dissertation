{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN nb0.75 aug WPL0.9.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625274849639,"user_tz":-60,"elapsed":17147,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"7cf10829-60b4-4c3d-dbb3-8b96e1066b73"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625274850514,"user_tz":-60,"elapsed":877,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625274850870,"user_tz":-60,"elapsed":357,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625274850870,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"0af668f4-d39a-4c68-8934-170870cf267d"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","geometric_param = 0.75\n","perturb_loss_weight = 0.9\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f40f9e84a90>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625274851057,"user_tz":-60,"elapsed":188,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625274857222,"user_tz":-60,"elapsed":6167,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"5e6e301a-b658-43b8-d30b-57af601c85c6"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1625274857222,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"id":"quiet-module","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1625274857223,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625274860609,"user_tz":-60,"elapsed":3389,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"38cb6962-715d-45a0-fe2e-91eea037205e"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup with random neighbour perturbation #\n","        mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_breast_nb(inputs, labels, geometric_param, alpha)\n","        \n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_nb))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_nb = augment_outputs[original_num:]\n","        mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_nb + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_nb.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_nb.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print decomposed losses #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:151: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n","  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.1400445699691772 2.136000156402588 4.276044726371765\n","1: 2.100864589214325 2.1000856757164 4.200950264930725\n","2: 2.0407207012176514 2.0365394353866577 4.077260136604309\n","3: 1.9599626660346985 1.9579334259033203 3.917896091938019\n","4: 1.8735751509666443 1.8733757734298706 3.746950924396515\n","5: 1.7724361419677734 1.7771987915039062 3.5496349334716797\n","6: 1.6709890961647034 1.6720277070999146 3.343016803264618\n","7: 1.5564654469490051 1.5698036551475525 3.1262691020965576\n","8: 1.4605135023593903 1.4681985080242157 2.928712010383606\n","9: 1.3227586150169373 1.3444736301898956 2.667232245206833\n","10: 1.1793264150619507 1.224165439605713 2.4034918546676636\n","11: 1.1116993427276611 1.1207218766212463 2.2324212193489075\n","12: 0.9642056822776794 0.9974894225597382 1.9616951048374176\n","13: 0.8568389117717743 0.9039290249347687 1.760767936706543\n","14: 0.7797342985868454 0.8032337725162506 1.582968071103096\n","15: 0.6811381131410599 0.711848109960556 1.392986223101616\n","16: 0.6253068000078201 0.6692524999380112 1.2945592999458313\n","17: 0.5839616507291794 0.609993115067482 1.1939547657966614\n","18: 0.5439390242099762 0.5532148480415344 1.0971538722515106\n","19: 0.4871126413345337 0.5117351561784744 0.9988477975130081\n","20: 0.49370913207530975 0.4801837056875229 0.9738928377628326\n","21: 0.4481152668595314 0.45127449929714203 0.8993897661566734\n","22: 0.4037904441356659 0.43481191992759705 0.8386023640632629\n","23: 0.4078626483678818 0.41068942844867706 0.8185520768165588\n","24: 0.35251201689243317 0.3881011977791786 0.7406132146716118\n","25: 0.3811800479888916 0.3641471862792969 0.7453272342681885\n","26: 0.34824028611183167 0.3528200834989548 0.7010603696107864\n","27: 0.2864859700202942 0.3417232260107994 0.6282091960310936\n","28: 0.3336772173643112 0.3294346481561661 0.6631118655204773\n","29: 0.33606956899166107 0.3190201297402382 0.6550896987318993\n","30: 0.33733903989195824 0.326229527592659 0.6635685674846172\n","31: 0.4015572518110275 0.3194970414042473 0.7210542932152748\n","32: 0.273984856903553 0.306830957531929 0.580815814435482\n","33: 0.3000654950737953 0.3035413920879364 0.6036068871617317\n","34: 0.2995835170149803 0.27949972450733185 0.5790832415223122\n","35: 0.28320538252592087 0.2917183190584183 0.5749237015843391\n","36: 0.3004935309290886 0.27200910076498985 0.5725026316940784\n","37: 0.23514216765761375 0.2751181423664093 0.5102603100240231\n","38: 0.32591626048088074 0.2676072418689728 0.5935235023498535\n","39: 0.2764192409813404 0.25466226041316986 0.5310815013945103\n","40: 0.28575748950242996 0.2523270845413208 0.5380845740437508\n","41: 0.2592066377401352 0.2483728900551796 0.5075795277953148\n","42: 0.2535405308008194 0.24615000188350677 0.49969053268432617\n","43: 0.2549435719847679 0.2494407743215561 0.504384346306324\n","44: 0.28569262474775314 0.24519362673163414 0.5308862514793873\n","45: 0.23843233287334442 0.254343967884779 0.4927763007581234\n","46: 0.2601238526403904 0.26346292346715927 0.5235867761075497\n","47: 0.24541211873292923 0.22806278988718987 0.4734749086201191\n","48: 0.2129680346697569 0.22878668457269669 0.4417547192424536\n","49: 0.2503703236579895 0.21961261704564095 0.46998294070363045\n","50: 0.21381904557347298 0.21223623119294643 0.4260552767664194\n","51: 0.26905472576618195 0.2272942140698433 0.49634893983602524\n","52: 0.2642352022230625 0.22124845534563065 0.48548365756869316\n","53: 0.27570590004324913 0.24627083912491798 0.5219767391681671\n","54: 0.21449726074934006 0.23053961247205734 0.4450368732213974\n","55: 0.2317412793636322 0.22537629306316376 0.45711757242679596\n","56: 0.23418647795915604 0.22264118120074272 0.45682765915989876\n","57: 0.2972518652677536 0.21151116117835045 0.508763026446104\n","58: 0.17913886904716492 0.20727365836501122 0.38641252741217613\n","59: 0.2894986793398857 0.21858308464288712 0.5080817639827728\n","60: 0.2233976572751999 0.22014757990837097 0.44354523718357086\n","61: 0.24680247902870178 0.2142024114727974 0.4610048905014992\n","62: 0.2478141412138939 0.21411999315023422 0.4619341343641281\n","63: 0.2804815173149109 0.2249691914767027 0.5054507087916136\n","64: 0.22503752261400223 0.20852228999137878 0.433559812605381\n","65: 0.25527361780405045 0.20771057158708572 0.46298418939113617\n","66: 0.21417072415351868 0.1936721671372652 0.4078428912907839\n","67: 0.25239257887005806 0.19575367495417595 0.448146253824234\n","68: 0.23463577032089233 0.22669441998004913 0.46133019030094147\n","69: 0.2469412088394165 0.20232557877898216 0.44926678761839867\n","70: 0.21931055188179016 0.2124396674335003 0.43175021931529045\n","71: 0.28029947727918625 0.21009551733732224 0.4903949946165085\n","72: 0.1885957419872284 0.1937132067978382 0.3823089487850666\n","73: 0.22225967049598694 0.20259810239076614 0.4248577728867531\n","74: 0.25321992114186287 0.20435213670134544 0.4575720578432083\n","75: 0.1500153336673975 0.20293206349015236 0.35294739715754986\n","76: 0.27317608892917633 0.21890198066830635 0.4920780695974827\n","77: 0.22745835036039352 0.19921200722455978 0.4266703575849533\n","78: 0.18179405480623245 0.19457008689641953 0.376364141702652\n","79: 0.21319308876991272 0.2056996375322342 0.4188927263021469\n","80: 0.24412714689970016 0.21480263769626617 0.45892978459596634\n","81: 0.23870700597763062 0.18729012086987495 0.42599712684750557\n","82: 0.2419741302728653 0.195998165756464 0.4379722960293293\n","83: 0.26610203087329865 0.20567183010280132 0.47177386097609997\n","84: 0.1892690770328045 0.20619072392582893 0.3954598009586334\n","85: 0.2395591214299202 0.1991421841084957 0.4387013055384159\n","86: 0.16828645765781403 0.189584419131279 0.357870876789093\n","87: 0.23853321373462677 0.18193050101399422 0.420463714748621\n","88: 0.19291068986058235 0.1817966103553772 0.37470730021595955\n","89: 0.29840271919965744 0.17901589907705784 0.4774186182767153\n","90: 0.23257214576005936 0.1891850344836712 0.42175718024373055\n","91: 0.27650848776102066 0.20197322592139244 0.4784817136824131\n","92: 0.2578202784061432 0.18915971368551254 0.44697999209165573\n","93: 0.23415892105549574 0.2097795121371746 0.44393843319267035\n","94: 0.18787146359682083 0.17727545648813248 0.3651469200849533\n","95: 0.2942061126232147 0.19023924320936203 0.48444535583257675\n","96: 0.2496061734855175 0.1709504248574376 0.4205565983429551\n","97: 0.18760132789611816 0.17347808741033077 0.36107941530644894\n","98: 0.2176230363547802 0.17764894664287567 0.39527198299765587\n","99: 0.15695841051638126 0.17515236884355545 0.3321107793599367\n","100: 0.2522561550140381 0.20103876665234566 0.45329492166638374\n","101: 0.2021123170852661 0.17395344004034996 0.3760657571256161\n","102: 0.24389346688985825 0.17197694070637226 0.4158704075962305\n","103: 0.19176982529461384 0.20841578394174576 0.4001856092363596\n","104: 0.17444384098052979 0.17870812863111496 0.35315196961164474\n","105: 0.26233916729688644 0.20743419602513313 0.4697733633220196\n","106: 0.17413849383592606 0.1732710748910904 0.34740956872701645\n","107: 0.2341880165040493 0.18371868878602982 0.4179067052900791\n","108: 0.23967638239264488 0.18058415688574314 0.420260539278388\n","109: 0.2607136443257332 0.17545853927731514 0.4361721836030483\n","110: 0.15394592471420765 0.19176321290433407 0.3457091376185417\n","111: 0.22984904423356056 0.17398354411125183 0.4038325883448124\n","112: 0.33670908212661743 0.19296493381261826 0.5296740159392357\n","113: 0.19771384820342064 0.17716144770383835 0.374875295907259\n","114: 0.20399941876530647 0.1711924560368061 0.3751918748021126\n","115: 0.27522090822458267 0.18453484028577805 0.4597557485103607\n","116: 0.24556662142276764 0.190387524664402 0.43595414608716965\n","117: 0.20111767202615738 0.1748567298054695 0.3759744018316269\n","118: 0.31901971623301506 0.17730491235852242 0.4963246285915375\n","119: 0.19746603816747665 0.16413791105151176 0.3616039492189884\n","120: 0.17816902697086334 0.16837134584784508 0.3465403728187084\n","121: 0.14638296142220497 0.17898585461080074 0.3253688160330057\n","122: 0.1581769436597824 0.17912568524479866 0.33730262890458107\n","123: 0.18695291131734848 0.18614794313907623 0.3731008544564247\n","124: 0.17386918514966965 0.16830021888017654 0.3421694040298462\n","125: 0.2112581618130207 0.16666419804096222 0.3779223598539829\n","126: 0.18419555947184563 0.18793317489326 0.37212873436510563\n","127: 0.13516507297754288 0.16988462023437023 0.3050496932119131\n","128: 0.25467030331492424 0.18137390352785587 0.4360442068427801\n","129: 0.2117030918598175 0.16242605447769165 0.37412914633750916\n","130: 0.16231055930256844 0.19568503089249134 0.3579955901950598\n","131: 0.2147518191486597 0.1629546508193016 0.3777064699679613\n","132: 0.21258359029889107 0.17186349257826805 0.3844470828771591\n","133: 0.20169831812381744 0.18158178962767124 0.3832801077514887\n","134: 0.22351401671767235 0.1621372103691101 0.38565122708678246\n","135: 0.20596269331872463 0.17381247505545616 0.3797751683741808\n","136: 0.17492306232452393 0.1624059770256281 0.337329039350152\n","137: 0.24681981652975082 0.16151126101613045 0.40833107754588127\n","138: 0.14244842156767845 0.15893255546689034 0.3013809770345688\n","139: 0.19221707805991173 0.17078348994255066 0.3630005680024624\n","140: 0.2591501735150814 0.18100763112306595 0.44015780463814735\n","141: 0.18656381964683533 0.17397956363856792 0.36054338328540325\n","142: 0.16841194778680801 0.17455664090812206 0.3429685886949301\n","143: 0.16837523877620697 0.16093522310256958 0.32931046187877655\n","144: 0.2114841565489769 0.16119248047471046 0.37267663702368736\n","145: 0.20934802666306496 0.15811101906001568 0.36745904572308064\n","146: 0.2625393830239773 0.17429838702082634 0.4368377700448036\n","147: 0.2087319828569889 0.17926857620477676 0.38800055906176567\n","148: 0.17982432432472706 0.15582457184791565 0.3356488961726427\n","149: 0.20605402067303658 0.16087258979678154 0.3669266104698181\n","150: 0.19983744621276855 0.15971039980649948 0.35954784601926804\n","151: 0.2281493805348873 0.19874613173305988 0.4268955122679472\n","152: 0.26917488873004913 0.16163764148950577 0.4308125302195549\n","153: 0.18294067680835724 0.16139515675604343 0.3443358335644007\n","154: 0.17512425035238266 0.17724211513996124 0.3523663654923439\n","155: 0.18909218162298203 0.15902161970734596 0.348113801330328\n","156: 0.2858545184135437 0.16992351412773132 0.455778032541275\n","157: 0.20985104888677597 0.1566555742174387 0.36650662310421467\n","158: 0.2134307250380516 0.16354191675782204 0.37697264179587364\n","159: 0.2274195346981287 0.17725196108222008 0.4046714957803488\n","160: 0.179531779140234 0.15966574288904667 0.33919752202928066\n","161: 0.19754526764154434 0.17698627896606922 0.37453154660761356\n","162: 0.2675168961286545 0.19183109886944294 0.4593479949980974\n","163: 0.2730600945651531 0.16906793788075447 0.4421280324459076\n","164: 0.2348099909722805 0.1556318011134863 0.3904417920857668\n","165: 0.25315244495868683 0.1540325190871954 0.4071849640458822\n","166: 0.21032456308603287 0.16701611876487732 0.3773406818509102\n","167: 0.13271274790167809 0.15409037843346596 0.28680312633514404\n","168: 0.20358513295650482 0.15445666946470737 0.3580418024212122\n","169: 0.2354874052107334 0.16159942746162415 0.39708683267235756\n","170: 0.13294721953570843 0.15714233741164207 0.2900895569473505\n","171: 0.2449261173605919 0.16513000428676605 0.41005612164735794\n","172: 0.22256990522146225 0.17406049370765686 0.3966303989291191\n","173: 0.23359764739871025 0.17540425434708595 0.4090019017457962\n","174: 0.2345837652683258 0.17791962251067162 0.4125033877789974\n","175: 0.24959341436624527 0.1571684405207634 0.40676185488700867\n","176: 0.15761505253612995 0.16492938622832298 0.32254443876445293\n","177: 0.17964963614940643 0.1552823092788458 0.3349319454282522\n","178: 0.1747971698641777 0.15912865847349167 0.3339258283376694\n","179: 0.2691669315099716 0.19505183398723602 0.46421876549720764\n","180: 0.20524819567799568 0.15579507872462273 0.3610432744026184\n","181: 0.2491084262728691 0.17648890241980553 0.42559732869267464\n","182: 0.1905484814196825 0.17874708399176598 0.3692955654114485\n","183: 0.16246289759874344 0.1572359185665846 0.319698816165328\n","184: 0.1746025774627924 0.1529679549857974 0.3275705324485898\n","185: 0.23525961488485336 0.15725267492234707 0.39251228980720043\n","186: 0.2550230361521244 0.18964671902358532 0.4446697551757097\n","187: 0.21665778011083603 0.1561321448534727 0.37278992496430874\n","188: 0.08320516720414162 0.1934622824192047 0.27666744962334633\n","189: 0.26402671076357365 0.188162699341774 0.45218941010534763\n","190: 0.19695442914962769 0.1846117489039898 0.3815661780536175\n","191: 0.2234996110200882 0.16542498022317886 0.38892459124326706\n","192: 0.22512424737215042 0.15754089877009392 0.38266514614224434\n","193: 0.2849677801132202 0.1579423025250435 0.4429100826382637\n","194: 0.22642609477043152 0.15634719282388687 0.3827732875943184\n","195: 0.20805437862873077 0.1659155897796154 0.3739699684083462\n","196: 0.2838372476398945 0.1897585541009903 0.4735958017408848\n","197: 0.2756301164627075 0.175731448456645 0.45136156491935253\n","198: 0.20130698010325432 0.1556087825447321 0.3569157626479864\n","199: 0.17597228847444057 0.17460575699806213 0.3505780454725027\n","200: 0.20620928891003132 0.167605709284544 0.3738149981945753\n","201: 0.2180328406393528 0.16951486468315125 0.38754770532250404\n","202: 0.18834183737635612 0.155824176967144 0.34416601434350014\n","203: 0.19454025104641914 0.1684715524315834 0.36301180347800255\n","204: 0.17666234448552132 0.15429901331663132 0.33096135780215263\n","205: 0.20620325207710266 0.16724327579140663 0.3734465278685093\n","206: 0.24360105767846107 0.15511656738817692 0.398717625066638\n","207: 0.21959275007247925 0.17763032391667366 0.3972230739891529\n","208: 0.1911310777068138 0.17524514347314835 0.36637622117996216\n","209: 0.21190614253282547 0.16866495460271835 0.3805710971355438\n","210: 0.2293114922940731 0.19357353448867798 0.4228850267827511\n","211: 0.1829005889594555 0.1514550158753991 0.3343556048348546\n","212: 0.18137291073799133 0.15755725465714931 0.33893016539514065\n","213: 0.20428365096449852 0.19983542896807194 0.40411907993257046\n","214: 0.16401000320911407 0.15555448085069656 0.31956448405981064\n","215: 0.14924971014261246 0.16899973899126053 0.318249449133873\n","216: 0.23724552243947983 0.1571326795965433 0.39437820203602314\n","217: 0.22886288166046143 0.1863801572471857 0.41524303890764713\n","218: 0.18900560960173607 0.1568893101066351 0.34589491970837116\n","219: 0.169400442391634 0.15543387457728386 0.32483431696891785\n","220: 0.2091096118092537 0.1543371006846428 0.3634467124938965\n","221: 0.20887117832899094 0.1611602120101452 0.3700313903391361\n","222: 0.12383674085140228 0.16882434114813805 0.29266108199954033\n","223: 0.21130720525979996 0.15406480245292187 0.3653720077127218\n","224: 0.2501986362040043 0.16384467855095863 0.4140433147549629\n","225: 0.18056245148181915 0.16082148998975754 0.3413839414715767\n","226: 0.24621766433119774 0.16189140640199184 0.4081090707331896\n","227: 0.19257419183850288 0.17044762521982193 0.3630218170583248\n","228: 0.21751680597662926 0.18641505017876625 0.4039318561553955\n","229: 0.1838379167020321 0.15208607353270054 0.33592399023473263\n","230: 0.23533325642347336 0.15677059069275856 0.3921038471162319\n","231: 0.2094245683401823 0.15268542245030403 0.36210999079048634\n","232: 0.17334293760359287 0.16436929255723953 0.3377122301608324\n","233: 0.2047322802245617 0.15402174554765224 0.35875402577221394\n","234: 0.19419175758957863 0.15260294638574123 0.34679470397531986\n","235: 0.22677374258637428 0.15209014061838388 0.37886388320475817\n","236: 0.17332933098077774 0.16161287389695644 0.3349422048777342\n","237: 0.2383882962167263 0.1893893089145422 0.4277776051312685\n","238: 0.18185080215334892 0.17558220401406288 0.3574330061674118\n","239: 0.1380399987101555 0.15653464198112488 0.29457464069128036\n","240: 0.2442212849855423 0.15246576629579067 0.39668705128133297\n","241: 0.2629721499979496 0.18296571634709835 0.44593786634504795\n","242: 0.1693768985569477 0.16152314469218254 0.33090004324913025\n","243: 0.18269256502389908 0.15544765070080757 0.33814021572470665\n","244: 0.18214230798184872 0.15301907621324062 0.33516138419508934\n","245: 0.2096151039004326 0.15411595813930035 0.36373106203973293\n","246: 0.1821267530322075 0.16748245805501938 0.34960921108722687\n","247: 0.25216213800013065 0.16088511794805527 0.4130472559481859\n","248: 0.27759331464767456 0.15550925955176353 0.4331025741994381\n","249: 0.1678706258535385 0.16033819690346718 0.3282088227570057\n","250: 0.18796230852603912 0.17853952571749687 0.366501834243536\n","251: 0.15650109946727753 0.15765366330742836 0.3141547627747059\n","252: 0.23828551918268204 0.17643873021006584 0.4147242493927479\n","253: 0.19011179357767105 0.16190430521965027 0.3520160987973213\n","254: 0.2511577010154724 0.16366525553166866 0.4148229565471411\n","255: 0.2165358066558838 0.15597219578921795 0.37250800244510174\n","256: 0.22483240067958832 0.16081577911973 0.3856481797993183\n","257: 0.22395294532179832 0.16075215861201286 0.3847051039338112\n","258: 0.25835761800408363 0.18236095272004604 0.4407185707241297\n","259: 0.21107026934623718 0.17269523441791534 0.3837655037641525\n","260: 0.19528213888406754 0.16142379119992256 0.3567059300839901\n","261: 0.1930181384086609 0.16479336842894554 0.35781150683760643\n","262: 0.1900092400610447 0.1527472324669361 0.3427564725279808\n","263: 0.24078498780727386 0.16688532195985317 0.40767030976712704\n","264: 0.20222347043454647 0.15206693299114704 0.3542904034256935\n","265: 0.2060728296637535 0.16798347979784012 0.37405630946159363\n","266: 0.19373441487550735 0.15209538117051125 0.3458297960460186\n","267: 0.1405055783689022 0.1554695051163435 0.2959750834852457\n","268: 0.20083188638091087 0.159829780459404 0.36066166684031487\n","269: 0.19871638715267181 0.15471334010362625 0.35342972725629807\n","270: 0.20370832085609436 0.15441909059882164 0.358127411454916\n","271: 0.2182365208864212 0.15329002402722836 0.37152654491364956\n","272: 0.18939671106636524 0.15392004512250423 0.3433167561888695\n","273: 0.211063202470541 0.1612190045416355 0.3722822070121765\n","274: 0.27609696984291077 0.1855810470879078 0.46167801693081856\n","275: 0.23137613385915756 0.17521365359425545 0.406589787453413\n","276: 0.17722900211811066 0.15837359800934792 0.3356026001274586\n","277: 0.1230009738355875 0.18566326797008514 0.30866424180567265\n","278: 0.21027522161602974 0.1542159467935562 0.36449116840958595\n","279: 0.2749932110309601 0.15225128084421158 0.42724449187517166\n","280: 0.17112223245203495 0.15125914849340916 0.3223813809454441\n","281: 0.17590433731675148 0.18273832276463509 0.35864266008138657\n","282: 0.26413436233997345 0.17646921798586845 0.4406035803258419\n","283: 0.22663220390677452 0.15218503773212433 0.37881724163889885\n","284: 0.13145185261964798 0.15463138557970524 0.2860832381993532\n","285: 0.2418033927679062 0.1733989305794239 0.4152023233473301\n","286: 0.2305113635957241 0.16696207225322723 0.39747343584895134\n","287: 0.10615593567490578 0.15241630375385284 0.2585722394287586\n","288: 0.2552750576287508 0.17211951687932014 0.42739457450807095\n","289: 0.1790367253124714 0.1769557110965252 0.3559924364089966\n","290: 0.2472003623843193 0.16749490797519684 0.41469527035951614\n","291: 0.19773881137371063 0.1563751995563507 0.35411401093006134\n","292: 0.28848737478256226 0.18129941821098328 0.46978679299354553\n","293: 0.12591700814664364 0.15557551383972168 0.2814925219863653\n","294: 0.27434515953063965 0.1633700169622898 0.43771517649292946\n","295: 0.20809601247310638 0.1730707250535488 0.3811667375266552\n","296: 0.16296252608299255 0.15815377980470657 0.3211163058876991\n","297: 0.2505302429199219 0.15358938090503216 0.40411962382495403\n","298: 0.20550300180912018 0.1574198566377163 0.36292285844683647\n","299: 0.2203737571835518 0.15263975597918034 0.3730135131627321\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625274860610,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625274860610,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"f7c0edff-83d9-4634-83d9-7250655e5c2d"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9736842105263158\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625274860610,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"7eed9105-541b-4bb8-8eb6-d5c47a8db7a4"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9912023460410557\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625274860611,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":12,"outputs":[]}]}
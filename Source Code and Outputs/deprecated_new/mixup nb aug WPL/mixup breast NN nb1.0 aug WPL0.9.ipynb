{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN nb1.0 aug WPL0.9.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625275110900,"user_tz":-60,"elapsed":44342,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"44c6f0f7-1854-4455-f7ed-30ae47bec68d"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625275112317,"user_tz":-60,"elapsed":1419,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625275112318,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625275112318,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"f5da1662-611a-4672-8492-702ba75d95f2"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","geometric_param = 1.\n","perturb_loss_weight = 0.9\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fd5f11968b0>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625275112621,"user_tz":-60,"elapsed":306,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625275118230,"user_tz":-60,"elapsed":5611,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"86bb5df3-78e6-45ea-8d48-d559d9137461"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1625275118230,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"id":"quiet-module","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1625275118231,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625275121892,"user_tz":-60,"elapsed":3665,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"1e774f13-018d-4143-b09e-8164e35481d5"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup with random neighbour perturbation #\n","        mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_breast_nb(inputs, labels, geometric_param, alpha)\n","        \n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_nb))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_nb = augment_outputs[original_num:]\n","        mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_nb + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_nb.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_nb.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print decomposed losses #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:151: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n","  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.1398622393608093 2.132339358329773 4.272201597690582\n","1: 2.1000126004219055 2.09616219997406 4.196174800395966\n","2: 2.050644814968109 2.0445547699928284 4.0951995849609375\n","3: 1.9754096269607544 1.9679291248321533 3.9433387517929077\n","4: 1.8862329125404358 1.881439447402954 3.76767235994339\n","5: 1.7956305742263794 1.7952868938446045 3.590917468070984\n","6: 1.696577548980713 1.6992105841636658 3.3957881331443787\n","7: 1.6021804213523865 1.6029685139656067 3.205148935317993\n","8: 1.492719143629074 1.4927934408187866 2.9855125844478607\n","9: 1.3851220607757568 1.3808859884738922 2.766008049249649\n","10: 1.2441740334033966 1.2634851932525635 2.50765922665596\n","11: 1.138142168521881 1.1506102085113525 2.2887523770332336\n","12: 1.0265742242336273 1.0535233318805695 2.0800975561141968\n","13: 0.9238226115703583 0.9569126069545746 1.8807352185249329\n","14: 0.8491981029510498 0.8541188836097717 1.7033169865608215\n","15: 0.7252748161554337 0.7687876224517822 1.4940624386072159\n","16: 0.687872439622879 0.6975587904453278 1.3854312300682068\n","17: 0.6348099857568741 0.648482158780098 1.283292144536972\n","18: 0.6038636267185211 0.6006492674350739 1.204512894153595\n","19: 0.5325738787651062 0.550376757979393 1.0829506367444992\n","20: 0.5069434493780136 0.5191841870546341 1.0261276364326477\n","21: 0.47199125587940216 0.478996679186821 0.9509879350662231\n","22: 0.45270972698926926 0.44174958765506744 0.8944593146443367\n","23: 0.41064825654029846 0.424183614552021 0.8348318710923195\n","24: 0.4069134220480919 0.39686277508735657 0.8037761971354485\n","25: 0.37268826365470886 0.38974468410015106 0.7624329477548599\n","26: 0.37203680723905563 0.3674132525920868 0.7394500598311424\n","27: 0.34469860047101974 0.3474786803126335 0.6921772807836533\n","28: 0.36793676763772964 0.33555375784635544 0.7034905254840851\n","29: 0.36313143372535706 0.3344946578145027 0.6976260915398598\n","30: 0.3412816673517227 0.3308257609605789 0.6721074283123016\n","31: 0.3239140659570694 0.3113352730870247 0.6352493390440941\n","32: 0.2755912020802498 0.3131288215517998 0.5887200236320496\n","33: 0.30966048687696457 0.30046915262937546 0.61012963950634\n","34: 0.33039356768131256 0.3016016408801079 0.6319952085614204\n","35: 0.2943761795759201 0.28346672654151917 0.5778429061174393\n","36: 0.27967946231365204 0.2672504372894764 0.5469298996031284\n","37: 0.2488035373389721 0.2610391899943352 0.5098427273333073\n","38: 0.3018444702029228 0.26966624706983566 0.5715107172727585\n","39: 0.2412790283560753 0.2550027221441269 0.4962817505002022\n","40: 0.261555977165699 0.2509100139141083 0.5124659910798073\n","41: 0.3146444335579872 0.2530280873179436 0.5676725208759308\n","42: 0.2210572026669979 0.24642866104841232 0.46748586371541023\n","43: 0.2873828634619713 0.2535681426525116 0.5409510061144829\n","44: 0.2758554443717003 0.2514113634824753 0.5272668078541756\n","45: 0.22636747360229492 0.2458856664597988 0.47225314006209373\n","46: 0.2662418335676193 0.24191083759069443 0.5081526711583138\n","47: 0.2572406306862831 0.2389303632080555 0.4961709938943386\n","48: 0.2599188834428787 0.22924192994832993 0.48916081339120865\n","49: 0.2861674353480339 0.22146938741207123 0.5076368227601051\n","50: 0.2676326483488083 0.21779131516814232 0.4854239635169506\n","51: 0.23560089617967606 0.21475186571478844 0.4503527618944645\n","52: 0.2721165642142296 0.22334842383861542 0.495464988052845\n","53: 0.2487928569316864 0.22427476942539215 0.47306762635707855\n","54: 0.24297072738409042 0.22397032380104065 0.4669410511851311\n","55: 0.24524960666894913 0.2152867093682289 0.46053631603717804\n","56: 0.25521645694971085 0.2118326723575592 0.46704912930727005\n","57: 0.293859638273716 0.20276832580566406 0.49662796407938004\n","58: 0.22399958968162537 0.2209540382027626 0.44495362788438797\n","59: 0.24612408131361008 0.21474063768982887 0.46086471900343895\n","60: 0.2477198988199234 0.21898816153407097 0.46670806035399437\n","61: 0.2586452588438988 0.2147194668650627 0.4733647257089615\n","62: 0.29276712238788605 0.21817547082901 0.5109425932168961\n","63: 0.29297182708978653 0.20521453768014908 0.4981863647699356\n","64: 0.2642028257250786 0.20890560373663902 0.4731084294617176\n","65: 0.26435375213623047 0.2127518430352211 0.47710559517145157\n","66: 0.2538556829094887 0.2099519819021225 0.4638076648116112\n","67: 0.22954025864601135 0.19310305267572403 0.4226433113217354\n","68: 0.20045553892850876 0.21297897398471832 0.4134345129132271\n","69: 0.2534308210015297 0.20588698238134384 0.45931780338287354\n","70: 0.24393365532159805 0.19719083607196808 0.44112449139356613\n","71: 0.3260662406682968 0.21816591545939445 0.5442321561276913\n","72: 0.28032058477401733 0.20031725615262985 0.4806378409266472\n","73: 0.23126086220145226 0.19133837148547173 0.422599233686924\n","74: 0.3202749341726303 0.1994929239153862 0.5197678580880165\n","75: 0.21502870321273804 0.19947249442338943 0.41450119763612747\n","76: 0.24536199122667313 0.1891862004995346 0.43454819172620773\n","77: 0.2663237303495407 0.19927985221147537 0.4656035825610161\n","78: 0.21009278297424316 0.20872526988387108 0.41881805285811424\n","79: 0.29438723623752594 0.18496513739228249 0.4793523736298084\n","80: 0.33615966886281967 0.19553811848163605 0.5316977873444557\n","81: 0.22395869717001915 0.1879294402897358 0.41188813745975494\n","82: 0.23847929388284683 0.19291812181472778 0.4313974156975746\n","83: 0.20609671622514725 0.1930469609797001 0.39914367720484734\n","84: 0.30140694975852966 0.20293919369578362 0.5043461434543133\n","85: 0.3121520131826401 0.18352094665169716 0.49567295983433723\n","86: 0.19577738270163536 0.18725541234016418 0.38303279504179955\n","87: 0.23824284225702286 0.18881582841277122 0.4270586706697941\n","88: 0.23063581436872482 0.17759665101766586 0.4082324653863907\n","89: 0.32017795741558075 0.18552741408348083 0.5057053714990616\n","90: 0.23554599657654762 0.18157819658517838 0.417124193161726\n","91: 0.20268598571419716 0.18675497174263 0.38944095745682716\n","92: 0.3032067269086838 0.18522807210683823 0.488434799015522\n","93: 0.2114068605005741 0.17876915261149406 0.3901760131120682\n","94: 0.2481168732047081 0.18956491351127625 0.43768178671598434\n","95: 0.22535844892263412 0.18231764808297157 0.4076760970056057\n","96: 0.24045616388320923 0.17941785231232643 0.41987401619553566\n","97: 0.2402784824371338 0.1787075474858284 0.4189860299229622\n","98: 0.2891887351870537 0.18580108508467674 0.4749898202717304\n","99: 0.2601129040122032 0.18943392485380173 0.44954682886600494\n","100: 0.2855295240879059 0.19509585201740265 0.48062537610530853\n","101: 0.2168976366519928 0.17824655026197433 0.39514418691396713\n","102: 0.24261845648288727 0.17877357453107834 0.4213920310139656\n","103: 0.27818410471081734 0.18544457480311394 0.4636286795139313\n","104: 0.21401379257440567 0.18410979211330414 0.3981235846877098\n","105: 0.2506836950778961 0.16893204674124718 0.4196157418191433\n","106: 0.2553822845220566 0.17876305431127548 0.43414533883333206\n","107: 0.27893223613500595 0.1747783124446869 0.45371054857969284\n","108: 0.24588827416300774 0.18155528604984283 0.42744356021285057\n","109: 0.17811130732297897 0.17067434266209602 0.348785649985075\n","110: 0.21338889747858047 0.18141354992985725 0.39480244740843773\n","111: 0.22791484743356705 0.16705962270498276 0.3949744701385498\n","112: 0.2871115505695343 0.1786972861737013 0.4658088367432356\n","113: 0.2460557445883751 0.17722157388925552 0.4232773184776306\n","114: 0.23016541823744774 0.1652970090508461 0.39546242728829384\n","115: 0.28344254195690155 0.17013800516724586 0.4535805471241474\n","116: 0.23511749505996704 0.18096132576465607 0.4160788208246231\n","117: 0.21664327383041382 0.1812998242676258 0.3979430980980396\n","118: 0.16787078604102135 0.16662995144724846 0.3345007374882698\n","119: 0.2002191599458456 0.16184816509485245 0.36206732504069805\n","120: 0.20435496419668198 0.17011481896042824 0.3744697831571102\n","121: 0.15918291732668877 0.16933299973607063 0.3285159170627594\n","122: 0.21404389292001724 0.1704757809638977 0.38451967388391495\n","123: 0.23008549213409424 0.16565704345703125 0.3957425355911255\n","124: 0.27858375757932663 0.166761826723814 0.44534558430314064\n","125: 0.23976966738700867 0.16479162499308586 0.40456129238009453\n","126: 0.17137163877487183 0.15860003978013992 0.32997167855501175\n","127: 0.24407808482646942 0.1733727641403675 0.41745084896683693\n","128: 0.24199127033352852 0.15891952067613602 0.40091079100966454\n","129: 0.2689194828271866 0.16367480531334877 0.43259428814053535\n","130: 0.23982815071940422 0.1680116429924965 0.4078397937119007\n","131: 0.2527513727545738 0.15589665994048119 0.408648032695055\n","132: 0.19929375499486923 0.15508712455630302 0.35438087955117226\n","133: 0.2563165947794914 0.15322266332805157 0.409539258107543\n","134: 0.26391443610191345 0.16439978778362274 0.4283142238855362\n","135: 0.23184918612241745 0.1560557447373867 0.38790493085980415\n","136: 0.19323337636888027 0.16570177301764488 0.35893514938652515\n","137: 0.2903093248605728 0.16369488835334778 0.4540042132139206\n","138: 0.16398360766470432 0.16106217354536057 0.3250457812100649\n","139: 0.20001376420259476 0.14890969544649124 0.348923459649086\n","140: 0.20706848800182343 0.15359430760145187 0.3606627956032753\n","141: 0.21418669447302818 0.17356134578585625 0.38774804025888443\n","142: 0.21299798041582108 0.1501923743635416 0.3631903547793627\n","143: 0.17595276609063148 0.15435749664902687 0.33031026273965836\n","144: 0.3019837513566017 0.16307926177978516 0.46506301313638687\n","145: 0.17325028777122498 0.15376462414860725 0.32701491191983223\n","146: 0.21934593841433525 0.15197597816586494 0.3713219165802002\n","147: 0.21408097073435783 0.16046975180506706 0.3745507225394249\n","148: 0.16527919843792915 0.14586078375577927 0.3111399821937084\n","149: 0.19875023886561394 0.15230078995227814 0.3510510288178921\n","150: 0.27637137472629547 0.14958438649773598 0.42595576122403145\n","151: 0.1918402463197708 0.14507743902504444 0.33691768534481525\n","152: 0.2543059140443802 0.14964346587657928 0.4039493799209595\n","153: 0.18702666461467743 0.15186143666505814 0.33888810127973557\n","154: 0.19418146461248398 0.1521972082555294 0.3463786728680134\n","155: 0.2257147580385208 0.15621524676680565 0.38193000480532646\n","156: 0.1839325949549675 0.14730827324092388 0.3312408681958914\n","157: 0.18906201794743538 0.14662332832813263 0.335685346275568\n","158: 0.22285492345690727 0.16977019608020782 0.3926251195371151\n","159: 0.30079907923936844 0.15932980179786682 0.46012888103723526\n","160: 0.22388744726777077 0.16499249264597893 0.3888799399137497\n","161: 0.2385891079902649 0.16250919923186302 0.4010983072221279\n","162: 0.270350344479084 0.15291648730635643 0.42326683178544044\n","163: 0.22110862657427788 0.15153854340314865 0.37264716997742653\n","164: 0.2730281464755535 0.16163582354784012 0.43466397002339363\n","165: 0.24848077446222305 0.16570700891315937 0.4141877833753824\n","166: 0.22052785754203796 0.1706525795161724 0.3911804370582104\n","167: 0.18130235373973846 0.16089096292853355 0.342193316668272\n","168: 0.2327176108956337 0.1490706354379654 0.3817882463335991\n","169: 0.20982662960886955 0.15000734478235245 0.359833974391222\n","170: 0.21971765160560608 0.16664419695734978 0.38636184856295586\n","171: 0.27515415847301483 0.15577666833996773 0.43093082681298256\n","172: 0.21859639883041382 0.1549318302422762 0.37352822907269\n","173: 0.2339698038995266 0.16174570471048355 0.39571550861001015\n","174: 0.24153513461351395 0.1601174920797348 0.40165262669324875\n","175: 0.20284372195601463 0.1512494683265686 0.35409319028258324\n","176: 0.20908942818641663 0.1619446948170662 0.3710341230034828\n","177: 0.20640313252806664 0.15148768573999405 0.3578908182680607\n","178: 0.2834223061800003 0.1460370346903801 0.4294593408703804\n","179: 0.2468111403286457 0.14998968690633774 0.39680082723498344\n","180: 0.2515895441174507 0.15904826670885086 0.4106378108263016\n","181: 0.17177783697843552 0.15679022669792175 0.32856806367635727\n","182: 0.1908454429358244 0.15090962871909142 0.3417550716549158\n","183: 0.2361203134059906 0.16241173073649406 0.39853204414248466\n","184: 0.16126944869756699 0.14378190971910954 0.3050513584166765\n","185: 0.22269979119300842 0.14205086790025234 0.36475065909326077\n","186: 0.22111668437719345 0.15425837971270084 0.3753750640898943\n","187: 0.2015920951962471 0.15979221276938915 0.36138430796563625\n","188: 0.15910374745726585 0.1450788825750351 0.30418263003230095\n","189: 0.20048225671052933 0.15573007240891457 0.3562123291194439\n","190: 0.24842478334903717 0.15725114941596985 0.405675932765007\n","191: 0.21956413611769676 0.14569917507469654 0.3652633111923933\n","192: 0.3060699552297592 0.15741853788495064 0.46348849311470985\n","193: 0.22679449245333672 0.15838740393519402 0.38518189638853073\n","194: 0.19735131412744522 0.1482866257429123 0.3456379398703575\n","195: 0.18394628539681435 0.14869970455765724 0.3326459899544716\n","196: 0.21095996722579002 0.14886419847607613 0.35982416570186615\n","197: 0.23732051253318787 0.15051906555891037 0.38783957809209824\n","198: 0.25132177025079727 0.1686633825302124 0.4199851527810097\n","199: 0.21470460668206215 0.15756848081946373 0.3722730875015259\n","200: 0.19399137794971466 0.1433744803071022 0.33736585825681686\n","201: 0.2531861513853073 0.14932047948241234 0.40250663086771965\n","202: 0.22855329513549805 0.14751890301704407 0.3760721981525421\n","203: 0.21606310456991196 0.145949374884367 0.36201247945427895\n","204: 0.21329517289996147 0.1469908505678177 0.36028602346777916\n","205: 0.23946379125118256 0.14845336601138115 0.3879171572625637\n","206: 0.19858073443174362 0.15380661562085152 0.35238735005259514\n","207: 0.21440855041146278 0.1501270830631256 0.3645356334745884\n","208: 0.2253115251660347 0.15798215568065643 0.38329368084669113\n","209: 0.20878665521740913 0.1551026776432991 0.36388933286070824\n","210: 0.2126079834997654 0.15366145223379135 0.36626943573355675\n","211: 0.2427796833217144 0.16384373232722282 0.4066234156489372\n","212: 0.19371643476188183 0.1413759756833315 0.3350924104452133\n","213: 0.15778177976608276 0.14862053841352463 0.3064023181796074\n","214: 0.18912425637245178 0.15465699508786201 0.3437812514603138\n","215: 0.19752000644803047 0.15693608298897743 0.3544560894370079\n","216: 0.22867242246866226 0.15969973802566528 0.38837216049432755\n","217: 0.19505351409316063 0.15340300649404526 0.3484565205872059\n","218: 0.22044963762164116 0.1561247557401657 0.37657439336180687\n","219: 0.21565071120858192 0.151579137891531 0.3672298491001129\n","220: 0.22306722030043602 0.15554430335760117 0.3786115236580372\n","221: 0.23405301570892334 0.15059836581349373 0.38465138152241707\n","222: 0.23747090250253677 0.1585373617708683 0.3960082642734051\n","223: 0.17480745166540146 0.14367495849728584 0.3184824101626873\n","224: 0.24369119107723236 0.15298808366060257 0.39667927473783493\n","225: 0.17542993277311325 0.1624884493649006 0.33791838213801384\n","226: 0.27094002068042755 0.15695341676473618 0.4278934374451637\n","227: 0.27998078614473343 0.14922194182872772 0.42920272797346115\n","228: 0.2846502214670181 0.15397237986326218 0.4386226013302803\n","229: 0.23920127749443054 0.160334512591362 0.39953579008579254\n","230: 0.2886704206466675 0.15160536020994186 0.44027578085660934\n","231: 0.206130713224411 0.15000540390610695 0.35613611713051796\n","232: 0.23722226545214653 0.14696355536580086 0.3841858208179474\n","233: 0.2705255560576916 0.16140800155699253 0.4319335576146841\n","234: 0.1285582110285759 0.1443416103720665 0.2728998214006424\n","235: 0.19783063605427742 0.15482941828668118 0.3526600543409586\n","236: 0.1976802758872509 0.14168225601315498 0.3393625319004059\n","237: 0.2029106318950653 0.1472550816833973 0.3501657135784626\n","238: 0.18746384978294373 0.14109616540372372 0.32856001518666744\n","239: 0.22196126729249954 0.14558347687125206 0.3675447441637516\n","240: 0.29825664311647415 0.15526943653821945 0.4535260796546936\n","241: 0.1980442777276039 0.1540682651102543 0.3521125428378582\n","242: 0.224857859313488 0.14338088408112526 0.36823874339461327\n","243: 0.2531493976712227 0.1615113988518715 0.4146607965230942\n","244: 0.19792097806930542 0.15071284025907516 0.3486338183283806\n","245: 0.24216460436582565 0.15692821890115738 0.39909282326698303\n","246: 0.1750062294304371 0.15739897638559341 0.3324052058160305\n","247: 0.25535160303115845 0.16013531386852264 0.4154869168996811\n","248: 0.22463051602244377 0.1560038886964321 0.3806344047188759\n","249: 0.1832796186208725 0.1510930322110653 0.3343726508319378\n","250: 0.23588014394044876 0.14582982659339905 0.3817099705338478\n","251: 0.21470914036035538 0.13954836688935757 0.35425750724971294\n","252: 0.24709699302911758 0.1495719514787197 0.3966689445078373\n","253: 0.1766132339835167 0.14829130470752716 0.32490453869104385\n","254: 0.20593595504760742 0.1467384621500969 0.3526744171977043\n","255: 0.23844227939844131 0.14887172169983387 0.3873140010982752\n","256: 0.1940314918756485 0.14380578882992268 0.3378372807055712\n","257: 0.23535553365945816 0.14464744925498962 0.3800029829144478\n","258: 0.21840791404247284 0.14807083830237389 0.3664787523448467\n","259: 0.20830004662275314 0.1613046806305647 0.36960472725331783\n","260: 0.23761674016714096 0.1424834504723549 0.38010019063949585\n","261: 0.18688547797501087 0.15625540725886822 0.3431408852338791\n","262: 0.24761142022907734 0.16873264871537685 0.4163440689444542\n","263: 0.22474897280335426 0.16100660152733326 0.3857555743306875\n","264: 0.1612398773431778 0.14906201884150505 0.31030189618468285\n","265: 0.17258642241358757 0.14720313996076584 0.3197895623743534\n","266: 0.17485303431749344 0.14998595044016838 0.3248389847576618\n","267: 0.16790444031357765 0.13975870050489902 0.3076631408184767\n","268: 0.19449807330965996 0.14664961025118828 0.34114768356084824\n","269: 0.22421525418758392 0.14611850585788488 0.3703337600454688\n","270: 0.1725924126803875 0.1453705132007599 0.3179629258811474\n","271: 0.29112718999385834 0.15881840139627457 0.4499455913901329\n","272: 0.2233591042459011 0.1425383761525154 0.3658974803984165\n","273: 0.1898285523056984 0.14038573950529099 0.3302142918109894\n","274: 0.18209976702928543 0.14051548019051552 0.32261524721980095\n","275: 0.21504337340593338 0.15063346177339554 0.3656768351793289\n","276: 0.3089871406555176 0.155117429792881 0.4641045704483986\n","277: 0.18473822623491287 0.14307857491075993 0.3278168011456728\n","278: 0.20996685698628426 0.1540297530591488 0.36399661004543304\n","279: 0.23437432572245598 0.14138696528971195 0.37576129101216793\n","280: 0.21122616156935692 0.15034765005111694 0.36157381162047386\n","281: 0.19051622226834297 0.14560141041874886 0.3361176326870918\n","282: 0.2185903638601303 0.14132927171885967 0.35991963557899\n","283: 0.20860671997070312 0.1405533365905285 0.3491600565612316\n","284: 0.1932019181549549 0.1459423378109932 0.3391442559659481\n","285: 0.22533604502677917 0.15058002434670925 0.3759160693734884\n","286: 0.2275000475347042 0.14739787578582764 0.37489792332053185\n","287: 0.18988929688930511 0.14317672699689865 0.33306602388620377\n","288: 0.23792237415909767 0.161449721083045 0.3993720952421427\n","289: 0.1681355983018875 0.14045820012688637 0.3085937984287739\n","290: 0.26098022423684597 0.1481976080685854 0.40917783230543137\n","291: 0.19058098271489143 0.14842251501977444 0.33900349773466587\n","292: 0.2907121405005455 0.16354794427752495 0.45426008477807045\n","293: 0.1748824305832386 0.16027198359370232 0.3351544141769409\n","294: 0.2509041279554367 0.1454947628080845 0.3963988907635212\n","295: 0.17496425099670887 0.13701884914189577 0.31198310013860464\n","296: 0.1862751916050911 0.1491679958999157 0.3354431875050068\n","297: 0.24185126274824142 0.14760923385620117 0.3894604966044426\n","298: 0.1939446460455656 0.15308703482151031 0.3470316808670759\n","299: 0.19500556215643883 0.14656490832567215 0.341570470482111\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625275121892,"user_tz":-60,"elapsed":13,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625275121893,"user_tz":-60,"elapsed":13,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"b499d1cc-c972-44eb-bcc0-c25a6a878cc7"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9736842105263158\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625275121893,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"5492358c-900f-465b-c5cb-31213072e8df"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9882697947214076\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625275121894,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":12,"outputs":[]}]}
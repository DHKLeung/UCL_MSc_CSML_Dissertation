{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN nb0.75 aug WPL0.75.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625274919859,"user_tz":-60,"elapsed":57198,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"4d12a4b3-e8f7-458d-f730-2b356e9b0dea"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625274920883,"user_tz":-60,"elapsed":1032,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625274921249,"user_tz":-60,"elapsed":367,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625274921249,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"5d401bb6-f078-465f-83a6-addb6c8801df"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","geometric_param = 0.75\n","perturb_loss_weight = 0.75\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7ffacbd77a90>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625274921250,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625274926995,"user_tz":-60,"elapsed":5748,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"ae2ca4a0-45d3-44a3-c13f-3a9118e2c41e"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1625274926995,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"id":"quiet-module","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1625274926996,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625274930304,"user_tz":-60,"elapsed":3311,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"9c0752e8-a070-4f3e-dfda-2994c2e57800"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup with random neighbour perturbation #\n","        mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_breast_nb(inputs, labels, geometric_param, alpha)\n","        \n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_nb))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_nb = augment_outputs[original_num:]\n","        mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_nb + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_nb.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_nb.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print decomposed losses #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:151: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n","  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.1383384466171265 2.1367753744125366 4.275113821029663\n","1: 2.1064079999923706 2.101792275905609 4.20820027589798\n","2: 2.049243927001953 2.046008586883545 4.095252513885498\n","3: 1.979017734527588 1.9720099568367004 3.9510276913642883\n","4: 1.888068675994873 1.8901099562644958 3.778178632259369\n","5: 1.8064868450164795 1.8018158078193665 3.608302652835846\n","6: 1.7008273005485535 1.7036027312278748 3.4044300317764282\n","7: 1.6049848794937134 1.6038753986358643 3.2088602781295776\n","8: 1.4897239804267883 1.4945291876792908 2.984253168106079\n","9: 1.401807963848114 1.3897734880447388 2.791581451892853\n","10: 1.2661372125148773 1.2690912783145905 2.5352284908294678\n","11: 1.1634820699691772 1.1538384556770325 2.3173205256462097\n","12: 1.0584204196929932 1.0510710775852203 2.1094914972782135\n","13: 0.9405180513858795 0.9536066651344299 1.8941247165203094\n","14: 0.8364757001399994 0.8535197973251343 1.6899954974651337\n","15: 0.7554261535406113 0.7756516635417938 1.531077817082405\n","16: 0.6961131542921066 0.7040578871965408 1.4001710414886475\n","17: 0.6174467951059341 0.6396818310022354 1.2571286261081696\n","18: 0.5880844593048096 0.5740576833486557 1.1621421426534653\n","19: 0.5473119616508484 0.5580664426088333 1.1053784042596817\n","20: 0.4965486228466034 0.5181720405817032 1.0147206634283066\n","21: 0.4726632907986641 0.48069287836551666 0.9533561691641808\n","22: 0.43723203241825104 0.45723073184490204 0.8944627642631531\n","23: 0.4464659318327904 0.4269826263189316 0.873448558151722\n","24: 0.4061477780342102 0.4019356071949005 0.8080833852291107\n","25: 0.3926156610250473 0.38435591012239456 0.7769715711474419\n","26: 0.3752540126442909 0.3662002459168434 0.7414542585611343\n","27: 0.35436712950468063 0.34744301438331604 0.7018101438879967\n","28: 0.32604626566171646 0.33357979357242584 0.6596260592341423\n","29: 0.34565794467926025 0.3197164684534073 0.6653744131326675\n","30: 0.326020322740078 0.30943015962839127 0.6354504823684692\n","31: 0.35688789933919907 0.30997905135154724 0.6668669506907463\n","32: 0.34271515160799026 0.29345835000276566 0.6361735016107559\n","33: 0.34093236178159714 0.2816177308559418 0.6225500926375389\n","34: 0.3077260032296181 0.27573874592781067 0.5834647491574287\n","35: 0.253510482609272 0.25847527384757996 0.511985756456852\n","36: 0.24595815688371658 0.2679490000009537 0.5139071568846703\n","37: 0.2197117954492569 0.2533699497580528 0.4730817452073097\n","38: 0.2700507268309593 0.23996680974960327 0.5100175365805626\n","39: 0.23208441585302353 0.2362261265516281 0.46831054240465164\n","40: 0.26770030707120895 0.23168206587433815 0.4993823729455471\n","41: 0.2628059387207031 0.23378968611359596 0.4965956248342991\n","42: 0.21446272730827332 0.24017682671546936 0.4546395540237427\n","43: 0.25746359676122665 0.2138722687959671 0.47133586555719376\n","44: 0.22950778901576996 0.20748644694685936 0.4369942359626293\n","45: 0.22667228430509567 0.2173151671886444 0.4439874514937401\n","46: 0.2391892410814762 0.20823007076978683 0.44741931185126305\n","47: 0.25248415023088455 0.21620002388954163 0.4686841741204262\n","48: 0.30769963562488556 0.2163410447537899 0.5240406803786755\n","49: 0.28099504858255386 0.19612034782767296 0.4771153964102268\n","50: 0.2528936639428139 0.19319912791252136 0.44609279185533524\n","51: 0.25406669825315475 0.1923767738044262 0.44644347205758095\n","52: 0.23702195286750793 0.19888024777173996 0.4359022006392479\n","53: 0.29596929624676704 0.18833161890506744 0.4843009151518345\n","54: 0.24032533913850784 0.19189728051424026 0.4322226196527481\n","55: 0.21572093665599823 0.18820519372820854 0.40392613038420677\n","56: 0.25687913969159126 0.17917504534125328 0.43605418503284454\n","57: 0.25922636687755585 0.18084093928337097 0.4400673061609268\n","58: 0.22209599614143372 0.17928820848464966 0.4013842046260834\n","59: 0.23146352916955948 0.17111226171255112 0.4025757908821106\n","60: 0.2391827069222927 0.17126896977424622 0.4104516766965389\n","61: 0.22643399983644485 0.17066557705402374 0.3970995768904686\n","62: 0.3150976933538914 0.17473752051591873 0.4898352138698101\n","63: 0.22240256518125534 0.17138293385505676 0.3937854990363121\n","64: 0.18572647869586945 0.16617725044488907 0.3519037291407585\n","65: 0.2231123447418213 0.1655801609158516 0.3886925056576729\n","66: 0.2189977690577507 0.1891050785779953 0.408102847635746\n","67: 0.23053081333637238 0.1635824777185917 0.39411329105496407\n","68: 0.16285864263772964 0.15978676825761795 0.3226454108953476\n","69: 0.19191113486886024 0.16438842937350273 0.356299564242363\n","70: 0.18828278966248035 0.15849070623517036 0.3467734958976507\n","71: 0.25867220759391785 0.1563180685043335 0.41499027609825134\n","72: 0.3050418719649315 0.17672641202807426 0.48176828399300575\n","73: 0.15891532972455025 0.1763419583439827 0.33525728806853294\n","74: 0.2137773483991623 0.16852642595767975 0.38230377435684204\n","75: 0.21783598139882088 0.15681551024317741 0.3746514916419983\n","76: 0.24551719427108765 0.1712392158806324 0.41675641015172005\n","77: 0.22250157594680786 0.15324267745018005 0.3757442533969879\n","78: 0.20875414833426476 0.16922304406762123 0.377977192401886\n","79: 0.261135958135128 0.14996295794844627 0.4110989160835743\n","80: 0.24316759407520294 0.1710374318063259 0.41420502588152885\n","81: 0.19351471215486526 0.14448661915957928 0.33800133131444454\n","82: 0.21047761663794518 0.14215636812150478 0.35263398475944996\n","83: 0.31430842727422714 0.14532154239714146 0.4596299696713686\n","84: 0.22563061863183975 0.14578049257397652 0.37141111120581627\n","85: 0.25156302750110626 0.15553417056798935 0.4070971980690956\n","86: 0.14749445393681526 0.1414356529712677 0.28893010690808296\n","87: 0.24064778164029121 0.14418489672243595 0.38483267836272717\n","88: 0.3303825557231903 0.15554839745163918 0.4859309531748295\n","89: 0.2057974711060524 0.15766949206590652 0.3634669631719589\n","90: 0.20116764679551125 0.1599156502634287 0.36108329705893993\n","91: 0.18200262635946274 0.13718477822840214 0.3191874045878649\n","92: 0.276739738881588 0.13679327070713043 0.4135330095887184\n","93: 0.205225576646626 0.14050928875803947 0.34573486540466547\n","94: 0.21198968216776848 0.15260634943842888 0.36459603160619736\n","95: 0.23059729114174843 0.13970018923282623 0.37029748037457466\n","96: 0.235124871134758 0.13612407632172108 0.3712489474564791\n","97: 0.1968652755022049 0.15190818905830383 0.34877346456050873\n","98: 0.22383825108408928 0.13565553724765778 0.35949378833174706\n","99: 0.23866741359233856 0.13189883902668953 0.3705662526190281\n","100: 0.17089547961950302 0.1387620698660612 0.30965754948556423\n","101: 0.20027233846485615 0.1353308167308569 0.33560315519571304\n","102: 0.14801662042737007 0.13241313584148884 0.2804297562688589\n","103: 0.13217488303780556 0.149686086922884 0.28186096996068954\n","104: 0.10633472166955471 0.13179724290966988 0.2381319645792246\n","105: 0.24615032970905304 0.13094690814614296 0.377097237855196\n","106: 0.16057844273746014 0.128219373524189 0.28879781626164913\n","107: 0.21793131157755852 0.13285741955041885 0.35078873112797737\n","108: 0.2033908162266016 0.12888061068952084 0.33227142691612244\n","109: 0.21873517334461212 0.12652593851089478 0.3452611118555069\n","110: 0.18351930007338524 0.14692779816687107 0.3304470982402563\n","111: 0.2119336761534214 0.12368587404489517 0.3356195501983166\n","112: 0.20137064903974533 0.12779410742223263 0.32916475646197796\n","113: 0.1474253386259079 0.12700077891349792 0.2744261175394058\n","114: 0.22975291311740875 0.12295433692634106 0.3527072500437498\n","115: 0.19387029111385345 0.14359213039278984 0.3374624215066433\n","116: 0.20223047584295273 0.12348233163356781 0.32571280747652054\n","117: 0.17506581917405128 0.12417768500745296 0.29924350418150425\n","118: 0.24274052679538727 0.12245926260948181 0.3651997894048691\n","119: 0.19005538523197174 0.1192494872957468 0.30930487252771854\n","120: 0.15859966166317463 0.12219507619738579 0.2807947378605604\n","121: 0.1647893339395523 0.12158200331032276 0.28637133724987507\n","122: 0.16138781607151031 0.12561853975057602 0.28700635582208633\n","123: 0.18330014497041702 0.12177254259586334 0.30507268756628036\n","124: 0.31601202487945557 0.12282110750675201 0.4388331323862076\n","125: 0.16166039928793907 0.12037538923323154 0.2820357885211706\n","126: 0.2012087032198906 0.11914101429283619 0.3203497175127268\n","127: 0.20228267461061478 0.12106285244226456 0.32334552705287933\n","128: 0.23254911229014397 0.1355116982012987 0.3680608104914427\n","129: 0.1835077404975891 0.11856449395418167 0.3020722344517708\n","130: 0.14677546545863152 0.13952964916825294 0.28630511462688446\n","131: 0.23464836925268173 0.13830233179032803 0.37295070104300976\n","132: 0.2005392722785473 0.12021749839186668 0.32075677067041397\n","133: 0.20193293504416943 0.13573874160647392 0.33767167665064335\n","134: 0.19943981990218163 0.11295595020055771 0.31239577010273933\n","135: 0.21731437742710114 0.11322795320302248 0.3305423306301236\n","136: 0.17734070867300034 0.111698804423213 0.28903951309621334\n","137: 0.2336154766380787 0.11749259941279888 0.35110807605087757\n","138: 0.12009814754128456 0.11824608594179153 0.2383442334830761\n","139: 0.22314979508519173 0.14094198495149612 0.36409178003668785\n","140: 0.167853444814682 0.12876159697771072 0.29661504179239273\n","141: 0.20200509205460548 0.13113107159733772 0.3331361636519432\n","142: 0.1945061758160591 0.13148866966366768 0.3259948454797268\n","143: 0.19798927754163742 0.11106421984732151 0.30905349738895893\n","144: 0.2049599550664425 0.12868112325668335 0.33364107832312584\n","145: 0.16049207001924515 0.11269733309745789 0.27318940311670303\n","146: 0.18694672547280788 0.11392482556402683 0.3008715510368347\n","147: 0.16189786791801453 0.11583050340414047 0.277728371322155\n","148: 0.18239823542535305 0.11167498864233494 0.294073224067688\n","149: 0.2079402506351471 0.11093920469284058 0.31887945532798767\n","150: 0.2726699858903885 0.11113911867141724 0.3838091045618057\n","151: 0.15731915459036827 0.1140921339392662 0.2714112885296345\n","152: 0.296424463391304 0.10918611660599709 0.4056105799973011\n","153: 0.18224118277430534 0.11377561464905739 0.29601679742336273\n","154: 0.22650514915585518 0.12988629564642906 0.35639144480228424\n","155: 0.21012428775429726 0.11499683931469917 0.32512112706899643\n","156: 0.20759164541959763 0.11047540605068207 0.3180670514702797\n","157: 0.22075603157281876 0.11398316826671362 0.3347391998395324\n","158: 0.19965871796011925 0.1262109000235796 0.32586961798369884\n","159: 0.22386743873357773 0.11431331187486649 0.3381807506084442\n","160: 0.15907283127307892 0.108252864331007 0.2673256956040859\n","161: 0.16594020649790764 0.13108564540743828 0.2970258519053459\n","162: 0.22070641070604324 0.117253752425313 0.33796016313135624\n","163: 0.14078885316848755 0.11203112825751305 0.2528199814260006\n","164: 0.16079941764473915 0.1091974638402462 0.26999688148498535\n","165: 0.13874169252812862 0.10996875166893005 0.24871044419705868\n","166: 0.21037649549543858 0.1118981409817934 0.322274636477232\n","167: 0.17372218146920204 0.11174021288752556 0.2854623943567276\n","168: 0.2158847637474537 0.12240536324679852 0.3382901269942522\n","169: 0.19026723876595497 0.10911754705011845 0.2993847858160734\n","170: 0.32179349288344383 0.11046100407838821 0.43225449696183205\n","171: 0.24163131043314934 0.10894864797592163 0.35057995840907097\n","172: 0.19604684226214886 0.10649509262293577 0.30254193488508463\n","173: 0.14904913678765297 0.11230908706784248 0.26135822385549545\n","174: 0.20764203369617462 0.11542195454239845 0.3230639882385731\n","175: 0.20304232463240623 0.11046794801950455 0.3135102726519108\n","176: 0.14091974589973688 0.10829709842801094 0.24921684432774782\n","177: 0.17968332022428513 0.11065390333533287 0.290337223559618\n","178: 0.24664457887411118 0.12842713296413422 0.3750717118382454\n","179: 0.21906759589910507 0.11244874075055122 0.3315163366496563\n","180: 0.1541324146091938 0.11052990332245827 0.26466231793165207\n","181: 0.21338998526334763 0.11222418583929539 0.325614171102643\n","182: 0.16339703649282455 0.11065681837499142 0.27405385486781597\n","183: 0.13555259816348553 0.1118320357054472 0.24738463386893272\n","184: 0.13524309173226357 0.10772373154759407 0.24296682327985764\n","185: 0.13783313240855932 0.10686828847974539 0.2447014208883047\n","186: 0.19327469170093536 0.11516659148037434 0.3084412831813097\n","187: 0.18739238381385803 0.11314871534705162 0.30054109916090965\n","188: 0.1276225484907627 0.113130709156394 0.24075325764715672\n","189: 0.2630198523402214 0.11430945806205273 0.37732931040227413\n","190: 0.24069960415363312 0.13174886256456375 0.37244846671819687\n","191: 0.20280714705586433 0.10974040441215038 0.3125475514680147\n","192: 0.28888940438628197 0.11212160438299179 0.40101100876927376\n","193: 0.2529832161962986 0.10576856043189764 0.35875177662819624\n","194: 0.19876730255782604 0.10901950672268867 0.3077868092805147\n","195: 0.181378535926342 0.1112754512578249 0.2926539871841669\n","196: 0.13780547305941582 0.12955722026526928 0.2673626933246851\n","197: 0.2847376763820648 0.13353139534592628 0.4182690717279911\n","198: 0.20088346302509308 0.11473565176129341 0.3156191147863865\n","199: 0.15155737102031708 0.10805685259401798 0.25961422361433506\n","200: 0.17388849705457687 0.11035927385091782 0.2842477709054947\n","201: 0.2121172919869423 0.11213986575603485 0.32425715774297714\n","202: 0.1906088963150978 0.10786058939993382 0.2984694857150316\n","203: 0.17710090428590775 0.10731261968612671 0.28441352397203445\n","204: 0.2163182646036148 0.11340084671974182 0.32971911132335663\n","205: 0.1766463704407215 0.12848822958767414 0.30513460002839565\n","206: 0.18073501996695995 0.10990775935351849 0.29064277932047844\n","207: 0.24753552675247192 0.12892508879303932 0.37646061554551125\n","208: 0.2254306599497795 0.10864800401031971 0.3340786639600992\n","209: 0.14076385088264942 0.12481780909001827 0.2655816599726677\n","210: 0.14407437294721603 0.11285724118351936 0.2569316141307354\n","211: 0.19857723638415337 0.10910166427493095 0.3076789006590843\n","212: 0.20113041624426842 0.1274260338395834 0.3285564500838518\n","213: 0.14310283213853836 0.1125945933163166 0.25569742545485497\n","214: 0.1611820012331009 0.1066628647968173 0.2678448660299182\n","215: 0.1889100894331932 0.10916903056204319 0.2980791199952364\n","216: 0.21492360159754753 0.10844896174967289 0.3233725633472204\n","217: 0.151625107973814 0.10873747617006302 0.26036258414387703\n","218: 0.16161190904676914 0.10860138945281506 0.2702132984995842\n","219: 0.17273559421300888 0.1133987233042717 0.2861343175172806\n","220: 0.20491860806941986 0.10918523371219635 0.3141038417816162\n","221: 0.14818400889635086 0.11169604770839214 0.259880056604743\n","222: 0.1751787681132555 0.10770370997488499 0.2828824780881405\n","223: 0.16187215968966484 0.10942945256829262 0.27130161225795746\n","224: 0.23866849392652512 0.12492422200739384 0.36359271593391895\n","225: 0.2606434337794781 0.12454953975975513 0.3851929735392332\n","226: 0.17547518759965897 0.10855193436145782 0.2840271219611168\n","227: 0.2416771650314331 0.10645744577050209 0.3481346108019352\n","228: 0.2358422353863716 0.12946331314742565 0.36530554853379726\n","229: 0.2887538932263851 0.1314205266535282 0.42017441987991333\n","230: 0.19686133414506912 0.1074875146150589 0.304348848760128\n","231: 0.17401934415102005 0.10876836255192757 0.2827877067029476\n","232: 0.2822701707482338 0.12326259352266788 0.4055327642709017\n","233: 0.3229388892650604 0.12840007431805134 0.45133896358311176\n","234: 0.1469261571764946 0.11205705255270004 0.25898320972919464\n","235: 0.15354781225323677 0.1067652478814125 0.2603130601346493\n","236: 0.17451281286776066 0.10987301729619503 0.2843858301639557\n","237: 0.16775406152009964 0.11420059949159622 0.28195466101169586\n","238: 0.21802660077810287 0.11625946313142776 0.33428606390953064\n","239: 0.19813303649425507 0.10667993128299713 0.3048129677772522\n","240: 0.30564212799072266 0.11194200813770294 0.4175841361284256\n","241: 0.2776224687695503 0.1144042108207941 0.39202667959034443\n","242: 0.1577334627509117 0.10804127342998981 0.2657747361809015\n","243: 0.19218174368143082 0.11109386943280697 0.3032756131142378\n","244: 0.1776027213782072 0.10635818913578987 0.2839609105139971\n","245: 0.18838492408394814 0.10899491794407368 0.2973798420280218\n","246: 0.14218174293637276 0.11245467886328697 0.25463642179965973\n","247: 0.20099396258592606 0.10860711336135864 0.3096010759472847\n","248: 0.1991749219596386 0.1080247201025486 0.3071996420621872\n","249: 0.1890056487172842 0.12469569966197014 0.31370134837925434\n","250: 0.2319006249308586 0.11140179634094238 0.343302421271801\n","251: 0.19435350224375725 0.10919403284788132 0.30354753509163857\n","252: 0.24263391643762589 0.11228724755346775 0.35492116399109364\n","253: 0.16328932158648968 0.12439901195466518 0.28768833354115486\n","254: 0.2134074568748474 0.1165616512298584 0.3299691081047058\n","255: 0.17810150980949402 0.10822715237736702 0.28632866218686104\n","256: 0.14452598243951797 0.10727787017822266 0.25180385261774063\n","257: 0.21624324284493923 0.10793906264007092 0.32418230548501015\n","258: 0.21576640754938126 0.1152016818523407 0.33096808940172195\n","259: 0.189794784411788 0.1323548574000597 0.3221496418118477\n","260: 0.20067189633846283 0.10348628275096416 0.304158179089427\n","261: 0.17868514731526375 0.12350957840681076 0.3021947257220745\n","262: 0.1975211352109909 0.1259280201047659 0.3234491553157568\n","263: 0.23266611248254776 0.10478147491812706 0.3374475874006748\n","264: 0.15566580183804035 0.10820440575480461 0.26387020759284496\n","265: 0.14795328583568335 0.11016738694161177 0.2581206727772951\n","266: 0.19947852566838264 0.1297372579574585 0.32921578362584114\n","267: 0.12404814176261425 0.10843458026647568 0.23248272202908993\n","268: 0.18628789484500885 0.1114313006401062 0.29771919548511505\n","269: 0.22830897569656372 0.10950906574726105 0.33781804144382477\n","270: 0.23249021917581558 0.12439616583287716 0.35688638500869274\n","271: 0.2865835428237915 0.11310775578022003 0.39969129860401154\n","272: 0.16838087514042854 0.10395242273807526 0.2723332978785038\n","273: 0.19757653027772903 0.12194615602493286 0.3195226863026619\n","274: 0.2454456388950348 0.1059611551463604 0.3514067940413952\n","275: 0.16480055078864098 0.11092522367835045 0.2757257744669914\n","276: 0.22115696594119072 0.10760773159563541 0.32876469753682613\n","277: 0.2740805111825466 0.12523913756012917 0.3993196487426758\n","278: 0.21803045086562634 0.10528479516506195 0.3233152460306883\n","279: 0.2805212326347828 0.10731927864253521 0.387840511277318\n","280: 0.14911597594618797 0.12545313872396946 0.27456911467015743\n","281: 0.23857783898711205 0.1289461124688387 0.36752395145595074\n","282: 0.22341981530189514 0.10947734862565994 0.3328971639275551\n","283: 0.19653036072850227 0.11440832912921906 0.31093868985772133\n","284: 0.2677917964756489 0.1106080487370491 0.378399845212698\n","285: 0.23120715469121933 0.10638955794274807 0.3375967126339674\n","286: 0.23237383738160133 0.10770448483526707 0.3400783222168684\n","287: 0.23535113781690598 0.11231848038733006 0.34766961820423603\n","288: 0.2174220122396946 0.11040684953331947 0.32782886177301407\n","289: 0.17583588883280754 0.10583585873246193 0.28167174756526947\n","290: 0.26347456872463226 0.10957787930965424 0.3730524480342865\n","291: 0.1460608933120966 0.125169076025486 0.2712299693375826\n","292: 0.20133063942193985 0.11020422354340553 0.3115348629653454\n","293: 0.17108118534088135 0.1266086921095848 0.29768987745046616\n","294: 0.2333335429430008 0.11041790433228016 0.34375144727528095\n","295: 0.15272360667586327 0.10833332873880863 0.2610569354146719\n","296: 0.237988518550992 0.10396117810159922 0.34194969665259123\n","297: 0.192892424762249 0.10584457591176033 0.2987370006740093\n","298: 0.19035204499959946 0.12662095949053764 0.3169730044901371\n","299: 0.24431052803993225 0.12328178063035011 0.36759230867028236\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625274930305,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625274930305,"user_tz":-60,"elapsed":11,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"5a2a6cb1-f605-4a0e-b33e-74baee96f1ac"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9649122807017544\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625274930306,"user_tz":-60,"elapsed":7,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"5388fd7c-2e4a-476c-d4b0-da42cb3e174d"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9970674486803519\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625274930306,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":12,"outputs":[]}]}
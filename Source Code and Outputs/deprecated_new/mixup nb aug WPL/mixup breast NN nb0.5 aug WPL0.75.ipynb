{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN nb0.5 aug WPL0.75.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625274557272,"user_tz":-60,"elapsed":21890,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"3219d71d-cc22-4632-bb00-b0e4b0560c2f"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625274558208,"user_tz":-60,"elapsed":939,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625274558209,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625274558209,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"bc7be5d4-b393-4e1e-d401-376d4496f0d3"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","geometric_param = 0.5\n","perturb_loss_weight = 0.75\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fd0a6c8da50>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625274558209,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625274564262,"user_tz":-60,"elapsed":6056,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"644efc8b-4248-4b9f-8896-2cbe8df64bfc"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1625274564263,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"id":"quiet-module","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1625274564263,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625274567587,"user_tz":-60,"elapsed":3327,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"204e55b7-24cf-4f81-976c-bbcbb792b529"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup with random neighbour perturbation #\n","        mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_breast_nb(inputs, labels, geometric_param, alpha)\n","        \n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_nb))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_nb = augment_outputs[original_num:]\n","        mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_nb + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_nb.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_nb.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print decomposed losses #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:151: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n","  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.1440510749816895 2.1419355869293213 4.285986661911011\n","1: 2.1099321842193604 2.1056857109069824 4.215617895126343\n","2: 2.038690984249115 2.039808988571167 4.078499972820282\n","3: 1.9608619213104248 1.9569478034973145 3.9178097248077393\n","4: 1.8538286685943604 1.8619326949119568 3.715761363506317\n","5: 1.764367163181305 1.7634767293930054 3.5278438925743103\n","6: 1.6586159467697144 1.6591266989707947 3.317742645740509\n","7: 1.5393982529640198 1.5420481860637665 3.0814464390277863\n","8: 1.409039556980133 1.4185816943645477 2.827621251344681\n","9: 1.3119495511054993 1.3119716346263885 2.623921185731888\n","10: 1.155487835407257 1.1794460713863373 2.3349339067935944\n","11: 1.049704223871231 1.0622584521770477 2.111962676048279\n","12: 0.9442615211009979 0.9620932042598724 1.9063547253608704\n","13: 0.8069540411233902 0.8453624844551086 1.6523165255784988\n","14: 0.7487746775150299 0.7645994126796722 1.5133740901947021\n","15: 0.6480423212051392 0.6606124043464661 1.3086547255516052\n","16: 0.610875204205513 0.6190655827522278 1.2299407869577408\n","17: 0.5221020877361298 0.566669151186943 1.0887712389230728\n","18: 0.5287137031555176 0.5246559381484985 1.0533696413040161\n","19: 0.47373227775096893 0.4748057574033737 0.9485380351543427\n","20: 0.4856244772672653 0.45953719317913055 0.9451616704463959\n","21: 0.44908909499645233 0.41786427050828934 0.8669533655047417\n","22: 0.4167957380414009 0.4034316688776016 0.8202274069190025\n","23: 0.3911158889532089 0.3725113049149513 0.7636271938681602\n","24: 0.39085693657398224 0.352079801261425 0.7429367378354073\n","25: 0.3349840044975281 0.34488721936941147 0.6798712238669395\n","26: 0.3851318806409836 0.35317906737327576 0.7383109480142593\n","27: 0.33101629465818405 0.30901138484477997 0.640027679502964\n","28: 0.31807781755924225 0.30363861471414566 0.6217164322733879\n","29: 0.3085416853427887 0.3027174472808838 0.6112591326236725\n","30: 0.3112484961748123 0.2982452064752579 0.6094937026500702\n","31: 0.3218256086111069 0.28006966412067413 0.601895272731781\n","32: 0.2799767553806305 0.27208007872104645 0.5520568341016769\n","33: 0.2586202099919319 0.2458403781056404 0.5044605880975723\n","34: 0.2702319771051407 0.24194273352622986 0.5121747106313705\n","35: 0.24915681779384613 0.23942185938358307 0.4885786771774292\n","36: 0.2625689096748829 0.22260732389986515 0.48517623357474804\n","37: 0.250786405056715 0.24988995492458344 0.5006763599812984\n","38: 0.2725384309887886 0.21773680299520493 0.49027523398399353\n","39: 0.31297025084495544 0.22932952642440796 0.5422997772693634\n","40: 0.27501827478408813 0.21042130514979362 0.48543957993388176\n","41: 0.2341584712266922 0.2156175822019577 0.4497760534286499\n","42: 0.18901939690113068 0.2203649953007698 0.4093843922019005\n","43: 0.2546190321445465 0.20218988880515099 0.4568089209496975\n","44: 0.26357361301779747 0.216613058000803 0.48018667101860046\n","45: 0.23261117190122604 0.20885127782821655 0.4414624497294426\n","46: 0.18919702991843224 0.19260811060667038 0.3818051405251026\n","47: 0.20155449956655502 0.19498374313116074 0.39653824269771576\n","48: 0.18621937185525894 0.192616306245327 0.37883567810058594\n","49: 0.2851485088467598 0.20112908631563187 0.48627759516239166\n","50: 0.19002068042755127 0.17964552342891693 0.3696662038564682\n","51: 0.26817090064287186 0.1910334676504135 0.45920436829328537\n","52: 0.224134661257267 0.18402952700853348 0.4081641882658005\n","53: 0.2231474071741104 0.1856875717639923 0.4088349789381027\n","54: 0.210493428632617 0.17157026939094067 0.38206369802355766\n","55: 0.20836623758077621 0.19148731976747513 0.39985355734825134\n","56: 0.2630580738186836 0.1712973453104496 0.4343554191291332\n","57: 0.22882802039384842 0.17629773914813995 0.4051257595419884\n","58: 0.21297771111130714 0.16621223464608192 0.37918994575738907\n","59: 0.22309083491563797 0.16649295389652252 0.3895837888121605\n","60: 0.2235531359910965 0.19816634431481361 0.4217194803059101\n","61: 0.2737986892461777 0.19158798456192017 0.46538667380809784\n","62: 0.1990780122578144 0.16894404217600822 0.36802205443382263\n","63: 0.23035182058811188 0.18377919495105743 0.4141310155391693\n","64: 0.21100813895463943 0.17313135787844658 0.384139496833086\n","65: 0.23427403718233109 0.1554593127220869 0.389733349904418\n","66: 0.21997521445155144 0.16899049282073975 0.3889657072722912\n","67: 0.3481859937310219 0.17553160712122917 0.523717600852251\n","68: 0.22328968346118927 0.16147982701659203 0.3847695104777813\n","69: 0.2511333879083395 0.15311786904931068 0.4042512569576502\n","70: 0.1675245203077793 0.15445546992123127 0.3219799902290106\n","71: 0.27521485835313797 0.15427332744002342 0.4294881857931614\n","72: 0.28436604142189026 0.1688229739665985 0.45318901538848877\n","73: 0.21226835250854492 0.15471937134861946 0.3669877238571644\n","74: 0.23820769041776657 0.15642798133194447 0.39463567174971104\n","75: 0.23129146918654442 0.1545185185968876 0.385809987783432\n","76: 0.25545474886894226 0.14922376722097397 0.40467851608991623\n","77: 0.24019550904631615 0.15655013173818588 0.39674564078450203\n","78: 0.26397179439663887 0.1518719159066677 0.4158437103033066\n","79: 0.24895085394382477 0.14802549220621586 0.3969763461500406\n","80: 0.16440798714756966 0.15552911907434464 0.3199371062219143\n","81: 0.16402029618620872 0.17099659331142902 0.33501688949763775\n","82: 0.28936097770929337 0.14546372555196285 0.4348247032612562\n","83: 0.27096354961395264 0.14968689531087875 0.4206504449248314\n","84: 0.26196949928998947 0.15383708663284779 0.41580658592283726\n","85: 0.23784346505999565 0.1464136466383934 0.38425711169838905\n","86: 0.17676248401403427 0.1488034427165985 0.3255659267306328\n","87: 0.17981042340397835 0.144597839564085 0.32440826296806335\n","88: 0.1637449786067009 0.14271051436662674 0.30645549297332764\n","89: 0.1968320645391941 0.13932905718684196 0.33616112172603607\n","90: 0.19025206565856934 0.13878813199698925 0.3290401976555586\n","91: 0.18011726066470146 0.1392065491527319 0.31932380981743336\n","92: 0.30873432755470276 0.1530148983001709 0.46174922585487366\n","93: 0.2648985832929611 0.1675388514995575 0.4324374347925186\n","94: 0.1710822768509388 0.16636045277118683 0.3374427296221256\n","95: 0.2064724750816822 0.14914393611252308 0.3556164111942053\n","96: 0.24509606510400772 0.1393091268837452 0.3844051919877529\n","97: 0.1657563541084528 0.1397854331880808 0.3055417872965336\n","98: 0.22666030004620552 0.1391752939671278 0.3658355940133333\n","99: 0.2047373428940773 0.13885841518640518 0.3435957580804825\n","100: 0.21132691949605942 0.13839947991073132 0.34972639940679073\n","101: 0.19227177370339632 0.13617187552154064 0.32844364922493696\n","102: 0.19350054115056992 0.1363601814955473 0.3298607226461172\n","103: 0.13936975225806236 0.16141227073967457 0.30078202299773693\n","104: 0.14934556558728218 0.13921406120061874 0.2885596267879009\n","105: 0.2446729987859726 0.13387239910662174 0.37854539789259434\n","106: 0.16790414229035378 0.13555788062512875 0.3034620229154825\n","107: 0.21136979013681412 0.13105524517595768 0.3424250353127718\n","108: 0.26766275614500046 0.16212771832942963 0.4297904744744301\n","109: 0.2655903920531273 0.13976076245307922 0.4053511545062065\n","110: 0.14590478874742985 0.1329786293208599 0.27888341806828976\n","111: 0.1917766034603119 0.15040099248290062 0.3421775959432125\n","112: 0.24070025235414505 0.13745459727942944 0.3781548496335745\n","113: 0.2662666514515877 0.13536118157207966 0.40162783302366734\n","114: 0.18060576915740967 0.13516958057880402 0.3157753497362137\n","115: 0.19854100793600082 0.1289586592465639 0.32749966718256474\n","116: 0.19239790551364422 0.14776037633419037 0.3401582818478346\n","117: 0.18906094878911972 0.1310406494885683 0.320101598277688\n","118: 0.22490322217345238 0.12924681045114994 0.3541500326246023\n","119: 0.17193234711885452 0.12877711281180382 0.30070945993065834\n","120: 0.13934820890426636 0.1511054579168558 0.29045366682112217\n","121: 0.16536155715584755 0.1284487061202526 0.29381026327610016\n","122: 0.13705620355904102 0.1257349057123065 0.2627911092713475\n","123: 0.2207108996808529 0.13089612312614918 0.35160702280700207\n","124: 0.21874933317303658 0.13969023898243904 0.3584395721554756\n","125: 0.20775046199560165 0.1270434595644474 0.33479392156004906\n","126: 0.14169450476765633 0.1258267480880022 0.26752125285565853\n","127: 0.17927353084087372 0.13254759833216667 0.3118211291730404\n","128: 0.24892744049429893 0.1273659560829401 0.37629339657723904\n","129: 0.2163514941930771 0.13180703297257423 0.3481585271656513\n","130: 0.28757740929722786 0.1251380518078804 0.41271546110510826\n","131: 0.2444879561662674 0.12891589663922787 0.37340385280549526\n","132: 0.17082223296165466 0.146906565874815 0.31772879883646965\n","133: 0.22679776512086391 0.1441670972853899 0.3709648624062538\n","134: 0.21466444432735443 0.1238015666604042 0.33846601098775864\n","135: 0.25752441585063934 0.12255370803177357 0.3800781238824129\n","136: 0.235849741846323 0.13229955360293388 0.3681492954492569\n","137: 0.26603471487760544 0.1512482799589634 0.41728299483656883\n","138: 0.1355627328157425 0.12574913911521435 0.26131187193095684\n","139: 0.24301714077591896 0.12532222643494606 0.368339367210865\n","140: 0.21794559806585312 0.12492838688194752 0.34287398494780064\n","141: 0.21046162769198418 0.13227003440260887 0.34273166209459305\n","142: 0.27385782077908516 0.12624289095401764 0.4001007117331028\n","143: 0.17212459444999695 0.13633964024484158 0.3084642346948385\n","144: 0.22124602645635605 0.12241607531905174 0.3436621017754078\n","145: 0.16704781912267208 0.13281262665987015 0.29986044578254223\n","146: 0.2236531339585781 0.12702573277056217 0.3506788667291403\n","147: 0.20146908052265644 0.12876758351922035 0.3302366640418768\n","148: 0.18903318233788013 0.12348808534443378 0.3125212676823139\n","149: 0.22417368739843369 0.12546495720744133 0.349638644605875\n","150: 0.22387751564383507 0.12540815398097038 0.34928566962480545\n","151: 0.20420461148023605 0.1259340550750494 0.33013866655528545\n","152: 0.17770309001207352 0.12362506054341793 0.30132815055549145\n","153: 0.20523610897362232 0.12149644084274769 0.32673254981637\n","154: 0.21617573499679565 0.1223707515746355 0.33854648657143116\n","155: 0.184418685734272 0.1465357169508934 0.3309544026851654\n","156: 0.2408464103937149 0.12479772791266441 0.3656441383063793\n","157: 0.21233683452010155 0.12165414541959763 0.3339909799396992\n","158: 0.20948931202292442 0.12117088586091995 0.3306601978838444\n","159: 0.24300535023212433 0.12640425190329552 0.36940960213541985\n","160: 0.1424904242157936 0.14695797115564346 0.2894483953714371\n","161: 0.16560732573270798 0.13106486573815346 0.29667219147086143\n","162: 0.268094502389431 0.12534332647919655 0.39343782886862755\n","163: 0.19234227389097214 0.1272364743053913 0.31957874819636345\n","164: 0.1821587271988392 0.13180754706263542 0.3139662742614746\n","165: 0.18369606137275696 0.12225841730833054 0.3059544786810875\n","166: 0.1681768037378788 0.1208720225840807 0.2890488263219595\n","167: 0.1418154016137123 0.12343328818678856 0.26524868980050087\n","168: 0.19122697785496712 0.1492902711033821 0.34051724895834923\n","169: 0.1848706528544426 0.11993481032550335 0.30480546317994595\n","170: 0.15504653565585613 0.12226352654397488 0.277310062199831\n","171: 0.15586066991090775 0.12584776245057583 0.2817084323614836\n","172: 0.2390301302075386 0.14264470152556896 0.38167483173310757\n","173: 0.15294765681028366 0.1228560321033001 0.27580368891358376\n","174: 0.17894085869193077 0.12233082018792629 0.30127167887985706\n","175: 0.17894072458148003 0.14362378418445587 0.3225645087659359\n","176: 0.13663902878761292 0.12730016559362411 0.26393919438123703\n","177: 0.21895870566368103 0.14039252698421478 0.3593512326478958\n","178: 0.29927680641412735 0.12222648598253727 0.4215032923966646\n","179: 0.2567811347544193 0.14832236617803574 0.40510350093245506\n","180: 0.19563737884163857 0.12930425629019737 0.32494163513183594\n","181: 0.22551853954792023 0.14293865486979485 0.3684571944177151\n","182: 0.08752402104437351 0.12685345858335495 0.21437747962772846\n","183: 0.15746422298252583 0.12133285589516163 0.27879707887768745\n","184: 0.15973665565252304 0.12958696484565735 0.2893236204981804\n","185: 0.2890438511967659 0.1300938595086336 0.4191377107053995\n","186: 0.24774078279733658 0.15163039416074753 0.3993711769580841\n","187: 0.1771278101950884 0.11971346940845251 0.2968412796035409\n","188: 0.11436782777309418 0.1232182402163744 0.23758606798946857\n","189: 0.15581414476037025 0.1232562642544508 0.27907040901482105\n","190: 0.20965763181447983 0.14152419194579124 0.3511818237602711\n","191: 0.18632589653134346 0.11997998505830765 0.3063058815896511\n","192: 0.25012393295764923 0.12653953582048416 0.3766634687781334\n","193: 0.25254231691360474 0.14496647752821445 0.3975087944418192\n","194: 0.23746871203184128 0.15334859117865562 0.3908173032104969\n","195: 0.17508668079972267 0.12970187701284885 0.3047885578125715\n","196: 0.22681263834238052 0.12518894858658314 0.35200158692896366\n","197: 0.26190998777747154 0.1411452703177929 0.40305525809526443\n","198: 0.26939044892787933 0.14143938571214676 0.4108298346400261\n","199: 0.2078828364610672 0.12219016626477242 0.3300730027258396\n","200: 0.2207036167383194 0.13043634593486786 0.35113996267318726\n","201: 0.2451929859817028 0.13244819082319736 0.37764117680490017\n","202: 0.19651130586862564 0.141243951395154 0.33775525726377964\n","203: 0.2134130448102951 0.12804768607020378 0.3414607308804989\n","204: 0.2319704294204712 0.14154872670769691 0.3735191561281681\n","205: 0.17409756034612656 0.12169244885444641 0.29579000920057297\n","206: 0.15892881900072098 0.11977013945579529 0.27869895845651627\n","207: 0.1648852489888668 0.11855364218354225 0.28343889117240906\n","208: 0.2752354294061661 0.12757674790918827 0.40281217731535435\n","209: 0.2026963960379362 0.12169802002608776 0.32439441606402397\n","210: 0.26868847757577896 0.14191438630223274 0.4106028638780117\n","211: 0.16932525485754013 0.1322535201907158 0.3015787750482559\n","212: 0.1987508088350296 0.14257455617189407 0.3413253650069237\n","213: 0.16446513310074806 0.12015809118747711 0.2846232242882252\n","214: 0.1874735690653324 0.14247022569179535 0.32994379475712776\n","215: 0.2125384286046028 0.11921714898198843 0.33175557758659124\n","216: 0.24487825110554695 0.12212745286524296 0.3670057039707899\n","217: 0.14824941754341125 0.12594427168369293 0.2741936892271042\n","218: 0.1526467688381672 0.14414462447166443 0.2967913933098316\n","219: 0.19392666220664978 0.12561410665512085 0.31954076886177063\n","220: 0.19276699051260948 0.1251777894794941 0.3179447799921036\n","221: 0.21754329651594162 0.120451545342803 0.3379948418587446\n","222: 0.18598034232854843 0.14270319417119026 0.3286835364997387\n","223: 0.1710405834019184 0.11927301622927189 0.2903135996311903\n","224: 0.24035046249628067 0.1469867518171668 0.3873372143134475\n","225: 0.11195930652320385 0.14107693172991276 0.2530362382531166\n","226: 0.24044977501034737 0.13150350376963615 0.3719532787799835\n","227: 0.23294366151094437 0.13896091282367706 0.37190457433462143\n","228: 0.24221756681799889 0.14414234086871147 0.38635990768671036\n","229: 0.20402854308485985 0.1443728804588318 0.34840142354369164\n","230: 0.22190744057297707 0.12046322971582413 0.3423706702888012\n","231: 0.19477864541113377 0.12162857502698898 0.31640722043812275\n","232: 0.2277340665459633 0.13130190409719944 0.3590359706431627\n","233: 0.236789308488369 0.1295417882502079 0.3663310967385769\n","234: 0.2021280936896801 0.12347318977117538 0.3256012834608555\n","235: 0.13810466974973679 0.12976577877998352 0.2678704485297203\n","236: 0.16727609559893608 0.12016940861940384 0.2874455042183399\n","237: 0.20427019149065018 0.118575694039464 0.3228458855301142\n","238: 0.16337813436985016 0.12136716209352016 0.2847452964633703\n","239: 0.1903478354215622 0.1418619602918625 0.3322097957134247\n","240: 0.3618006333708763 0.14276359230279922 0.5045642256736755\n","241: 0.17674820870161057 0.141261113807559 0.3180093225091696\n","242: 0.23027049005031586 0.12034707516431808 0.35061756521463394\n","243: 0.2576082833111286 0.14642897993326187 0.4040372632443905\n","244: 0.17201346345245838 0.12445872835814953 0.2964721918106079\n","245: 0.17428582161664963 0.12229735031723976 0.2965831719338894\n","246: 0.14231382496654987 0.11917464062571526 0.26148846559226513\n","247: 0.22504713386297226 0.12028327398002148 0.34533040784299374\n","248: 0.20550819858908653 0.11825701128691435 0.3237652098760009\n","249: 0.19229086861014366 0.12182693742215633 0.3141178060323\n","250: 0.2644672393798828 0.14090307988226414 0.40537031926214695\n","251: 0.18458837270736694 0.12320208735764027 0.3077904600650072\n","252: 0.19484985619783401 0.12175914458930492 0.31660900078713894\n","253: 0.20401329547166824 0.14206768944859505 0.3460809849202633\n","254: 0.2231149859726429 0.11944740265607834 0.34256238862872124\n","255: 0.3041238710284233 0.12023473717272282 0.4243586082011461\n","256: 0.2050083689391613 0.1421649269759655 0.3471732959151268\n","257: 0.19319616071879864 0.12062550336122513 0.31382166408002377\n","258: 0.23429155349731445 0.14026007056236267 0.3745516240596771\n","259: 0.19082799553871155 0.1399647556245327 0.33079275116324425\n","260: 0.21604544296860695 0.12095659971237183 0.3370020426809788\n","261: 0.1767626740038395 0.12190642766654491 0.2986691016703844\n","262: 0.1788366660475731 0.1389218084514141 0.3177584744989872\n","263: 0.1980886310338974 0.1401984728872776 0.338287103921175\n","264: 0.14076686277985573 0.12074478529393673 0.26151164807379246\n","265: 0.19220726191997528 0.12162051349878311 0.3138277754187584\n","266: 0.21204764023423195 0.11708251666277647 0.3291301568970084\n","267: 0.15278072282671928 0.12191356718540192 0.2746942900121212\n","268: 0.22797565534710884 0.11997606791555882 0.34795172326266766\n","269: 0.21210484579205513 0.13881439343094826 0.3509192392230034\n","270: 0.22491397708654404 0.14490601420402527 0.3698199912905693\n","271: 0.19591432809829712 0.12113936617970467 0.3170536942780018\n","272: 0.19274888932704926 0.11995816417038441 0.31270705349743366\n","273: 0.11139282584190369 0.11982853524386883 0.23122136108577251\n","274: 0.2010948359966278 0.14430012553930283 0.34539496153593063\n","275: 0.1385091207921505 0.1211649477481842 0.2596740685403347\n","276: 0.18844987824559212 0.11995812878012657 0.3084080070257187\n","277: 0.20672771334648132 0.11943027004599571 0.32615798339247704\n","278: 0.19568618386983871 0.1245962455868721 0.3202824294567108\n","279: 0.23355725780129433 0.11979355104267597 0.3533508088439703\n","280: 0.17651456221938133 0.12680612690746784 0.3033206891268492\n","281: 0.19407348334789276 0.13886846601963043 0.3329419493675232\n","282: 0.1622021086513996 0.12548362463712692 0.28768573328852654\n","283: 0.22749564796686172 0.11853582039475441 0.34603146836161613\n","284: 0.29683301597833633 0.1302282540127635 0.42706126999109983\n","285: 0.20317862927913666 0.1239762231707573 0.32715485244989395\n","286: 0.18905747309327126 0.12043209746479988 0.30948957055807114\n","287: 0.20832277834415436 0.12369468808174133 0.3320174664258957\n","288: 0.22430222481489182 0.1187449898570776 0.3430472146719694\n","289: 0.17759857699275017 0.14008126873522997 0.31767984572798014\n","290: 0.24993649125099182 0.11932135745882988 0.3692578487098217\n","291: 0.19549426808953285 0.1290629841387272 0.32455725222826004\n","292: 0.19726893119513988 0.11739875748753548 0.31466768868267536\n","293: 0.2074269950389862 0.13870103284716606 0.34612802788615227\n","294: 0.19185234233736992 0.13789642415940762 0.32974876649677753\n","295: 0.19869694113731384 0.13966489396989346 0.3383618351072073\n","296: 0.18677425011992455 0.12654772400856018 0.3133219741284847\n","297: 0.2136010304093361 0.1211419515311718 0.3347429819405079\n","298: 0.17633420415222645 0.11779307946562767 0.2941272836178541\n","299: 0.1519663706421852 0.13197630271315575 0.28394267335534096\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625274567588,"user_tz":-60,"elapsed":13,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625274567588,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"4310c2a6-8874-42ee-fa6f-02006213ae2b"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9473684210526315\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625274567589,"user_tz":-60,"elapsed":7,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"6d928cf6-23aa-4791-dfc4-2a2f2eab5280"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9941348973607038\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625274567589,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":12,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN nb0.5 aug WPL0.9.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625274338329,"user_tz":-60,"elapsed":17398,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"338ec275-7fd7-4201-8702-a12ff6aad749"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625274339087,"user_tz":-60,"elapsed":760,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625274339480,"user_tz":-60,"elapsed":394,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625274339481,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"338c8c2e-70e6-44bf-86f4-d15a22718fcc"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","geometric_param = 0.5\n","perturb_loss_weight = 0.9\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f65b05d4a90>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625274339481,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625274346339,"user_tz":-60,"elapsed":6582,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"1191bce2-f1ca-4f19-b616-4701c20e00ee"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1625274346339,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"id":"quiet-module","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1625274346340,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625274349835,"user_tz":-60,"elapsed":3498,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"e01dafa2-afc1-43f0-a168-14dd343e21f8"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup with random neighbour perturbation #\n","        mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_breast_nb(inputs, labels, geometric_param, alpha)\n","        \n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_nb))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_nb = augment_outputs[original_num:]\n","        mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_nb + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_nb.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_nb.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print decomposed losses #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:151: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n","  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.151705026626587 2.1417846083641052 4.293489634990692\n","1: 2.1143149733543396 2.1046818494796753 4.218996822834015\n","2: 2.0555968284606934 2.0491944551467896 4.104791283607483\n","3: 1.9843795895576477 1.97834312915802 3.9627227187156677\n","4: 1.902700662612915 1.89450341463089 3.797204077243805\n","5: 1.8080593347549438 1.8077366352081299 3.6157959699630737\n","6: 1.7114468216896057 1.711703896522522 3.4231507182121277\n","7: 1.609506070613861 1.6163228154182434 3.2258288860321045\n","8: 1.4969298243522644 1.502932369709015 2.9998621940612793\n","9: 1.4033982753753662 1.4094926714897156 2.812890946865082\n","10: 1.2901833951473236 1.300627440214157 2.5908108353614807\n","11: 1.1774076223373413 1.1794850528240204 2.3568926751613617\n","12: 1.0592747032642365 1.083522468805313 2.1427971720695496\n","13: 1.0017963647842407 0.9821163713932037 1.9839127361774445\n","14: 0.8758089244365692 0.9105907380580902 1.7863996624946594\n","15: 0.8286861479282379 0.8219969868659973 1.6506831347942352\n","16: 0.66417096555233 0.7364691942930222 1.4006401598453522\n","17: 0.6768247336149216 0.669943705201149 1.3467684388160706\n","18: 0.6163428127765656 0.6283489465713501 1.2446917593479156\n","19: 0.5908210724592209 0.5880135595798492 1.1788346320390701\n","20: 0.6021949648857117 0.5867716670036316 1.1889666318893433\n","21: 0.5082885921001434 0.549818828701973 1.0581074208021164\n","22: 0.5455412566661835 0.5052780210971832 1.0508192777633667\n","23: 0.5258339047431946 0.47421327233314514 1.0000471770763397\n","24: 0.4584994465112686 0.46762068569660187 0.9261201322078705\n","25: 0.4619837775826454 0.4521792083978653 0.9141629859805107\n","26: 0.4420734941959381 0.4329097867012024 0.8749832808971405\n","27: 0.47137682139873505 0.4114468991756439 0.882823720574379\n","28: 0.42933256924152374 0.404775895178318 0.8341084644198418\n","29: 0.4693622589111328 0.38451091200113297 0.8538731709122658\n","30: 0.4231744259595871 0.38227757811546326 0.8054520040750504\n","31: 0.41212400794029236 0.36038388311862946 0.7725078910589218\n","32: 0.4014369025826454 0.34960325062274933 0.7510401532053947\n","33: 0.411501482129097 0.354227676987648 0.765729159116745\n","34: 0.4573569893836975 0.3662244454026222 0.8235814347863197\n","35: 0.39954058825969696 0.33859603852033615 0.7381366267800331\n","36: 0.3838348686695099 0.3481330871582031 0.731967955827713\n","37: 0.3776310756802559 0.3180180788040161 0.695649154484272\n","38: 0.45089899003505707 0.3325292840600014 0.7834282740950584\n","39: 0.3607853055000305 0.32522810250520706 0.6860134080052376\n","40: 0.4194464460015297 0.30846749246120453 0.7279139384627342\n","41: 0.36793866008520126 0.30953560024499893 0.6774742603302002\n","42: 0.3281220272183418 0.34198812395334244 0.6701101511716843\n","43: 0.32090675085783005 0.30807915329933167 0.6289859041571617\n","44: 0.38300691545009613 0.3257121592760086 0.7087190747261047\n","45: 0.3680586516857147 0.30939310789108276 0.6774517595767975\n","46: 0.3065735250711441 0.2999562472105026 0.6065297722816467\n","47: 0.420557864010334 0.3241486921906471 0.7447065562009811\n","48: 0.2931559793651104 0.2958488017320633 0.5890047810971737\n","49: 0.36033741384744644 0.2890050709247589 0.6493424847722054\n","50: 0.3167740926146507 0.2970307767391205 0.6138048693537712\n","51: 0.4182209223508835 0.2813296392560005 0.699550561606884\n","52: 0.40929313004016876 0.2987784594297409 0.7080715894699097\n","53: 0.29576611518859863 0.27585288882255554 0.5716190040111542\n","54: 0.32964247465133667 0.2856208384037018 0.6152633130550385\n","55: 0.33567309379577637 0.28577301651239395 0.6214461103081703\n","56: 0.3753243163228035 0.2906474992632866 0.6659718155860901\n","57: 0.40226659178733826 0.2802742123603821 0.6825408041477203\n","58: 0.2852368578314781 0.2720535919070244 0.5572904497385025\n","59: 0.3270449936389923 0.2982628084719181 0.6253078021109104\n","60: 0.3460248187184334 0.2656637728214264 0.6116885915398598\n","61: 0.34076523035764694 0.27782346308231354 0.6185886934399605\n","62: 0.2452738806605339 0.2649945393204689 0.5102684199810028\n","63: 0.32879386097192764 0.2618187591433525 0.5906126201152802\n","64: 0.33360882103443146 0.25249527022242546 0.5861040912568569\n","65: 0.29222533851861954 0.26863009482622147 0.560855433344841\n","66: 0.2856478989124298 0.2651679664850235 0.5508158653974533\n","67: 0.37287189811468124 0.2730793356895447 0.6459512338042259\n","68: 0.43156807124614716 0.2554040029644966 0.6869720742106438\n","69: 0.30258793383836746 0.24689017236232758 0.549478106200695\n","70: 0.28716273605823517 0.2637270390987396 0.5508897751569748\n","71: 0.3991212621331215 0.2606743052601814 0.6597955673933029\n","72: 0.34466032683849335 0.25239499658346176 0.5970553234219551\n","73: 0.23737304657697678 0.24866695702075958 0.48604000359773636\n","74: 0.3713705390691757 0.25499552488327026 0.626366063952446\n","75: 0.24713841080665588 0.2537369281053543 0.5008753389120102\n","76: 0.3720925599336624 0.2460913583636284 0.6181839182972908\n","77: 0.3040098696947098 0.24944546818733215 0.5534553378820419\n","78: 0.2996695190668106 0.2539891041815281 0.5536586232483387\n","79: 0.375854529440403 0.2730192318558693 0.6488737612962723\n","80: 0.3639763966202736 0.23608604818582535 0.6000624448060989\n","81: 0.29069581627845764 0.23874889314174652 0.5294447094202042\n","82: 0.3602772727608681 0.25198812782764435 0.6122654005885124\n","83: 0.2798078991472721 0.23444894328713417 0.5142568424344063\n","84: 0.30584753677248955 0.2413744181394577 0.5472219549119473\n","85: 0.38164325058460236 0.24501997977495193 0.6266632303595543\n","86: 0.3431827612221241 0.25727858021855354 0.6004613414406776\n","87: 0.3097306117415428 0.25600722059607506 0.5657378323376179\n","88: 0.3127046898007393 0.24359260499477386 0.5562972947955132\n","89: 0.33716098219156265 0.23803041130304337 0.575191393494606\n","90: 0.27226514369249344 0.24108948558568954 0.513354629278183\n","91: 0.3048274852335453 0.25415703654289246 0.5589845217764378\n","92: 0.403476357460022 0.23986875265836716 0.6433451101183891\n","93: 0.33296700939536095 0.26336633414030075 0.5963333435356617\n","94: 0.24471187591552734 0.24240799993276596 0.4871198758482933\n","95: 0.40374527871608734 0.2566799074411392 0.6604251861572266\n","96: 0.26608244702219963 0.22385786846280098 0.4899403154850006\n","97: 0.30876893177628517 0.22405746579170227 0.5328263975679874\n","98: 0.27700721472501755 0.22354526072740555 0.5005524754524231\n","99: 0.31805460155010223 0.23764237016439438 0.5556969717144966\n","100: 0.4064345732331276 0.23227903991937637 0.638713613152504\n","101: 0.2786499671638012 0.22670166939496994 0.5053516365587711\n","102: 0.2868097126483917 0.2335026040673256 0.5203123167157173\n","103: 0.31474367529153824 0.2377709150314331 0.5525145903229713\n","104: 0.2648710496723652 0.2524952292442322 0.5173662789165974\n","105: 0.30162356048822403 0.23147395998239517 0.5330975204706192\n","106: 0.35356658697128296 0.23227549344301224 0.5858420804142952\n","107: 0.3851909711956978 0.22386321052908897 0.6090541817247868\n","108: 0.2922074794769287 0.23366952687501907 0.5258770063519478\n","109: 0.33377306908369064 0.22424691170454025 0.5580199807882309\n","110: 0.3476636242121458 0.2362818606197834 0.5839454848319292\n","111: 0.28882502764463425 0.21569256484508514 0.5045175924897194\n","112: 0.319193035364151 0.21981705352663994 0.5390100888907909\n","113: 0.36520887166261673 0.22056962549686432 0.585778497159481\n","114: 0.2543398104608059 0.22197029367089272 0.4763101041316986\n","115: 0.2575584203004837 0.21756795421242714 0.47512637451291084\n","116: 0.35456790030002594 0.23745791241526604 0.592025812715292\n","117: 0.3494659662246704 0.23339911550283432 0.5828650817275047\n","118: 0.40357743203639984 0.24550660699605942 0.6490840390324593\n","119: 0.2912404462695122 0.23071352392435074 0.5219539701938629\n","120: 0.31644147634506226 0.23467155173420906 0.5511130280792713\n","121: 0.30689694732427597 0.22372979670763016 0.5306267440319061\n","122: 0.31647051125764847 0.2417946457862854 0.5582651570439339\n","123: 0.3157387301325798 0.2107444740831852 0.526483204215765\n","124: 0.37301308661699295 0.22645573690533638 0.5994688235223293\n","125: 0.322841789573431 0.24440723657608032 0.5672490261495113\n","126: 0.2746109962463379 0.21331977099180222 0.4879307672381401\n","127: 0.19320937618613243 0.20999258011579514 0.40320195630192757\n","128: 0.26948754489421844 0.21841827034950256 0.487905815243721\n","129: 0.2763068713247776 0.21521279215812683 0.49151966348290443\n","130: 0.27769194543361664 0.20820392668247223 0.48589587211608887\n","131: 0.3168042302131653 0.220391396433115 0.5371956266462803\n","132: 0.3394731283187866 0.21015121042728424 0.5496243387460709\n","133: 0.3307642340660095 0.20550624653697014 0.5362704806029797\n","134: 0.30599629133939743 0.23168637230992317 0.5376826636493206\n","135: 0.29491837322711945 0.20749791711568832 0.5024162903428078\n","136: 0.25065939500927925 0.2332843840122223 0.48394377902150154\n","137: 0.33149461448192596 0.21562659740447998 0.547121211886406\n","138: 0.24293462559580803 0.2083650343120098 0.45129965990781784\n","139: 0.31965240836143494 0.2065979763865471 0.526250384747982\n","140: 0.2704683020710945 0.21093133464455605 0.48139963671565056\n","141: 0.3431285619735718 0.21900644153356552 0.5621350035071373\n","142: 0.3165673166513443 0.22947490215301514 0.5460422188043594\n","143: 0.27731291577219963 0.2061368264257908 0.4834497421979904\n","144: 0.30070842057466507 0.20075666531920433 0.5014650858938694\n","145: 0.24164902418851852 0.22169681265950203 0.46334583684802055\n","146: 0.2691897004842758 0.21737375855445862 0.48656345903873444\n","147: 0.315976370126009 0.22186243534088135 0.5378388054668903\n","148: 0.1622510850429535 0.22361435368657112 0.3858654387295246\n","149: 0.29380591958761215 0.21123171970248222 0.5050376392900944\n","150: 0.24206435307860374 0.19686764851212502 0.43893200159072876\n","151: 0.3584400862455368 0.21177828311920166 0.5702183693647385\n","152: 0.3290843367576599 0.21966461837291718 0.5487489551305771\n","153: 0.26049721986055374 0.208993811160326 0.46949103102087975\n","154: 0.3620755672454834 0.21414128690958023 0.5762168541550636\n","155: 0.3033592626452446 0.22017384320497513 0.5235331058502197\n","156: 0.35815560817718506 0.21026397496461868 0.5684195831418037\n","157: 0.3205714523792267 0.22539492696523666 0.5459663793444633\n","158: 0.315677709877491 0.22861192375421524 0.5442896336317062\n","159: 0.30195415765047073 0.2089436948299408 0.5108978524804115\n","160: 0.32413189858198166 0.225956778973341 0.5500886775553226\n","161: 0.35998403280973434 0.21742822974920273 0.5774122625589371\n","162: 0.40882378816604614 0.2043376788496971 0.6131614670157433\n","163: 0.3894129917025566 0.22964230924844742 0.619055300951004\n","164: 0.29791322723031044 0.22276843711733818 0.5206816643476486\n","165: 0.27328576892614365 0.20120638608932495 0.4744921550154686\n","166: 0.2873605266213417 0.2256547436118126 0.5130152702331543\n","167: 0.26058050990104675 0.2168363593518734 0.47741686925292015\n","168: 0.2996559292078018 0.20779233798384666 0.5074482671916485\n","169: 0.28369662910699844 0.20358461514115334 0.4872812442481518\n","170: 0.3811350390315056 0.20242633298039436 0.5835613720119\n","171: 0.2920614629983902 0.2053373046219349 0.4973987676203251\n","172: 0.23852728307247162 0.19822685420513153 0.43675413727760315\n","173: 0.29193586111068726 0.21129920333623886 0.5032350644469261\n","174: 0.32050686329603195 0.22145932167768478 0.5419661849737167\n","175: 0.38422268629074097 0.21699584275484085 0.6012185290455818\n","176: 0.34956344962120056 0.22576162591576576 0.5753250755369663\n","177: 0.25189533829689026 0.19876618310809135 0.4506615214049816\n","178: 0.33451705425977707 0.23073266074061394 0.565249715000391\n","179: 0.335570253431797 0.21587061136960983 0.5514408648014069\n","180: 0.24773652851581573 0.2130989246070385 0.46083545312285423\n","181: 0.35336759313941 0.20953164994716644 0.5628992430865765\n","182: 0.3257106989622116 0.21269629895687103 0.5384069979190826\n","183: 0.28181974962353706 0.20268529653549194 0.484505046159029\n","184: 0.2380831614136696 0.2012554556131363 0.4393386170268059\n","185: 0.325631745159626 0.21088802814483643 0.5365197733044624\n","186: 0.31942151114344597 0.20215480774641037 0.5215763188898563\n","187: 0.265081062912941 0.22096426412463188 0.48604532703757286\n","188: 0.22989217564463615 0.21316150948405266 0.4430536851286888\n","189: 0.25714293867349625 0.21083608269691467 0.4679790213704109\n","190: 0.36233431100845337 0.1994604654610157 0.5617947764694691\n","191: 0.31753886491060257 0.22006958723068237 0.5376084521412849\n","192: 0.2936772592365742 0.2064710073173046 0.5001482665538788\n","193: 0.3447637967765331 0.20684389397501945 0.5516076907515526\n","194: 0.41924434900283813 0.20361676812171936 0.6228611171245575\n","195: 0.32970473170280457 0.22745754942297935 0.5571622811257839\n","196: 0.2730960547924042 0.20101729035377502 0.4741133451461792\n","197: 0.35830046981573105 0.20748229697346687 0.5657827667891979\n","198: 0.3024524301290512 0.2029595710337162 0.5054120011627674\n","199: 0.2862897291779518 0.2082967832684517 0.4945865124464035\n","200: 0.24911367520689964 0.2273969016969204 0.47651057690382004\n","201: 0.31523922830820084 0.21685348451137543 0.5320927128195763\n","202: 0.33730392158031464 0.22022376954555511 0.5575276911258698\n","203: 0.36560288071632385 0.20552416518330574 0.5711270458996296\n","204: 0.2730182260274887 0.1970810331404209 0.4700992591679096\n","205: 0.2678854502737522 0.19876045733690262 0.46664590761065483\n","206: 0.2847691923379898 0.1993962749838829 0.4841654673218727\n","207: 0.29048317670822144 0.1992364451289177 0.48971962183713913\n","208: 0.40485717356204987 0.20825954899191856 0.6131167225539684\n","209: 0.29480912536382675 0.19594724103808403 0.4907563664019108\n","210: 0.32582278177142143 0.20255372673273087 0.5283765085041523\n","211: 0.3099312037229538 0.1970451921224594 0.5069763958454132\n","212: 0.29301026836037636 0.20013348385691643 0.4931437522172928\n","213: 0.2812884747982025 0.20532815903425217 0.4866166338324547\n","214: 0.26264213398098946 0.213911272585392 0.47655340656638145\n","215: 0.2924836501479149 0.20544788986444473 0.4979315400123596\n","216: 0.2832992449402809 0.2228889837861061 0.506188228726387\n","217: 0.263505294919014 0.19745661318302155 0.4609619081020355\n","218: 0.27961183339357376 0.22601797431707382 0.5056298077106476\n","219: 0.2734163627028465 0.20455482229590416 0.4779711849987507\n","220: 0.29187850654125214 0.21070174872875214 0.5025802552700043\n","221: 0.34789006412029266 0.22008919343352318 0.5679792575538158\n","222: 0.2721913270652294 0.19600771740078926 0.4681990444660187\n","223: 0.26041217893362045 0.20319592952728271 0.46360810846090317\n","224: 0.3080809563398361 0.2238384187221527 0.5319193750619888\n","225: 0.2554864101111889 0.19619732350111008 0.45168373361229897\n","226: 0.21892669796943665 0.2283791396766901 0.44730583764612675\n","227: 0.39677951484918594 0.2165566086769104 0.6133361235260963\n","228: 0.3228112682700157 0.20709963887929916 0.5299109071493149\n","229: 0.2885292023420334 0.21743258088827133 0.5059617832303047\n","230: 0.33624275773763657 0.2042011097073555 0.5404438674449921\n","231: 0.36572811007499695 0.22641418129205704 0.592142291367054\n","232: 0.28572552651166916 0.2115597501397133 0.49728527665138245\n","233: 0.35217082500457764 0.21563534438610077 0.5678061693906784\n","234: 0.2656353935599327 0.219231978058815 0.4848673716187477\n","235: 0.2918572537600994 0.19447268173098564 0.48632993549108505\n","236: 0.24208524450659752 0.20560194924473763 0.44768719375133514\n","237: 0.3394751250743866 0.20491310581564903 0.5443882308900356\n","238: 0.22433825954794884 0.19688938930630684 0.4212276488542557\n","239: 0.30457616224884987 0.22212237864732742 0.5266985408961773\n","240: 0.3970850259065628 0.2071515992283821 0.6042366251349449\n","241: 0.29728590697050095 0.2143978364765644 0.5116837434470654\n","242: 0.3032507970929146 0.2146463505923748 0.5178971476852894\n","243: 0.3852154314517975 0.19857381284236908 0.5837892442941666\n","244: 0.28456243872642517 0.20238758623600006 0.48695002496242523\n","245: 0.2559453882277012 0.20636119320988655 0.46230658143758774\n","246: 0.23889510333538055 0.21633850783109665 0.4552336111664772\n","247: 0.38161370903253555 0.20932496339082718 0.5909386724233627\n","248: 0.28041496127843857 0.20125430449843407 0.48166926577687263\n","249: 0.2632935680449009 0.21879196166992188 0.48208552971482277\n","250: 0.23850281909108162 0.2079787701368332 0.4464815892279148\n","251: 0.2736094668507576 0.20266729965806007 0.4762767665088177\n","252: 0.3256660979241133 0.19041218794882298 0.5160782858729362\n","253: 0.23391330987215042 0.19981643185019493 0.43372974172234535\n","254: 0.33587712049484253 0.19601256772875786 0.5318896882236004\n","255: 0.3487751856446266 0.19806192815303802 0.5468371137976646\n","256: 0.21242991089820862 0.20348187163472176 0.4159117825329304\n","257: 0.3306170478463173 0.21453143656253815 0.5451484844088554\n","258: 0.35430896282196045 0.20121224969625473 0.5555212125182152\n","259: 0.25097985193133354 0.19398879632353783 0.44496864825487137\n","260: 0.33253099769353867 0.2065846025943756 0.5391156002879143\n","261: 0.24849606677889824 0.21453160047531128 0.4630276672542095\n","262: 0.25092658028006554 0.1978388875722885 0.44876546785235405\n","263: 0.24968256801366806 0.2042824737727642 0.45396504178643227\n","264: 0.2654462903738022 0.20780190452933311 0.4732481949031353\n","265: 0.2672177031636238 0.20534361898899078 0.4725613221526146\n","266: 0.3068214878439903 0.20012765005230904 0.5069491378962994\n","267: 0.26675765588879585 0.2195008620619774 0.48625851795077324\n","268: 0.29541947692632675 0.2086603045463562 0.504079781472683\n","269: 0.3748873323202133 0.22024713084101677 0.5951344631612301\n","270: 0.3423531726002693 0.19660140946507454 0.5389545820653439\n","271: 0.3420129641890526 0.20594719052314758 0.5479601547122002\n","272: 0.27657948806881905 0.19578533247113228 0.4723648205399513\n","273: 0.3216736651957035 0.1913931407034397 0.5130668058991432\n","274: 0.31427326798439026 0.20699670538306236 0.5212699733674526\n","275: 0.23078229278326035 0.19628426060080528 0.42706655338406563\n","276: 0.3810776174068451 0.20262392982840538 0.5837015472352505\n","277: 0.29894573241472244 0.19708271324634552 0.49602844566106796\n","278: 0.2547309622168541 0.20914463698863983 0.4638755992054939\n","279: 0.3701961860060692 0.21775735169649124 0.5879535377025604\n","280: 0.2555212751030922 0.2058885134756565 0.4614097885787487\n","281: 0.34675542265176773 0.22438601776957512 0.5711414404213428\n","282: 0.37356434762477875 0.19637562707066536 0.5699399746954441\n","283: 0.2803836762905121 0.20116189494729042 0.4815455712378025\n","284: 0.2740815654397011 0.1984422653913498 0.4725238308310509\n","285: 0.3570740520954132 0.2300143465399742 0.5870883986353874\n","286: 0.3538079038262367 0.22322483360767365 0.5770327374339104\n","287: 0.23740152269601822 0.21732015907764435 0.45472168177366257\n","288: 0.223080612719059 0.21885790303349495 0.44193851575255394\n","289: 0.31497564166784286 0.20530781894922256 0.5202834606170654\n","290: 0.3966996446251869 0.2031554840505123 0.5998551286756992\n","291: 0.20644907280802727 0.19871951639652252 0.4051685892045498\n","292: 0.3490600436925888 0.19756048917770386 0.5466205328702927\n","293: 0.3111649863421917 0.20851223170757294 0.5196772180497646\n","294: 0.3867999538779259 0.21772387623786926 0.6045238301157951\n","295: 0.3182063102722168 0.22547882422804832 0.5436851345002651\n","296: 0.35816311836242676 0.2083224281668663 0.5664855465292931\n","297: 0.35178446769714355 0.19828422740101814 0.5500686950981617\n","298: 0.32272595912218094 0.2140587419271469 0.5367847010493279\n","299: 0.2904890924692154 0.1951519027352333 0.4856409952044487\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625274349836,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625274350202,"user_tz":-60,"elapsed":374,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"d7fab7cc-2faf-42a0-9a94-c519f9fa7d01"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9824561403508771\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625274350202,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"df506e2b-39d5-4396-ffba-c70b6dc9a9bc"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9853372434017595\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625274350203,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":12,"outputs":[]}]}
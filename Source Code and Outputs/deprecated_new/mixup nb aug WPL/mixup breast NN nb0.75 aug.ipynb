{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN nb0.75 aug.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625275091207,"user_tz":-60,"elapsed":27796,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"a55c3a16-c876-4853-e5c6-2f00b2f6bfe2"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625275092410,"user_tz":-60,"elapsed":1205,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625275092821,"user_tz":-60,"elapsed":413,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625275092821,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"65878865-837f-4a66-9f3e-1b9e5329ee4a"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","geometric_param = 0.75\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fcb82f64a90>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625275092821,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625275099274,"user_tz":-60,"elapsed":6456,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"f615ebc7-dbd3-4f35-f85a-8a01dcdb6d04"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1625275099274,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"id":"quiet-module","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1625275099275,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625275103165,"user_tz":-60,"elapsed":3894,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"568ada38-aaa2-4f4a-e660-843f242c10c8"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup with random neighbour perturbation #\n","        mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_breast_nb(inputs, labels, geometric_param, alpha)\n","        \n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_nb))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_nb = augment_outputs[original_num:]\n","        mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        augment_loss = mixup_loss_nb + loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_nb.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += augment_loss.item()\n","\n","        # Gradient Calculation & Optimisation #\n","        augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print decomposed losses #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:151: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n","  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.133987545967102 2.1302232146263123 4.2642107009887695\n","1: 2.068622052669525 2.066440224647522 4.135062217712402\n","2: 1.9667953252792358 1.9618099331855774 3.9286051988601685\n","3: 1.8301122188568115 1.832196593284607 3.6623088121414185\n","4: 1.670665442943573 1.6774832606315613 3.3481485843658447\n","5: 1.4947774708271027 1.5115978121757507 3.006375312805176\n","6: 1.3349603414535522 1.3353720903396606 2.670332431793213\n","7: 1.1636759042739868 1.1650200486183167 2.3286959528923035\n","8: 0.9676244258880615 0.973323404788971 1.9409477710723877\n","9: 0.8293415606021881 0.834194153547287 1.663535714149475\n","10: 0.6578018963336945 0.7205499708652496 1.3783518970012665\n","11: 0.6254058182239532 0.6166160851716995 1.2420218884944916\n","12: 0.5857246071100235 0.5475759357213974 1.133300542831421\n","13: 0.47356144338846207 0.512852355837822 0.986413836479187\n","14: 0.4869498610496521 0.44020264595746994 0.9271525144577026\n","15: 0.4293859452009201 0.4144541174173355 0.8438400626182556\n","16: 0.3602534383535385 0.3818529471755028 0.7421064078807831\n","17: 0.4343729391694069 0.36858464777469635 0.8029575943946838\n","18: 0.41168056428432465 0.35928376019001007 0.7709643244743347\n","19: 0.3195177838206291 0.3438214361667633 0.6633392125368118\n","20: 0.37248413264751434 0.32237105071544647 0.6948551833629608\n","21: 0.36153072863817215 0.3352895528078079 0.6968202739953995\n","22: 0.32963190972805023 0.2995622903108597 0.6291942149400711\n","23: 0.3626371771097183 0.3053259700536728 0.6679631471633911\n","24: 0.3349577635526657 0.30585262924432755 0.6408103853464127\n","25: 0.33762992918491364 0.31606442481279373 0.6536943465471268\n","26: 0.3373798206448555 0.29054079204797745 0.6279206052422523\n","27: 0.32460710406303406 0.2934761345386505 0.6180832386016846\n","28: 0.37437106668949127 0.3011259138584137 0.6754969954490662\n","29: 0.3356649801135063 0.2784847095608711 0.6141496896743774\n","30: 0.34264273941516876 0.29463789612054825 0.6372806429862976\n","31: 0.3608348146080971 0.2697652652859688 0.6306000724434853\n","32: 0.29683201760053635 0.2944813668727875 0.5913133770227432\n","33: 0.3280528783798218 0.2756374478340149 0.6036903411149979\n","34: 0.3115522190928459 0.2877958416938782 0.5993480533361435\n","35: 0.4506772793829441 0.31888484209775925 0.7695621103048325\n","36: 0.3409120738506317 0.27015846222639084 0.611070528626442\n","37: 0.2676408812403679 0.26806650310754776 0.5357073917984962\n","38: 0.3339540958404541 0.2793610021471977 0.6133150905370712\n","39: 0.35856372117996216 0.28196394070982933 0.640527680516243\n","40: 0.29629773646593094 0.2715565487742424 0.5678542852401733\n","41: 0.3894229084253311 0.29594605788588524 0.6853689700365067\n","42: 0.26087911427021027 0.28324444591999054 0.5441235601902008\n","43: 0.37644387781620026 0.2697100266814232 0.6461538970470428\n","44: 0.3138355016708374 0.25967736542224884 0.5735128670930862\n","45: 0.2686584144830704 0.2731024771928787 0.5417608916759491\n","46: 0.30999454855918884 0.2635980397462845 0.5735925883054733\n","47: 0.32875681668519974 0.2410077378153801 0.5697645545005798\n","48: 0.2619769126176834 0.25179604440927505 0.5137729570269585\n","49: 0.37767497450113297 0.2608656585216522 0.6385406255722046\n","50: 0.2902747541666031 0.24340243265032768 0.5336771756410599\n","51: 0.31814174354076385 0.2597583532333374 0.5779000967741013\n","52: 0.31901511549949646 0.24627966061234474 0.5652947723865509\n","53: 0.3435761332511902 0.25312676653265953 0.5967029184103012\n","54: 0.31620215997099876 0.26785237342119217 0.5840545296669006\n","55: 0.3350851386785507 0.23828968405723572 0.5733748078346252\n","56: 0.32315942645072937 0.23814257606863976 0.5613020062446594\n","57: 0.33568330854177475 0.24068600311875343 0.5763693004846573\n","58: 0.28513048589229584 0.2527380883693695 0.5378685742616653\n","59: 0.3275858759880066 0.25789058208465576 0.5854764580726624\n","60: 0.32837287336587906 0.25488119199872017 0.5832540541887283\n","61: 0.3735908269882202 0.2490837275981903 0.6226745545864105\n","62: 0.36386092007160187 0.24675139412283897 0.6106123179197311\n","63: 0.3431392312049866 0.24427515268325806 0.5874143689870834\n","64: 0.2636817842721939 0.24985986575484276 0.5135416388511658\n","65: 0.3240140974521637 0.24942754581570625 0.5734416544437408\n","66: 0.30002591013908386 0.25250640511512756 0.552532285451889\n","67: 0.4066428616642952 0.22779148817062378 0.6344343572854996\n","68: 0.252365805208683 0.23559115827083588 0.4879569709300995\n","69: 0.293275848031044 0.23625094443559647 0.5295267999172211\n","70: 0.3436289578676224 0.22793596982955933 0.5715649276971817\n","71: 0.32328571379184723 0.24808062613010406 0.5713663548231125\n","72: 0.31047946959733963 0.23089520633220673 0.541374683380127\n","73: 0.28543441742658615 0.2351808398962021 0.5206152498722076\n","74: 0.32491694763302803 0.21419414319097996 0.5391110703349113\n","75: 0.3017084449529648 0.24566596001386642 0.5473744049668312\n","76: 0.27427656203508377 0.2157716155052185 0.49004819244146347\n","77: 0.2906738743185997 0.21987029165029526 0.510544165968895\n","78: 0.2511499375104904 0.22912956029176712 0.48027949780225754\n","79: 0.2924603521823883 0.2166818603873253 0.5091422125697136\n","80: 0.26105499267578125 0.2091905828565359 0.4702455922961235\n","81: 0.3000147044658661 0.2290331944823265 0.529047891497612\n","82: 0.30787988007068634 0.20868488773703575 0.5165647566318512\n","83: 0.39914874359965324 0.21829523146152496 0.6174439787864685\n","84: 0.2433343008160591 0.23448782041668892 0.47782212495803833\n","85: 0.3059110715985298 0.22029834985733032 0.5262094289064407\n","86: 0.20891522616147995 0.21628618985414505 0.4252014085650444\n","87: 0.3002503402531147 0.20768756046891212 0.5079379007220268\n","88: 0.26255419850349426 0.2157536782324314 0.4783078730106354\n","89: 0.33229947090148926 0.21806105226278305 0.5503605306148529\n","90: 0.2678050398826599 0.20925386250019073 0.47705890238285065\n","91: 0.39878735691308975 0.2486354038119316 0.6474227458238602\n","92: 0.2492549568414688 0.21443487703800201 0.4636898413300514\n","93: 0.2477721218019724 0.2162603661417961 0.46403249353170395\n","94: 0.23666682094335556 0.19859079457819462 0.43525761365890503\n","95: 0.30043213814496994 0.19736792519688606 0.4978000521659851\n","96: 0.3427517041563988 0.20270628482103348 0.5454579591751099\n","97: 0.2555222138762474 0.19986405596137047 0.4553862661123276\n","98: 0.29270464926958084 0.20171860605478287 0.4944232627749443\n","99: 0.29801638051867485 0.217240571975708 0.5152569562196732\n","100: 0.32536888122558594 0.23146408051252365 0.5568329468369484\n","101: 0.23617197945713997 0.20380334556102753 0.4399753361940384\n","102: 0.2776455245912075 0.20973898470401764 0.48738451302051544\n","103: 0.23596778884530067 0.2023298516869545 0.43829765170812607\n","104: 0.2508773487061262 0.22554504126310349 0.47642239183187485\n","105: 0.2669771909713745 0.19188500568270683 0.45886218547821045\n","106: 0.31215257942676544 0.20567156374454498 0.5178241431713104\n","107: 0.24751490354537964 0.19555693864822388 0.4430718272924423\n","108: 0.33852051198482513 0.20827007293701172 0.546790599822998\n","109: 0.34519046545028687 0.19355712085962296 0.5387475788593292\n","110: 0.19459137320518494 0.19118934497237206 0.3857807219028473\n","111: 0.2259519323706627 0.18773601949214935 0.41368794441223145\n","112: 0.2860356643795967 0.19629378616809845 0.48232945799827576\n","113: 0.3544757068157196 0.1834898628294468 0.5379655659198761\n","114: 0.2492595613002777 0.18672725558280945 0.43598680943250656\n","115: 0.3026132732629776 0.197029709815979 0.4996429830789566\n","116: 0.278055552393198 0.189022246748209 0.4670778065919876\n","117: 0.2662270963191986 0.19503255188465118 0.4612596407532692\n","118: 0.3611867278814316 0.19461844116449356 0.5558051764965057\n","119: 0.29003289341926575 0.1813872940838337 0.47142018377780914\n","120: 0.261791355907917 0.19488411396741867 0.4566754698753357\n","121: 0.24423107877373695 0.19192542880773544 0.4361565113067627\n","122: 0.22566325962543488 0.20046374946832657 0.42612700909376144\n","123: 0.2588081359863281 0.19554144144058228 0.4543495774269104\n","124: 0.271183617413044 0.1797097586095333 0.450893372297287\n","125: 0.2618927266448736 0.17615053802728653 0.4380432516336441\n","126: 0.24774211645126343 0.17721391469240189 0.4249560236930847\n","127: 0.29049763828516006 0.2020292729139328 0.49252691864967346\n","128: 0.3012176901102066 0.1818879470229149 0.4831056296825409\n","129: 0.24241863191127777 0.19201042503118515 0.4344290494918823\n","130: 0.26270385831594467 0.19466149434447289 0.45736534893512726\n","131: 0.29962877929210663 0.1923610456287861 0.4919898211956024\n","132: 0.3083582669496536 0.17873800173401833 0.48709626495838165\n","133: 0.28593411296606064 0.1856864131987095 0.4716205298900604\n","134: 0.3149898946285248 0.17558305338025093 0.4905729442834854\n","135: 0.285706102848053 0.18305465206503868 0.46876074373722076\n","136: 0.2771289050579071 0.18795368447899818 0.465082585811615\n","137: 0.3213527128100395 0.1998199187219143 0.5211726352572441\n","138: 0.31422414630651474 0.1771203950047493 0.49134454131126404\n","139: 0.24145403690636158 0.17131290957331657 0.4127669408917427\n","140: 0.2897646129131317 0.18139049410820007 0.4711551144719124\n","141: 0.32798219472169876 0.18510928750038147 0.5130914896726608\n","142: 0.3048481419682503 0.1840198077261448 0.48886794596910477\n","143: 0.17447520419955254 0.1716032400727272 0.34607844054698944\n","144: 0.2993573769927025 0.1879408359527588 0.48729822039604187\n","145: 0.2288958840072155 0.17406395822763443 0.4029598534107208\n","146: 0.29575830698013306 0.17157039418816566 0.4673286974430084\n","147: 0.24926960095763206 0.185149434953928 0.43441903591156006\n","148: 0.22253803350031376 0.1677825003862381 0.3903205320239067\n","149: 0.27863559126853943 0.1624993346631527 0.4411349296569824\n","150: 0.28551237285137177 0.17792968079447746 0.46344204992055893\n","151: 0.264511376619339 0.16913043335080147 0.43364180624485016\n","152: 0.3433377668261528 0.1681075543165207 0.5114453136920929\n","153: 0.3118708059191704 0.16163154505193233 0.47350235283374786\n","154: 0.30466244369745255 0.16726186498999596 0.4719243198633194\n","155: 0.23725196719169617 0.17678477615118027 0.41403674334287643\n","156: 0.2925352528691292 0.17273465543985367 0.46526992321014404\n","157: 0.2273399904370308 0.17101570218801498 0.3983556777238846\n","158: 0.29208917915821075 0.19258801639080048 0.4846772029995918\n","159: 0.21288961824029684 0.15791777335107327 0.37080738320946693\n","160: 0.26810459792613983 0.19205273315310478 0.4601573199033737\n","161: 0.30201300233602524 0.1755557432770729 0.47756874561309814\n","162: 0.32991430163383484 0.16636762768030167 0.4962819218635559\n","163: 0.21989858895540237 0.1651381030678749 0.3850366845726967\n","164: 0.348128367215395 0.18545439839363098 0.5335827618837357\n","165: 0.255695216357708 0.17499465122818947 0.43068987131118774\n","166: 0.3191157579421997 0.1775772050023079 0.496692955493927\n","167: 0.1808016337454319 0.17731373384594917 0.3581153601408005\n","168: 0.29811759293079376 0.1602226160466671 0.45834020525217056\n","169: 0.3355124890804291 0.17055747658014297 0.5060699582099915\n","170: 0.19922829419374466 0.17726704478263855 0.3764953315258026\n","171: 0.27082551270723343 0.18067584559321404 0.45150136947631836\n","172: 0.2549446001648903 0.17683352530002594 0.4317781329154968\n","173: 0.26949387043714523 0.18644288182258606 0.4559367522597313\n","174: 0.29222825169563293 0.16263524442911148 0.454863503575325\n","175: 0.2721531465649605 0.1633395366370678 0.435492679476738\n","176: 0.27500494569540024 0.1855832375586033 0.46058816835284233\n","177: 0.27649548649787903 0.16542434692382812 0.44191983342170715\n","178: 0.39790041744709015 0.17161361128091812 0.5695140361785889\n","179: 0.28945621848106384 0.17120397090911865 0.4606602042913437\n","180: 0.2355775609612465 0.17623600363731384 0.41181356459856033\n","181: 0.2548530772328377 0.18033726140856743 0.4351903349161148\n","182: 0.2200431115925312 0.15819205902516842 0.3782351687550545\n","183: 0.28966058418154716 0.1789950206875801 0.468655601143837\n","184: 0.193496473133564 0.16262584924697876 0.35612232983112335\n","185: 0.32926303148269653 0.16181299835443497 0.4910760372877121\n","186: 0.3126136511564255 0.16255158931016922 0.4751652330160141\n","187: 0.2658405527472496 0.16648174822330475 0.43232230097055435\n","188: 0.2116592936217785 0.17108982801437378 0.38274911046028137\n","189: 0.24313747882843018 0.15934310108423233 0.4024805799126625\n","190: 0.3107196167111397 0.15892098285257816 0.4696406051516533\n","191: 0.2687194012105465 0.15925651788711548 0.42797592282295227\n","192: 0.34100786596536636 0.18437344580888748 0.525381326675415\n","193: 0.2839043438434601 0.16764449700713158 0.45154883712530136\n","194: 0.3686508387327194 0.16588745266199112 0.5345382839441299\n","195: 0.2729645371437073 0.1770736500620842 0.4500381797552109\n","196: 0.258149404078722 0.1581791453063488 0.4163285493850708\n","197: 0.27773619070649147 0.16127916052937508 0.43901534378528595\n","198: 0.2292059287428856 0.16336051374673843 0.3925664350390434\n","199: 0.2322636917233467 0.17129024118185043 0.40355394035577774\n","200: 0.25470706075429916 0.1641477830708027 0.41885483264923096\n","201: 0.25091903656721115 0.15988300554454327 0.41080204397439957\n","202: 0.22248538583517075 0.15823540464043617 0.3807207942008972\n","203: 0.2574562653899193 0.1615426316857338 0.4189989045262337\n","204: 0.27429547905921936 0.18215908855199814 0.4564545676112175\n","205: 0.26550690457224846 0.16121724620461464 0.4267241433262825\n","206: 0.29709598422050476 0.1670827530324459 0.4641787260770798\n","207: 0.23899098485708237 0.17457552626729012 0.4135665148496628\n","208: 0.33007851615548134 0.18762538582086563 0.5177039057016373\n","209: 0.20323050767183304 0.1562525425106287 0.3594830483198166\n","210: 0.321255162358284 0.1748015619814396 0.4960567355155945\n","211: 0.19415638782083988 0.1705102063715458 0.3646665960550308\n","212: 0.2398729771375656 0.1720564179122448 0.4119293987751007\n","213: 0.19438716024160385 0.17139505222439766 0.3657822050154209\n","214: 0.24650178104639053 0.16475947201251984 0.41126125305891037\n","215: 0.26078348234295845 0.17571686208248138 0.43650034070014954\n","216: 0.24634234234690666 0.15754534304141998 0.40388768166303635\n","217: 0.3088577538728714 0.19095832481980324 0.49981607496738434\n","218: 0.269294660538435 0.1761840358376503 0.445478692650795\n","219: 0.1777186542749405 0.15769352950155735 0.3354121819138527\n","220: 0.28413714468479156 0.1642942577600479 0.4484313875436783\n","221: 0.3176184557378292 0.16796198301017284 0.4855804368853569\n","222: 0.2918054535984993 0.1735677495598793 0.4653732106089592\n","223: 0.24325556680560112 0.16215655952692032 0.40541212260723114\n","224: 0.2304847501218319 0.1552082933485508 0.385693047195673\n","225: 0.19575756415724754 0.1889919638633728 0.38474953919649124\n","226: 0.27365202456712723 0.16748344153165817 0.4411354660987854\n","227: 0.2826457843184471 0.1693565510213375 0.4520023241639137\n","228: 0.3093486428260803 0.1658390313386917 0.4751876890659332\n","229: 0.31593745946884155 0.15967054665088654 0.4756079912185669\n","230: 0.31698159873485565 0.16615401208400726 0.4831356108188629\n","231: 0.2730638161301613 0.16411114484071732 0.4371749609708786\n","232: 0.23281846195459366 0.15630937926471233 0.38912784308195114\n","233: 0.32428906857967377 0.16081663966178894 0.4851057156920433\n","234: 0.2082003653049469 0.15942280367016792 0.3676231577992439\n","235: 0.23084521293640137 0.16217700019478798 0.39302220940589905\n","236: 0.24529996886849403 0.15525596402585506 0.40055593848228455\n","237: 0.2925203666090965 0.16939303278923035 0.46191340684890747\n","238: 0.245235837996006 0.16183578968048096 0.40707163512706757\n","239: 0.25058548152446747 0.1659746989607811 0.41656018048524857\n","240: 0.31269439309835434 0.1772702857851982 0.48996467888355255\n","241: 0.2771766558289528 0.16798461601138115 0.44516126811504364\n","242: 0.2920157462358475 0.15916719660162926 0.45118294656276703\n","243: 0.29013150185346603 0.1806303821504116 0.47076189517974854\n","244: 0.28874479979276657 0.16688277572393417 0.45562756061553955\n","245: 0.2159503549337387 0.17130206152796745 0.38725241273641586\n","246: 0.21316265687346458 0.1571216732263565 0.3702843338251114\n","247: 0.28673143684864044 0.1701701544225216 0.45690159499645233\n","248: 0.24911393225193024 0.1588660441339016 0.40797998011112213\n","249: 0.23899704962968826 0.1633143350481987 0.40231136977672577\n","250: 0.2869488298892975 0.17063629999756813 0.4575851410627365\n","251: 0.25594961643218994 0.16739188507199287 0.4233414977788925\n","252: 0.3502970486879349 0.16504840552806854 0.5153454542160034\n","253: 0.19658955931663513 0.16022687405347824 0.35681644082069397\n","254: 0.3487570062279701 0.1584211364388466 0.5071781277656555\n","255: 0.2942141517996788 0.16170205362141132 0.455916203558445\n","256: 0.2665570806711912 0.1553216241300106 0.42187870293855667\n","257: 0.3047107458114624 0.15726615116000175 0.46197690069675446\n","258: 0.3124559670686722 0.1560897808521986 0.46854573488235474\n","259: 0.19539957121014595 0.1670094132423401 0.36240898072719574\n","260: 0.24705282226204872 0.1695798747241497 0.416632704436779\n","261: 0.20252976939082146 0.15771517530083656 0.360244944691658\n","262: 0.2830008864402771 0.17475119978189468 0.4577520862221718\n","263: 0.27285923436284065 0.17539867013692856 0.4482579082250595\n","264: 0.20022964477539062 0.15355320647358894 0.35378285124897957\n","265: 0.2104458138346672 0.15694626793265343 0.36739208549261093\n","266: 0.22457429021596909 0.15982213243842125 0.38439641892910004\n","267: 0.2427619770169258 0.1673373207449913 0.4100993126630783\n","268: 0.2690418213605881 0.1700575165450573 0.4390993267297745\n","269: 0.2519962713122368 0.16453969106078148 0.41653595864772797\n","270: 0.33185143768787384 0.16677913069725037 0.498630553483963\n","271: 0.342778829857707 0.16879157349467278 0.5115704014897346\n","272: 0.2537459544837475 0.15817965939641 0.4119256064295769\n","273: 0.2548479996621609 0.16309452429413795 0.41794252395629883\n","274: 0.29773739725351334 0.16591498255729675 0.4636523723602295\n","275: 0.26747679710388184 0.16858968883752823 0.43606647849082947\n","276: 0.27517465502023697 0.18266009725630283 0.45783475786447525\n","277: 0.25506947934627533 0.1743044313043356 0.42937391996383667\n","278: 0.3101835288107395 0.168311208486557 0.4784947484731674\n","279: 0.3078938499093056 0.15544432401657104 0.4633381888270378\n","280: 0.20390946045517921 0.1546678226441145 0.35857728123664856\n","281: 0.21275365352630615 0.15243322774767876 0.3651868887245655\n","282: 0.29053274542093277 0.16211536154150963 0.4526481181383133\n","283: 0.28713662177324295 0.15678900480270386 0.4439256340265274\n","284: 0.26970957964658737 0.15744781494140625 0.427157387137413\n","285: 0.3241450246423483 0.16236374713480473 0.486508771777153\n","286: 0.3159681409597397 0.1664126180112362 0.482380747795105\n","287: 0.25479212403297424 0.16600139066576958 0.4207935184240341\n","288: 0.2615477293729782 0.17508666962385178 0.43663439899683\n","289: 0.2849694713950157 0.16447601281106472 0.4494454860687256\n","290: 0.36776936054229736 0.15807608142495155 0.5258454531431198\n","291: 0.21156268194317818 0.15277530625462532 0.3643379807472229\n","292: 0.42905326932668686 0.175011545419693 0.6040648221969604\n","293: 0.23570913076400757 0.18280118331313133 0.418510302901268\n","294: 0.2561902180314064 0.1615053527057171 0.4176955670118332\n","295: 0.223972886800766 0.1612498164176941 0.3852227032184601\n","296: 0.2541118338704109 0.16083303838968277 0.4149448722600937\n","297: 0.3494158163666725 0.1774783879518509 0.526894211769104\n","298: 0.2887317892163992 0.16684622131288052 0.4555780105292797\n","299: 0.26472290232777596 0.1725102663040161 0.43723317608237267\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625275103166,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625275103166,"user_tz":-60,"elapsed":11,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"aaeefaab-1720-4e1d-ccda-33a550a133e3"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9956140350877193\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625275103167,"user_tz":-60,"elapsed":7,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"ca442e55-1e39-49b8-8cc9-bab1500cdac6"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9853372434017595\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625275103167,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":12,"outputs":[]}]}
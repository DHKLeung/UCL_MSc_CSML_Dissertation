{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN nb0.5 aug.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625274770297,"user_tz":-60,"elapsed":21187,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"a3a1b393-9ceb-4379-b702-eeed86c81634"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625274771063,"user_tz":-60,"elapsed":768,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625274771549,"user_tz":-60,"elapsed":488,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625274771550,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"b8540706-44da-4680-ef51-6644885d3481"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","geometric_param = 0.5\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f68a73d4a50>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625274771550,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625274777873,"user_tz":-60,"elapsed":6326,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"85dc2605-ae7f-4449-8624-00710e8dcbb5"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1625274777874,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_nb(inputs, labels, geometric_param, alpha):\n","    inner_batch_size = labels.size(0)\n","    inputs_flatten = inputs.reshape(inner_batch_size, -1)\n","\n","    # Compute pair-wise distances & sort the distances #\n","    dists = torch.cdist(inputs_flatten, inputs_flatten)\n","    sort_idx = torch.argsort(dists, dim=1)\n","    sort_idx_no_itself = sort_idx[:, 1:]\n","\n","    # Generate geometric random variables for selecting neighbours & get the index of selected neighbour data #\n","    select_idx = torch.distributions.geometric.Geometric(geometric_param).sample_n(inner_batch_size).type(torch.LongTensor).to('cuda')\n","    select_idx_clipped = torch.clamp(select_idx, max=inner_batch_size - 2)\n","    nb_idx = sort_idx_no_itself[torch.arange(inner_batch_size), select_idx_clipped]\n","\n","    # mixup with neighbours #\n","    inputs_nb = inputs[nb_idx]\n","    labels_nb = labels[nb_idx]\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_nb = lmbda * inputs + (1 - lmbda) * inputs_nb\n","    return mixup_inputs_nb, labels, labels_nb, lmbda"],"id":"quiet-module","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1625274777874,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625274781716,"user_tz":-60,"elapsed":3845,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"da947fbc-aae3-42ae-a349-0f5bc0ae5a97"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup with random neighbour perturbation #\n","        mixup_inputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda = mixup_breast_nb(inputs, labels, geometric_param, alpha)\n","        \n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_nb))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_nb = augment_outputs[original_num:]\n","        mixup_loss_nb = mixup_criterion(criterion, mixup_outputs_nb, mixup_labels_nb_a, mixup_labels_nb_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        augment_loss = mixup_loss_nb + loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_nb.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += augment_loss.item()\n","\n","        # Gradient Calculation & Optimisation #\n","        augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print decomposed losses #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:151: UserWarning: sample_n will be deprecated. Use .sample((n,)) instead\n","  warnings.warn('sample_n will be deprecated. Use .sample((n,)) instead', UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.131475806236267 2.1250818967819214 4.2565577030181885\n","1: 2.0665295720100403 2.060127317905426 4.126656770706177\n","2: 1.9522309303283691 1.947890818119049 3.9001216888427734\n","3: 1.8080973625183105 1.8079732060432434 3.616070508956909\n","4: 1.6546164155006409 1.6507559418678284 3.3053723573684692\n","5: 1.4597026705741882 1.4663020968437195 2.9260048270225525\n","6: 1.2873245775699615 1.2870024740695953 2.574327051639557\n","7: 1.065066546201706 1.0663768947124481 2.131443440914154\n","8: 0.8979346752166748 0.9026708006858826 1.8006054759025574\n","9: 0.7176916599273682 0.7423176020383835 1.460009217262268\n","10: 0.6464928984642029 0.6307202279567719 1.2772131264209747\n","11: 0.5005961358547211 0.535044714808464 1.035640835762024\n","12: 0.415591835975647 0.4583486020565033 0.8739404380321503\n","13: 0.39004214107990265 0.4067571386694908 0.7967992424964905\n","14: 0.4246383085846901 0.38351549208164215 0.8081537932157516\n","15: 0.4123803749680519 0.32596708089113235 0.7383474558591843\n","16: 0.3813716396689415 0.31058093905448914 0.6919525861740112\n","17: 0.34082914143800735 0.29171988368034363 0.6325490325689316\n","18: 0.2775542661547661 0.2619898319244385 0.5395441055297852\n","19: 0.2803874909877777 0.24602282047271729 0.526410311460495\n","20: 0.35086699575185776 0.23080020770430565 0.5816671997308731\n","21: 0.26123030483722687 0.2280479110777378 0.4892782121896744\n","22: 0.2861454598605633 0.23013875260949135 0.5162842199206352\n","23: 0.289570689201355 0.2180248461663723 0.5075955390930176\n","24: 0.31221330538392067 0.2248886413872242 0.5371019393205643\n","25: 0.2577206566929817 0.21235200017690659 0.4700726643204689\n","26: 0.2623785138130188 0.21127716079354286 0.47365567088127136\n","27: 0.22313037514686584 0.20153697207570076 0.4246673434972763\n","28: 0.2714175581932068 0.21248716115951538 0.48390473425388336\n","29: 0.33917806297540665 0.21660540997982025 0.5557834804058075\n","30: 0.2362566813826561 0.21045437827706337 0.44671106338500977\n","31: 0.3866482637822628 0.2031715326011181 0.5898198038339615\n","32: 0.2749739959836006 0.2045983374118805 0.4795723408460617\n","33: 0.25750725343823433 0.18289203569293022 0.44039928168058395\n","34: 0.21870607882738113 0.17243040446192026 0.3911364898085594\n","35: 0.2616477236151695 0.20257321745157242 0.46422094851732254\n","36: 0.2393704280257225 0.17615390196442604 0.41552433371543884\n","37: 0.21604307368397713 0.19341420009732246 0.4094572737812996\n","38: 0.27593525499105453 0.17798538878560066 0.4539206475019455\n","39: 0.22393450886011124 0.17254114151000977 0.396475650370121\n","40: 0.24871554970741272 0.19373884052038193 0.44245438277721405\n","41: 0.2919066473841667 0.20796386897563934 0.49987053126096725\n","42: 0.22115317359566689 0.16774510592222214 0.388898279517889\n","43: 0.25918471068143845 0.16587246395647526 0.42505718022584915\n","44: 0.25296304374933243 0.1707976721227169 0.42376071214675903\n","45: 0.23856817185878754 0.16799916326999664 0.4065673500299454\n","46: 0.24999311193823814 0.1625567078590393 0.41254982352256775\n","47: 0.25448034331202507 0.1623624563217163 0.4168428033590317\n","48: 0.2522040940821171 0.15976561605930328 0.41196971386671066\n","49: 0.2976578325033188 0.17854034900665283 0.4761981815099716\n","50: 0.21272380091249943 0.17405585199594498 0.38677964359521866\n","51: 0.2635887637734413 0.16227635741233826 0.425865113735199\n","52: 0.24802570044994354 0.17772207409143448 0.4257477819919586\n","53: 0.24956349283456802 0.16680464148521423 0.41636813059449196\n","54: 0.19902539625763893 0.16885646060109138 0.3678818494081497\n","55: 0.18501029536128044 0.1750607993453741 0.3600711077451706\n","56: 0.3086073026061058 0.17930717021226883 0.48791445791721344\n","57: 0.28395500779151917 0.15006056800484657 0.43401557952165604\n","58: 0.15736790746450424 0.17379982583224773 0.3311677426099777\n","59: 0.26187417656183243 0.1510612629354 0.41293543577194214\n","60: 0.22334467433393002 0.15126536041498184 0.3746100440621376\n","61: 0.25610387325286865 0.14649628847837448 0.4026001840829849\n","62: 0.27391398698091507 0.14837739802896976 0.4222913756966591\n","63: 0.24259483814239502 0.14899003319442272 0.3915848657488823\n","64: 0.22324838489294052 0.17120741307735443 0.39445580542087555\n","65: 0.23251038044691086 0.1522863209247589 0.3847966939210892\n","66: 0.23837261646986008 0.14880544692277908 0.38717806339263916\n","67: 0.2741641588509083 0.14701556973159313 0.42117973417043686\n","68: 0.24246728420257568 0.14430608972907066 0.38677337020635605\n","69: 0.22572727501392365 0.1484477035701275 0.37417498230934143\n","70: 0.2304992415010929 0.14844480715692043 0.3789440467953682\n","71: 0.24389050155878067 0.1449972540140152 0.38888775557279587\n","72: 0.21898614428937435 0.1488874889910221 0.3678736463189125\n","73: 0.22707032412290573 0.14420743472874165 0.37127775698900223\n","74: 0.26477302610874176 0.15330319479107857 0.41807620972394943\n","75: 0.195302689447999 0.14563514664769173 0.3409378379583359\n","76: 0.266271498054266 0.1402675975114107 0.40653909742832184\n","77: 0.24971003830432892 0.16080903261899948 0.4105190634727478\n","78: 0.22889839485287666 0.15097802691161633 0.37987641245126724\n","79: 0.31887656450271606 0.15425390377640724 0.473130464553833\n","80: 0.3191191405057907 0.1503639630973339 0.4694831073284149\n","81: 0.218988586217165 0.1503572165966034 0.3693457990884781\n","82: 0.3523673452436924 0.17049103789031506 0.5228583961725235\n","83: 0.2951722778379917 0.13918820768594742 0.43436048179864883\n","84: 0.27336716279387474 0.16546019911766052 0.43882738426327705\n","85: 0.2129667103290558 0.13539737556129694 0.34836408495903015\n","86: 0.13373399153351784 0.14478436671197414 0.27851835638284683\n","87: 0.2392168566584587 0.16020026803016663 0.39941713213920593\n","88: 0.17927082255482674 0.1712189670652151 0.3504897803068161\n","89: 0.24937883019447327 0.13690820708870888 0.38628703355789185\n","90: 0.2701232060790062 0.1612688973546028 0.431392103433609\n","91: 0.2890872210264206 0.14359484985470772 0.4326820746064186\n","92: 0.34231796115636826 0.13383325189352036 0.4761512130498886\n","93: 0.2392476424574852 0.134462371468544 0.3737100064754486\n","94: 0.24309703335165977 0.1386700626462698 0.3817670941352844\n","95: 0.21432769671082497 0.13635953702032566 0.3506872355937958\n","96: 0.2451290488243103 0.14725728146731853 0.3923863247036934\n","97: 0.22753246873617172 0.15550928004086018 0.38304175436496735\n","98: 0.21088071912527084 0.13238185457885265 0.34326257556676865\n","99: 0.20929625257849693 0.13161136768758297 0.34090761840343475\n","100: 0.28879475966095924 0.15368980541825294 0.4424845576286316\n","101: 0.19612638652324677 0.15584528632462025 0.35197167843580246\n","102: 0.17401185631752014 0.15324430167675018 0.3272561654448509\n","103: 0.17275432124733925 0.13210041634738445 0.30485473573207855\n","104: 0.14183361362665892 0.12858211807906628 0.2704157270491123\n","105: 0.3284168541431427 0.15157029405236244 0.47998715937137604\n","106: 0.19667568057775497 0.1336547303944826 0.33033041656017303\n","107: 0.26454104483127594 0.13521334901452065 0.3997544050216675\n","108: 0.24765313416719437 0.12875879928469658 0.37641194462776184\n","109: 0.25822845846414566 0.12685427069664001 0.3850827217102051\n","110: 0.15330454986542463 0.14273985475301743 0.29604439437389374\n","111: 0.23153545707464218 0.15074758417904377 0.3822830468416214\n","112: 0.260445486754179 0.12801851518452168 0.3884640038013458\n","113: 0.25309617072343826 0.12743678875267506 0.38053295761346817\n","114: 0.2307282406836748 0.13808972388505936 0.3688179701566696\n","115: 0.3557620570063591 0.14713991433382034 0.5029019713401794\n","116: 0.26485759392380714 0.157463563606143 0.4223211631178856\n","117: 0.17377155646681786 0.1316392570734024 0.30541080608963966\n","118: 0.2709903120994568 0.13700642623007298 0.4079967364668846\n","119: 0.20563387870788574 0.126693582162261 0.3323274552822113\n","120: 0.1447595003992319 0.1312179770320654 0.2759774848818779\n","121: 0.16053256392478943 0.132341168820858 0.29287372529506683\n","122: 0.13746747933328152 0.12677923031151295 0.26424670964479446\n","123: 0.16566428914666176 0.1314777210354805 0.29714201390743256\n","124: 0.24580969661474228 0.15340481325984 0.3992145210504532\n","125: 0.25548889115452766 0.13253743015229702 0.38802631199359894\n","126: 0.22387459129095078 0.13515005633234978 0.35902462899684906\n","127: 0.14621661230921745 0.15258479118347168 0.29880139976739883\n","128: 0.31544846296310425 0.1330520659685135 0.44850052893161774\n","129: 0.30223261564970016 0.12147572636604309 0.42370834201574326\n","130: 0.2154403030872345 0.1464299876242876 0.36187028884887695\n","131: 0.2533595748245716 0.12360141612589359 0.37696098536252975\n","132: 0.2394661232829094 0.12587861344218254 0.36534473299980164\n","133: 0.2557118386030197 0.14756568148732185 0.40327752009034157\n","134: 0.23745513707399368 0.12844247184693813 0.36589761078357697\n","135: 0.33390048891305923 0.12174015678465366 0.45564065873622894\n","136: 0.13223468139767647 0.12013779580593109 0.25237248092889786\n","137: 0.3183450549840927 0.12059502303600311 0.4389400780200958\n","138: 0.18500923924148083 0.14696606434881687 0.3319753035902977\n","139: 0.30843234062194824 0.12702398654073477 0.435456320643425\n","140: 0.23220845311880112 0.12298809178173542 0.355196550488472\n","141: 0.22117233648896217 0.128307880833745 0.3494802191853523\n","142: 0.2110058292746544 0.12027250602841377 0.33127833157777786\n","143: 0.21554410457611084 0.12116381991654634 0.3367079235613346\n","144: 0.23359157890081406 0.1426502875983715 0.37624187767505646\n","145: 0.18770389817655087 0.12094846554100513 0.3086523674428463\n","146: 0.15931851044297218 0.12005207687616348 0.27937059104442596\n","147: 0.23593320325016975 0.15148024819791317 0.3874134495854378\n","148: 0.14387955330312252 0.11856928560882807 0.26244883984327316\n","149: 0.2696526423096657 0.12005497328937054 0.38970761746168137\n","150: 0.2138427123427391 0.12010491639375687 0.33394762873649597\n","151: 0.16906771808862686 0.11946148611605167 0.2885291986167431\n","152: 0.23587267473340034 0.12495408393442631 0.3608267605304718\n","153: 0.2763042487204075 0.1219190564006567 0.39822329580783844\n","154: 0.2138235755264759 0.12265426479279995 0.3364778347313404\n","155: 0.29004039615392685 0.1159029146656394 0.4059433043003082\n","156: 0.29254306852817535 0.11985229700803757 0.4123953655362129\n","157: 0.23648465797305107 0.11955917440354824 0.35604383051395416\n","158: 0.18211429193615913 0.12152955308556557 0.3036438450217247\n","159: 0.2332456298172474 0.12319090403616428 0.3564365357160568\n","160: 0.17783451452851295 0.12233147211372852 0.30016598477959633\n","161: 0.20542854070663452 0.11774212308228016 0.32317066937685013\n","162: 0.21462657302618027 0.1445266604423523 0.35915324091911316\n","163: 0.2785186171531677 0.11828448250889778 0.3968030959367752\n","164: 0.18363892659544945 0.11719393730163574 0.3008328676223755\n","165: 0.2686949782073498 0.11907054297626019 0.3877655193209648\n","166: 0.2155422531068325 0.11915348470211029 0.3346957340836525\n","167: 0.17128561064600945 0.11808784119784832 0.2893734499812126\n","168: 0.2680435739457607 0.15409622713923454 0.42213980853557587\n","169: 0.3101847916841507 0.1436542021110654 0.4538389928638935\n","170: 0.22936773672699928 0.1267433762550354 0.356111116707325\n","171: 0.24136791564524174 0.15025907382369041 0.3916269838809967\n","172: 0.17790762148797512 0.1269400715827942 0.30484769493341446\n","173: 0.1880524829030037 0.14163138531148434 0.3296838626265526\n","174: 0.2944340780377388 0.11663285549730062 0.411066934466362\n","175: 0.22599875181913376 0.1216930914670229 0.3476918488740921\n","176: 0.17354056239128113 0.11928527429699898 0.2928258329629898\n","177: 0.21040286496281624 0.141940139234066 0.35234300047159195\n","178: 0.24331200867891312 0.12168584950268269 0.36499785631895065\n","179: 0.22275418043136597 0.12042373418807983 0.3431779034435749\n","180: 0.18696070089936256 0.13983801566064358 0.3267987072467804\n","181: 0.19458574801683426 0.1183380801230669 0.3129238188266754\n","182: 0.21659033745527267 0.14431332051753998 0.36090364679694176\n","183: 0.22638074681162834 0.1450875625014305 0.37146830186247826\n","184: 0.17193493619561195 0.11638510972261429 0.28832004591822624\n","185: 0.3066243380308151 0.12357213255017996 0.43019647151231766\n","186: 0.24277711659669876 0.1165973674505949 0.3593744859099388\n","187: 0.18351387605071068 0.11817530915141106 0.30168919265270233\n","188: 0.11467434279620647 0.1187574714422226 0.23343181610107422\n","189: 0.23558404296636581 0.11938033625483513 0.35496438294649124\n","190: 0.2880663275718689 0.11624686699360609 0.40431319922208786\n","191: 0.1817750185728073 0.11674684658646584 0.29852186888456345\n","192: 0.33991237729787827 0.12752142269164324 0.46743379905819893\n","193: 0.3060225173830986 0.11659405939280987 0.422616571187973\n","194: 0.3408844321966171 0.14047724194824696 0.48136167228221893\n","195: 0.24561559408903122 0.12423164770007133 0.36984725296497345\n","196: 0.219045989215374 0.11963123455643654 0.33867722749710083\n","197: 0.23960912972688675 0.13976397551596165 0.37937310338020325\n","198: 0.23299163579940796 0.11744824983179569 0.3504398912191391\n","199: 0.17581694945693016 0.1143292156048119 0.2901461571455002\n","200: 0.23948946222662926 0.11825777776539326 0.35774724185466766\n","201: 0.27657545171678066 0.11539584118872881 0.39197129383683205\n","202: 0.23535749316215515 0.14332923851907253 0.37868672609329224\n","203: 0.2075895145535469 0.12204238586127758 0.32963189482688904\n","204: 0.21650748327374458 0.11608708556741476 0.332594558596611\n","205: 0.2090858742594719 0.1438653375953436 0.35295122116804123\n","206: 0.21250585839152336 0.12426086701452732 0.33676673471927643\n","207: 0.24626868963241577 0.14562588930130005 0.3918945789337158\n","208: 0.33206604421138763 0.1398070640861988 0.47187311202287674\n","209: 0.24484491348266602 0.14354748465120792 0.3883924037218094\n","210: 0.18231108039617538 0.12459447607398033 0.3069055676460266\n","211: 0.1920711174607277 0.12223253399133682 0.3143036551773548\n","212: 0.20783749967813492 0.11936037242412567 0.3271978832781315\n","213: 0.18098978884518147 0.11486388463526964 0.2958536818623543\n","214: 0.18013763055205345 0.11564114037901163 0.2957787737250328\n","215: 0.21630077809095383 0.12293704319745302 0.3392378129065037\n","216: 0.21913940832018852 0.11769996955990791 0.33683936297893524\n","217: 0.20914790779352188 0.12737918831408024 0.33652709424495697\n","218: 0.15069418773055077 0.14160775765776634 0.2923019528388977\n","219: 0.18200367875397205 0.12319028750061989 0.3051939681172371\n","220: 0.22919001802802086 0.14483076706528664 0.3740207888185978\n","221: 0.29136016219854355 0.11878659948706627 0.4101467579603195\n","222: 0.19850554317235947 0.11995814181864262 0.31846369057893753\n","223: 0.2640104182064533 0.11874864622950554 0.38275905698537827\n","224: 0.30076368898153305 0.11994860507547855 0.42071229219436646\n","225: 0.14509323611855507 0.11501013953238726 0.26010336726903915\n","226: 0.24487419426441193 0.11430915631353855 0.3591833505779505\n","227: 0.25528189539909363 0.14281811192631721 0.39810001105070114\n","228: 0.25489164888858795 0.11718425713479519 0.3720759078860283\n","229: 0.19737817719578743 0.14090704824775457 0.3382852226495743\n","230: 0.28224748373031616 0.13850705325603485 0.420754536986351\n","231: 0.2277955710887909 0.11605596635490656 0.3438515439629555\n","232: 0.16080292314291 0.11733973026275635 0.27814264595508575\n","233: 0.35789328068494797 0.11949254013597965 0.47738581895828247\n","234: 0.20365138538181782 0.11717972531914711 0.3208311051130295\n","235: 0.15702080726623535 0.11565588880330324 0.272676695138216\n","236: 0.1682409793138504 0.11628120113164186 0.2845221720635891\n","237: 0.27571822702884674 0.12667452171444893 0.40239275246858597\n","238: 0.27875401824712753 0.12128237262368202 0.40003638714551926\n","239: 0.20752479881048203 0.11451196670532227 0.3220367655158043\n","240: 0.2293122634291649 0.11653178744018078 0.3458440601825714\n","241: 0.19381264597177505 0.1401167679578066 0.3339294195175171\n","242: 0.22129784151911736 0.11365921841934323 0.3349570594727993\n","243: 0.23380623012781143 0.12181561626493931 0.3556218594312668\n","244: 0.2520184610038996 0.11630422994494438 0.3683226928114891\n","245: 0.17964247427880764 0.11801690049469471 0.29765937104821205\n","246: 0.17275119200348854 0.11568414326757193 0.28843532502651215\n","247: 0.2473720982670784 0.14346891269087791 0.3908410146832466\n","248: 0.2100522294640541 0.13991461507976055 0.3499668389558792\n","249: 0.2980830445885658 0.11674600280821323 0.4148290529847145\n","250: 0.2306613028049469 0.1214108057320118 0.3520721085369587\n","251: 0.26366396993398666 0.1488204412162304 0.41248442232608795\n","252: 0.2041849046945572 0.14779173955321312 0.3519766554236412\n","253: 0.1861131265759468 0.12109282612800598 0.3072059601545334\n","254: 0.29714294523000717 0.12366050109267235 0.4208034574985504\n","255: 0.31978215277194977 0.11731651425361633 0.4370986670255661\n","256: 0.2753641903400421 0.13852222077548504 0.4138864129781723\n","257: 0.18221363797783852 0.1233014427125454 0.3055150918662548\n","258: 0.22194096446037292 0.11731638200581074 0.3392573446035385\n","259: 0.20924458280205727 0.11949244700372219 0.3287370279431343\n","260: 0.30692440271377563 0.11590429209172726 0.42282868921756744\n","261: 0.18190774880349636 0.12112114205956459 0.3030288815498352\n","262: 0.1874794028699398 0.14242042042315006 0.3298998177051544\n","263: 0.21294119395315647 0.11911744438111782 0.3320586495101452\n","264: 0.20908870548009872 0.14839105028659105 0.3574797585606575\n","265: 0.2222079634666443 0.11539515759795904 0.3376031145453453\n","266: 0.326265387237072 0.12150655873119831 0.44777194410562515\n","267: 0.08349446021020412 0.11821283027529716 0.20170728117227554\n","268: 0.16907107830047607 0.11795656196773052 0.28702763840556145\n","269: 0.18341094441711903 0.12151415087282658 0.3049251064658165\n","270: 0.2686290368437767 0.11608763039112091 0.384716659784317\n","271: 0.314160592854023 0.11558832228183746 0.42974891513586044\n","272: 0.18839489854872227 0.1170862689614296 0.3054811656475067\n","273: 0.2040959969162941 0.11824175901710987 0.3223377615213394\n","274: 0.2551315389573574 0.11726393923163414 0.37239547073841095\n","275: 0.1919681467115879 0.11797689273953438 0.3099450320005417\n","276: 0.3046969473361969 0.1414811797440052 0.446178138256073\n","277: 0.18433178216218948 0.11640145443379879 0.30073322355747223\n","278: 0.18462522700428963 0.14054174348711967 0.3251669593155384\n","279: 0.3243853598833084 0.1224560234695673 0.44684139639139175\n","280: 0.17971578799188137 0.11804450489580631 0.2977602928876877\n","281: 0.2606499344110489 0.1399601846933365 0.4006101116538048\n","282: 0.26274871081113815 0.12060216255486012 0.3833508640527725\n","283: 0.1624183990061283 0.11913207918405533 0.28155046701431274\n","284: 0.182627122849226 0.1165645606815815 0.2991916872560978\n","285: 0.26791615039110184 0.11719304881989956 0.38510920107364655\n","286: 0.19906923174858093 0.11996687017381191 0.319036103785038\n","287: 0.15099213272333145 0.11485877446830273 0.2658509090542793\n","288: 0.12454738840460777 0.11582279577851295 0.24037017673254013\n","289: 0.2772957645356655 0.14136876724660397 0.4186645373702049\n","290: 0.2901519015431404 0.1178010068833828 0.4079529121518135\n","291: 0.16514039784669876 0.1161887776106596 0.2813291624188423\n","292: 0.2824857272207737 0.11800701916217804 0.40049274265766144\n","293: 0.23811982572078705 0.11628718860447407 0.35440701246261597\n","294: 0.27939750254154205 0.11704281345009804 0.3964403122663498\n","295: 0.21157675981521606 0.1452940832823515 0.3568708375096321\n","296: 0.22142117097973824 0.1451700385659933 0.3665912002325058\n","297: 0.2859230116009712 0.1430893074721098 0.42901231348514557\n","298: 0.2348659224808216 0.13880838826298714 0.37367431074380875\n","299: 0.17840534448623657 0.12117435969412327 0.2995797023177147\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625274781716,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625274781717,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"8e036c04-38d7-408e-c398-56ccc28ffcad"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9692982456140351\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625274781717,"user_tz":-60,"elapsed":7,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"ee240115-ebb3-49a3-a2b8-9500cbe007fd"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9941348973607038\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625274781718,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":12,"outputs":[]}]}
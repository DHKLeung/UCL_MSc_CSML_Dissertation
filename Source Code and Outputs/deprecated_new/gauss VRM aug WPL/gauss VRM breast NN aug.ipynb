{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"gauss VRM breast NN aug.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mJeKpUtx-C0N"},"source":["# Import Libraries"],"id":"mJeKpUtx-C0N"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625021804839,"user_tz":-60,"elapsed":20897,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"e30a1697-efdf-4562-e138-e412c868bb34"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2UUHRNd_-NHP"},"source":["# Import outside code"],"id":"2UUHRNd_-NHP"},{"cell_type":"code","metadata":{"id":"SfjB--2P-O5B"},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"SfjB--2P-O5B","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cv5aqDFD-Oy5"},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"Cv5aqDFD-Oy5","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0ETUvlT-RxY"},"source":["# Configuration"],"id":"H0ETUvlT-RxY"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625021806033,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"488383b9-ab85-4819-8456-5d609f5313c8"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","gauss_vicinal_std = 0.25\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f3201deda70>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"vdzX23Qc-b-6"},"source":["# Data"],"id":"vdzX23Qc-b-6"},{"cell_type":"code","metadata":{"id":"compressed-schedule"},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EnoPL51W-kQG"},"source":["# Models, Loss, Optimiser"],"id":"EnoPL51W-kQG"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625021812522,"user_tz":-60,"elapsed":6212,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"de021e65-210a-41d5-c8be-4a71a95c6c71"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"E1KWzK_J-4EG"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"E1KWzK_J-4EG"},{"cell_type":"code","metadata":{"id":"protective-manual"},"source":["def gauss_vicinal(inputs, gauss_vicinal_std):\n","    inputs_gauss = torch.normal(inputs, gauss_vicinal_std)\n","    return inputs_gauss"],"id":"protective-manual","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-SmfAFOC_ILY"},"source":["# Training"],"id":"-SmfAFOC_ILY"},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625021814934,"user_tz":-60,"elapsed":2414,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"4bb25f01-f65b-4751-963a-ad0b02a3f29a"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_gauss_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Gauss Vicinal perturbation #\n","        inputs_gauss = gauss_vicinal(inputs, gauss_vicinal_std)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, inputs_gauss))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        outputs_gauss = augment_outputs[original_num:]\n","        loss_gauss = criterion(outputs_gauss, labels)\n","        loss = criterion(outputs, labels)\n","        augment_loss = loss_gauss + loss\n","\n","        # Record #\n","        epoch_gauss_loss += loss_gauss.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += augment_loss.item()\n","\n","        # Gradient Calculation & Optimisation #\n","        augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print decomposed loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_gauss_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.126177668571472 2.123421549797058 4.24959921836853\n","1: 2.057257294654846 2.056856095790863 4.1141133308410645\n","2: 1.9570080637931824 1.951980173587799 3.908988356590271\n","3: 1.826884925365448 1.824405014514923 3.6512898206710815\n","4: 1.669023871421814 1.662909209728241 3.33193302154541\n","5: 1.5023874044418335 1.4943897724151611 2.99677711725235\n","6: 1.3221362829208374 1.3129563331604004 2.635092616081238\n","7: 1.1267439126968384 1.1074715554714203 2.234215497970581\n","8: 0.9376955926418304 0.9276441633701324 1.865339756011963\n","9: 0.7874243855476379 0.7696166634559631 1.557041049003601\n","10: 0.6807000488042831 0.6680010408163071 1.3487010896205902\n","11: 0.5886551141738892 0.5691236406564713 1.1577787399291992\n","12: 0.5103987753391266 0.49631084501743317 1.0067095756530762\n","13: 0.44907912611961365 0.4582018554210663 0.9072809815406799\n","14: 0.41724584251642227 0.4014534130692482 0.8186992555856705\n","15: 0.40525778383016586 0.3705677166581154 0.7758255153894424\n","16: 0.38264600187540054 0.34216298162937164 0.7248089909553528\n","17: 0.3648549094796181 0.35234493762254715 0.7171998471021652\n","18: 0.33507291227579117 0.3109080046415329 0.6459809243679047\n","19: 0.3243417367339134 0.31947634369134903 0.6438180804252625\n","20: 0.3118297792971134 0.2836606502532959 0.595490425825119\n","21: 0.306277871131897 0.2736448869109154 0.5799227505922318\n","22: 0.30612415820360184 0.2794201001524925 0.5855442583560944\n","23: 0.2937081754207611 0.28488580137491226 0.5785939544439316\n","24: 0.30228250473737717 0.2693874165415764 0.5716699361801147\n","25: 0.3069291114807129 0.25921499729156494 0.5661441087722778\n","26: 0.3149833455681801 0.2821354269981384 0.5971187651157379\n","27: 0.274790957570076 0.24796117097139359 0.5227521359920502\n","28: 0.2622726038098335 0.2468845173716545 0.509157121181488\n","29: 0.28517839312553406 0.24340177327394485 0.5285801514983177\n","30: 0.297478511929512 0.24847160279750824 0.5459501147270203\n","31: 0.2504279911518097 0.24102181568741798 0.49144981801509857\n","32: 0.24898292124271393 0.2162201590836048 0.46520308032631874\n","33: 0.29933200776576996 0.2242778092622757 0.5236098170280457\n","34: 0.2456119917333126 0.22120797261595726 0.46681997925043106\n","35: 0.26904939115047455 0.25516246259212494 0.5242118537425995\n","36: 0.24834980815649033 0.2261584997177124 0.4745083153247833\n","37: 0.24411635100841522 0.2125968001782894 0.45671314746141434\n","38: 0.2662801593542099 0.2201056331396103 0.4863857999444008\n","39: 0.23861553147435188 0.2100973017513752 0.4487128332257271\n","40: 0.25954675674438477 0.2188190445303917 0.47836580127477646\n","41: 0.26880785822868347 0.2321651205420494 0.5009729638695717\n","42: 0.25618740171194077 0.2046661674976349 0.46085356920957565\n","43: 0.20882270112633705 0.1987205371260643 0.40754323452711105\n","44: 0.25399891659617424 0.23042204417288303 0.484420970082283\n","45: 0.21348823234438896 0.20280742272734642 0.416295662522316\n","46: 0.2544196657836437 0.22552253678441048 0.4799422025680542\n","47: 0.22888509184122086 0.2230130173265934 0.45189810916781425\n","48: 0.22509035468101501 0.2037576213479042 0.4288479685783386\n","49: 0.24406840652227402 0.19686134159564972 0.44092975556850433\n","50: 0.2633354291319847 0.20822368934750557 0.4715591073036194\n","51: 0.25646089017391205 0.1949557177722454 0.45141661167144775\n","52: 0.2348228618502617 0.19540324062108994 0.4302261024713516\n","53: 0.18250572308897972 0.19206741452217102 0.37457314133644104\n","54: 0.22233310714364052 0.19143198058009148 0.4137651026248932\n","55: 0.21728552877902985 0.1952119953930378 0.41249752044677734\n","56: 0.24764984846115112 0.18692706897854805 0.4345769137144089\n","57: 0.20002466440200806 0.19580735638737679 0.39583201706409454\n","58: 0.2800702154636383 0.21388636715710163 0.4939565658569336\n","59: 0.20159459859132767 0.1890687346458435 0.3906633257865906\n","60: 0.19911078736186028 0.18248500302433968 0.38159579038619995\n","61: 0.22060099244117737 0.18576752580702305 0.40636851638555527\n","62: 0.2288328856229782 0.20203742012381554 0.43087030947208405\n","63: 0.203750092536211 0.18073006719350815 0.38448016345500946\n","64: 0.26039162278175354 0.19577904418110847 0.4561706781387329\n","65: 0.21741248667240143 0.1861523725092411 0.4035648703575134\n","66: 0.22725150734186172 0.17602213472127914 0.40327364206314087\n","67: 0.24748307093977928 0.19119491055607796 0.43867798149585724\n","68: 0.19986391067504883 0.20626092329621315 0.4061248302459717\n","69: 0.20626583322882652 0.17743586376309395 0.38370169699192047\n","70: 0.22866694629192352 0.1961190365254879 0.4247859753668308\n","71: 0.23311477154493332 0.1911485493183136 0.4242633283138275\n","72: 0.24914557486772537 0.19778763502836227 0.44693321734666824\n","73: 0.2108394093811512 0.1720478069037199 0.38288721442222595\n","74: 0.19795549474656582 0.17974877916276455 0.37770427763462067\n","75: 0.24309629201889038 0.19604375585913658 0.43914004415273666\n","76: 0.22116751223802567 0.18806186318397522 0.4092293828725815\n","77: 0.20101552084088326 0.17699745669960976 0.3780129700899124\n","78: 0.21727712824940681 0.18174339085817337 0.3990205228328705\n","79: 0.1889824979007244 0.17473584413528442 0.36371835321187973\n","80: 0.18461500108242035 0.18618183210492134 0.3707968294620514\n","81: 0.20413019880652428 0.16540044359862804 0.36953064054250717\n","82: 0.2386547587811947 0.17216743156313896 0.41082219034433365\n","83: 0.20689771696925163 0.16768371686339378 0.3745814338326454\n","84: 0.18577665463089943 0.16831046901643276 0.35408712923526764\n","85: 0.21589631773531437 0.2171533703804016 0.433049701154232\n","86: 0.2319982796907425 0.18923070654273033 0.4212289899587631\n","87: 0.20813405141234398 0.17493858188390732 0.383072629570961\n","88: 0.19885386899113655 0.16726526990532875 0.3661191463470459\n","89: 0.23020408302545547 0.184532031416893 0.4147361218929291\n","90: 0.25780125707387924 0.18538350239396095 0.4431847631931305\n","91: 0.19357755407691002 0.17310351878404617 0.3666810691356659\n","92: 0.2229471243917942 0.1632792316377163 0.3862263560295105\n","93: 0.21753031015396118 0.17570916563272476 0.39323947578668594\n","94: 0.2298760712146759 0.18092044442892075 0.41079650819301605\n","95: 0.23481181263923645 0.18180663138628006 0.4166184440255165\n","96: 0.25186101347208023 0.18520600348711014 0.43706703186035156\n","97: 0.18865074962377548 0.16219027526676655 0.3508410304784775\n","98: 0.23129215463995934 0.16321141086518764 0.39450355619192123\n","99: 0.1868850477039814 0.17297494783997536 0.35985999554395676\n","100: 0.21215380728244781 0.16399627551436424 0.37615007907152176\n","101: 0.1983579285442829 0.1605606395751238 0.35891856998205185\n","102: 0.17470675706863403 0.17849130369722843 0.3531980589032173\n","103: 0.22241834923624992 0.19433950632810593 0.41675783693790436\n","104: 0.19254665076732635 0.18283838033676147 0.37538503110408783\n","105: 0.20611367374658585 0.16789142414927483 0.3740050941705704\n","106: 0.20942656695842743 0.17736325412988663 0.38678983598947525\n","107: 0.24931485205888748 0.1785880420356989 0.4279028996825218\n","108: 0.2524212673306465 0.18129351362586021 0.43371477723121643\n","109: 0.19012551754713058 0.16277335956692696 0.35289887338876724\n","110: 0.2410082295536995 0.16755570098757744 0.4085639417171478\n","111: 0.21816745027899742 0.15748800337314606 0.3756554424762726\n","112: 0.20901887863874435 0.15084118861705065 0.359860073775053\n","113: 0.18668578378856182 0.1616951785981655 0.3483809679746628\n","114: 0.1930827684700489 0.158611461520195 0.3516942262649536\n","115: 0.20331387966871262 0.1599329188466072 0.3632467985153198\n","116: 0.21764721348881721 0.15583301335573196 0.3734802380204201\n","117: 0.2096220999956131 0.16081299632787704 0.37043510377407074\n","118: 0.2229759246110916 0.16967582143843174 0.3926517516374588\n","119: 0.16843630746006966 0.16010833159089088 0.32854464650154114\n","120: 0.1856475993990898 0.165050707757473 0.3506983071565628\n","121: 0.21495264396071434 0.18001218885183334 0.3949648179113865\n","122: 0.2513938210904598 0.1597232036292553 0.4111170321702957\n","123: 0.20522194541990757 0.1567586548626423 0.361980602145195\n","124: 0.2183200605213642 0.17001691460609436 0.3883369714021683\n","125: 0.17244521714746952 0.15632092580199242 0.3287661448121071\n","126: 0.24259574711322784 0.17025316506624222 0.41284891217947006\n","127: 0.13485793955624104 0.15370073728263378 0.2885586768388748\n","128: 0.19829871132969856 0.15116355568170547 0.34946225211024284\n","129: 0.22400357201695442 0.18146586790680885 0.4054694399237633\n","130: 0.19996848702430725 0.17668486014008522 0.37665334716439247\n","131: 0.2219627257436514 0.15941227413713932 0.3813750147819519\n","132: 0.20610662922263145 0.15181567706167698 0.3579223081469536\n","133: 0.18609077855944633 0.1501685231924057 0.33625930920243263\n","134: 0.2080756425857544 0.16909980401396751 0.377175435423851\n","135: 0.21629192680120468 0.16805796697735786 0.38434989750385284\n","136: 0.20942619070410728 0.1489660944789648 0.35839228332042694\n","137: 0.20030668657273054 0.15871559083461761 0.35902228206396103\n","138: 0.24049397930502892 0.16905072703957558 0.4095447063446045\n","139: 0.1850726455450058 0.1520561296492815 0.33712876588106155\n","140: 0.16559521853923798 0.16010380163788795 0.32569901645183563\n","141: 0.20091969892382622 0.14487913995981216 0.3457988388836384\n","142: 0.2231334000825882 0.16882641799747944 0.3919598162174225\n","143: 0.23909377306699753 0.16463648527860641 0.40373025834560394\n","144: 0.23787682503461838 0.15149631723761559 0.38937315344810486\n","145: 0.1732967533171177 0.14404339902102947 0.3173401430249214\n","146: 0.1517016403377056 0.15189801156520844 0.30359964817762375\n","147: 0.1979477573186159 0.15120858326554298 0.34915633499622345\n","148: 0.17194462195038795 0.147130424156785 0.3190750554203987\n","149: 0.24219778552651405 0.14487630128860474 0.3870740830898285\n","150: 0.2633580081164837 0.14517995715141296 0.40853795781731606\n","151: 0.19418806955218315 0.15999099239706993 0.3541790544986725\n","152: 0.2153998240828514 0.14423738792538643 0.35963721573352814\n","153: 0.1795930601656437 0.1418805867433548 0.3214736469089985\n","154: 0.2163935974240303 0.15376375243067741 0.3701573312282562\n","155: 0.21227542124688625 0.1488506905734539 0.3611261174082756\n","156: 0.22255302965641022 0.16578207537531853 0.38833510503172874\n","157: 0.23730231821537018 0.17355291359126568 0.4108552411198616\n","158: 0.19095158576965332 0.14333814196288586 0.3342897295951843\n","159: 0.18236274644732475 0.15631548315286636 0.338678240776062\n","160: 0.18022547662258148 0.14805841445922852 0.3282838985323906\n","161: 0.23088686168193817 0.14671532437205315 0.3776021748781204\n","162: 0.19735579192638397 0.17436613515019417 0.37172193080186844\n","163: 0.1462518312036991 0.16490535624325275 0.3111571967601776\n","164: 0.20999746210873127 0.1394514385610819 0.34944889694452286\n","165: 0.1784732174128294 0.14063468482345343 0.31910790130496025\n","166: 0.14665302447974682 0.14341073296964169 0.2900637686252594\n","167: 0.2249481901526451 0.14500165544450283 0.3699498400092125\n","168: 0.1983913891017437 0.17477593943476677 0.37316733598709106\n","169: 0.19010750576853752 0.14302186109125614 0.3331293612718582\n","170: 0.272394098341465 0.1673961915075779 0.4397902935743332\n","171: 0.1992669701576233 0.15125339105725288 0.3505203649401665\n","172: 0.25810443609952927 0.166391272097826 0.42449571192264557\n","173: 0.19508953765034676 0.14754320681095123 0.3426327407360077\n","174: 0.1700047329068184 0.14623863250017166 0.31624337285757065\n","175: 0.2363915741443634 0.15691650286316872 0.3933080732822418\n","176: 0.2272881604731083 0.16225724294781685 0.38954539597034454\n","177: 0.22364720329642296 0.14788514375686646 0.3715323433279991\n","178: 0.21647847443819046 0.1619945392012596 0.3784730136394501\n","179: 0.14961851201951504 0.13968903105705976 0.2893075533211231\n","180: 0.21032002568244934 0.14184452034533024 0.35216453671455383\n","181: 0.20475141517817974 0.14651447348296642 0.35126588493585587\n","182: 0.25433388352394104 0.16903473436832428 0.4233686178922653\n","183: 0.2092788703739643 0.14960692822933197 0.358885794878006\n","184: 0.19112133607268333 0.15123969689011574 0.3423610180616379\n","185: 0.2136947140097618 0.16397996991872787 0.3776746690273285\n","186: 0.15801743790507317 0.1465103030204773 0.30452774465084076\n","187: 0.20382831059396267 0.14395646005868912 0.34778477996587753\n","188: 0.26497915759682655 0.16018034145236015 0.4251594990491867\n","189: 0.20692534931004047 0.17642848566174507 0.3833538331091404\n","190: 0.2123744934797287 0.1450928896665573 0.3574673905968666\n","191: 0.19447517022490501 0.1435341238975525 0.3380092978477478\n","192: 0.18278067093342543 0.13972459267824888 0.3225052710622549\n","193: 0.13951496221125126 0.14396541379392147 0.28348037600517273\n","194: 0.21233727782964706 0.14546148106455803 0.3577987477183342\n","195: 0.16717170923948288 0.13926182687282562 0.3064335398375988\n","196: 0.15651825070381165 0.15025481395423412 0.3067730590701103\n","197: 0.1808483973145485 0.15321114845573902 0.33405953645706177\n","198: 0.18519699946045876 0.15578724816441536 0.3409842476248741\n","199: 0.18302005901932716 0.15274153277277946 0.33576159179210663\n","200: 0.16262562572956085 0.1421690732240677 0.30479469895362854\n","201: 0.2286558486521244 0.1431915983557701 0.3718474395573139\n","202: 0.1823306493461132 0.14053871855139732 0.3228693753480911\n","203: 0.1957046575844288 0.15147194266319275 0.34717660397291183\n","204: 0.15532279014587402 0.16061407700181007 0.3159368708729744\n","205: 0.24489671364426613 0.15497948788106441 0.3998761996626854\n","206: 0.21198607049882412 0.143835062161088 0.3558211289346218\n","207: 0.22832626104354858 0.14411895722150803 0.3724452257156372\n","208: 0.1379067562520504 0.14437304809689522 0.2822798043489456\n","209: 0.18263352662324905 0.14129562117159367 0.32392914593219757\n","210: 0.15725720301270485 0.14471822045743465 0.30197542533278465\n","211: 0.20283501595258713 0.15710870549082756 0.3599437214434147\n","212: 0.16815357748419046 0.1388790924102068 0.30703266710042953\n","213: 0.1787795089185238 0.1453312449157238 0.3241107612848282\n","214: 0.12935614958405495 0.1448148861527443 0.27417103946208954\n","215: 0.20787794142961502 0.15583436191082 0.36371230334043503\n","216: 0.14493118599057198 0.14702378027141094 0.29195497184991837\n","217: 0.17524516582489014 0.13964473269879818 0.31488990038633347\n","218: 0.15877464413642883 0.1470828801393509 0.3058575242757797\n","219: 0.1765819936990738 0.14575307816267014 0.3223350718617439\n","220: 0.19812585413455963 0.14151754043996334 0.3396433964371681\n","221: 0.2314341999590397 0.13990210182964802 0.37133630365133286\n","222: 0.16866487637162209 0.14380193874239922 0.3124668076634407\n","223: 0.17365257441997528 0.13733166083693504 0.3109842464327812\n","224: 0.17493283934891224 0.14145104587078094 0.31638388335704803\n","225: 0.16189217567443848 0.1669830996543169 0.32887526601552963\n","226: 0.17558257281780243 0.1428418830037117 0.31842445582151413\n","227: 0.16555263847112656 0.15072613209486008 0.31627877056598663\n","228: 0.20177218317985535 0.1464804783463478 0.34825265407562256\n","229: 0.14892669767141342 0.1715717613697052 0.320498451590538\n","230: 0.2242238186299801 0.14058716036379337 0.3648109883069992\n","231: 0.19530944526195526 0.14004934392869473 0.33535879105329514\n","232: 0.17740893550217152 0.1415333952754736 0.3189423196017742\n","233: 0.16977259144186974 0.15134149231016636 0.3211140800267458\n","234: 0.22538426145911217 0.1466003619134426 0.3719846159219742\n","235: 0.19853628799319267 0.1432983372360468 0.3418346233665943\n","236: 0.2061963975429535 0.14513925835490227 0.35133565217256546\n","237: 0.21215318888425827 0.14873279258608818 0.36088597774505615\n","238: 0.1931683085858822 0.16167240217328072 0.3548407219350338\n","239: 0.18469622358679771 0.14564038068056107 0.3303366079926491\n","240: 0.19013911113142967 0.14035839028656483 0.33049751073122025\n","241: 0.23477740958333015 0.1704267868772149 0.4052041955292225\n","242: 0.2331925705075264 0.1379410745576024 0.37113364040851593\n","243: 0.20268115028738976 0.16495456174016 0.36763569712638855\n","244: 0.1934211105108261 0.16128288209438324 0.35470399260520935\n","245: 0.15643250569701195 0.13735376950353384 0.29378627240657806\n","246: 0.17640602961182594 0.14206757210195065 0.31847359985113144\n","247: 0.18528973497450352 0.16067563463002443 0.3459653779864311\n","248: 0.19648374989628792 0.13983495347201824 0.3363186940550804\n","249: 0.2182946428656578 0.15931306965649128 0.37760771065950394\n","250: 0.21867213770747185 0.14956300519406796 0.36823514103889465\n","251: 0.18707328662276268 0.1451021321117878 0.3321754187345505\n","252: 0.23695968091487885 0.15945013612508774 0.3964098244905472\n","253: 0.2453283965587616 0.15070883929729462 0.396037220954895\n","254: 0.20719925314188004 0.15206418931484222 0.35926343500614166\n","255: 0.15341094881296158 0.13852068409323692 0.2919316291809082\n","256: 0.21878470480442047 0.14720457792282104 0.3659892901778221\n","257: 0.26001565158367157 0.14354637637734413 0.4035620167851448\n","258: 0.18179356306791306 0.15765159204602242 0.3394451439380646\n","259: 0.25438081473112106 0.15142442844808102 0.4058052357286215\n","260: 0.19446029141545296 0.14533425867557526 0.3397945463657379\n","261: 0.1356337731704116 0.1402661930769682 0.2758999541401863\n","262: 0.1788085736334324 0.1417232733219862 0.32053184509277344\n","263: 0.23571445047855377 0.15341589972376823 0.3891303539276123\n","264: 0.16162138432264328 0.16658039763569832 0.3282017931342125\n","265: 0.1936530191451311 0.13959954865276814 0.33325257152318954\n","266: 0.2236685175448656 0.15221351385116577 0.37588202208280563\n","267: 0.24158716574311256 0.15256841480731964 0.3941555693745613\n","268: 0.21910036727786064 0.15679986774921417 0.3759002313017845\n","269: 0.19854251109063625 0.1706391740590334 0.36918168142437935\n","270: 0.18457437679171562 0.13924463838338852 0.32381901517510414\n","271: 0.2021091990172863 0.15891731530427933 0.3610265217721462\n","272: 0.15491671487689018 0.15786061063408852 0.3127773255109787\n","273: 0.19445114210247993 0.15690165013074875 0.3513527885079384\n","274: 0.15089303441345692 0.14762942865490913 0.2985224649310112\n","275: 0.18578695505857468 0.1495657842606306 0.33535274118185043\n","276: 0.20565266534686089 0.16065201070159674 0.3663046844303608\n","277: 0.1985030695796013 0.1424055378884077 0.34090860188007355\n","278: 0.1675148718059063 0.1406488548964262 0.30816373229026794\n","279: 0.23109303042292595 0.1389496922492981 0.37004272639751434\n","280: 0.17587783187627792 0.1473289504647255 0.3232067748904228\n","281: 0.1821628101170063 0.1399753876030445 0.3221381977200508\n","282: 0.223874319344759 0.14683173969388008 0.37070606648921967\n","283: 0.203738272190094 0.16046125069260597 0.36419953405857086\n","284: 0.1875688023865223 0.13992930576205254 0.32749810814857483\n","285: 0.20161506161093712 0.15160730481147766 0.353222381323576\n","286: 0.1549675017595291 0.1433112844824791 0.2982787899672985\n","287: 0.1987903192639351 0.15633734688162804 0.3551276735961437\n","288: 0.21032903343439102 0.1752810850739479 0.38561011850833893\n","289: 0.22241615131497383 0.14706824719905853 0.36948438733816147\n","290: 0.14909525774419308 0.14511729590594769 0.29421254992485046\n","291: 0.24783117324113846 0.1723577380180359 0.42018890753388405\n","292: 0.19311323389410973 0.15034601464867592 0.34345924109220505\n","293: 0.2021859623491764 0.13952490128576756 0.3417108654975891\n","294: 0.19026201963424683 0.17036686465144157 0.3606288768351078\n","295: 0.1893134005367756 0.1393649186939001 0.32867831736803055\n","296: 0.21654726564884186 0.14718440175056458 0.36373166739940643\n","297: 0.1387493647634983 0.14723212644457817 0.2859814912080765\n","298: 0.25459831207990646 0.14034444093704224 0.3949427381157875\n","299: 0.2525628134608269 0.16897256299853325 0.4215353690087795\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RC-LYWcs_q4i"},"source":["# Save model"],"id":"RC-LYWcs_q4i"},{"cell_type":"code","metadata":{"id":"brief-details"},"source":["# torch.save(model.state_dict(), './gauss_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./gauss_model_pytorch_breast'))"],"id":"brief-details","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IpVvYIVM_txS"},"source":["# Test on Test Data"],"id":"IpVvYIVM_txS"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625021814935,"user_tz":-60,"elapsed":11,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"94e19482-10c3-49b5-8e9d-cd214ec94e8f"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":null,"outputs":[{"output_type":"stream","text":["0.9824561403508771\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"gH-8a6IJ_vf-"},"source":["# Test on Train Data"],"id":"gH-8a6IJ_vf-"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625021814935,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"52e149f7-a302-4bd6-d362-4642ecd36de9"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":null,"outputs":[{"output_type":"stream","text":["0.9912023460410557\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy"},"source":[""],"id":"preceding-galaxy","execution_count":null,"outputs":[]}]}
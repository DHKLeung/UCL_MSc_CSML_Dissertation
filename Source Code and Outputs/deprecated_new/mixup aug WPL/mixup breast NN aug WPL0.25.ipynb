{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN aug WPL0.25.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1624971455628,"user_tz":-60,"elapsed":18538,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"7e0fa98c-bf6d-4443-b2ad-4cf8801af234"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1624971456303,"user_tz":-60,"elapsed":677,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1624971456700,"user_tz":-60,"elapsed":398,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1624971456700,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"3753c163-6405-4ff2-b37b-780ee86574e9"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","perturb_loss_weight = 0.25\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fd4a284ba70>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1624971456700,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1624971462792,"user_tz":-60,"elapsed":6095,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"53d82ec9-33ce-478b-c2bf-b33684173747"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1624971462792,"user_tz":-60,"elapsed":10,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    batch_size = labels.size(0)\n","    idx = torch.randperm(batch_size).to('cuda')\n","    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n","    labels_b = labels[idx]\n","    return mixup_inputs, labels, labels_b, lmbda"],"id":"quiet-module","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1624971462793,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1624971465552,"user_tz":-60,"elapsed":2768,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"81671161-0b0b-4a50-ee52-37676af42581"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup perturbation #\n","        mixup_inputs, mixup_labels_a, mixup_labels_b, lmbda = mixup_breast(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs = augment_outputs[original_num:]\n","        mixup_loss = mixup_criterion(criterion, mixup_outputs, mixup_labels_a, mixup_labels_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.136610209941864 2.143536388874054 4.280146598815918\n","1: 2.110995292663574 2.107624888420105 4.218620181083679\n","2: 2.0688422322273254 2.0523020029067993 4.121144235134125\n","3: 2.0030742287635803 1.9813275933265686 3.984401822090149\n","4: 1.9424755573272705 1.9007906317710876 3.843266189098358\n","5: 1.9043342471122742 1.8119545578956604 3.7162888050079346\n","6: 1.834341824054718 1.7102803587913513 3.5446221828460693\n","7: 1.7031277418136597 1.6136495471000671 3.316777288913727\n","8: 1.6539406180381775 1.511601984500885 3.1655426025390625\n","9: 1.5544270277023315 1.412583976984024 2.9670110046863556\n","10: 1.4526263773441315 1.289541631937027 2.7421680092811584\n","11: 1.2928228378295898 1.1796771585941315 2.4724999964237213\n","12: 1.2945116758346558 1.0625554919242859 2.3570671677589417\n","13: 1.1777032613754272 0.9663012623786926 2.14400452375412\n","14: 1.3478936851024628 0.8711278140544891 2.219021499156952\n","15: 1.094963937997818 0.7959165275096893 1.8908804655075073\n","16: 1.0474734902381897 0.7241061627864838 1.7715796530246735\n","17: 1.2056986093521118 0.6478601396083832 1.853558748960495\n","18: 1.1505606323480606 0.6067243367433548 1.7572849690914154\n","19: 1.1326449513435364 0.553439125418663 1.6860840767621994\n","20: 0.8196602761745453 0.5201403349637985 1.3398006111383438\n","21: 1.1992403268814087 0.4764583259820938 1.6756986528635025\n","22: 1.184349000453949 0.4671667367219925 1.6515157371759415\n","23: 1.1765748262405396 0.42707593739032745 1.603650763630867\n","24: 1.221357375383377 0.4189860671758652 1.6403434425592422\n","25: 1.2545824646949768 0.40411141514778137 1.6586938798427582\n","26: 1.1251851618289948 0.3842816650867462 1.509466826915741\n","27: 0.8897751122713089 0.36218784749507904 1.251962959766388\n","28: 0.7949943840503693 0.38027364015579224 1.1752680242061615\n","29: 1.0371049642562866 0.34975331276655197 1.3868582770228386\n","30: 1.0018700659275055 0.33995305746793747 1.341823123395443\n","31: 1.1015567183494568 0.33837295323610306 1.4399296715855598\n","32: 1.244222342967987 0.3319132551550865 1.5761355981230736\n","33: 1.1234540790319443 0.31555674970149994 1.4390108287334442\n","34: 1.156339280307293 0.32364028692245483 1.4799795672297478\n","35: 1.1748316884040833 0.30893879383802414 1.4837704822421074\n","36: 0.9456420540809631 0.3056951314210892 1.2513371855020523\n","37: 1.198987752199173 0.29940470308065414 1.4983924552798271\n","38: 0.7660480737686157 0.29391080886125565 1.0599588826298714\n","39: 1.1676602065563202 0.30008143931627274 1.467741645872593\n","40: 1.1762071549892426 0.3076285719871521 1.4838357269763947\n","41: 0.7170755043625832 0.275695264339447 0.9927707687020302\n","42: 1.352889209985733 0.2799554541707039 1.632844664156437\n","43: 1.1342581361532211 0.2729865536093712 1.4072446897625923\n","44: 1.155628778040409 0.2703034579753876 1.4259322360157967\n","45: 0.8961312770843506 0.2698827087879181 1.1660139858722687\n","46: 1.408487230539322 0.2652455121278763 1.6737327426671982\n","47: 1.064182423055172 0.2619025707244873 1.3260849937796593\n","48: 1.3823047280311584 0.29188840091228485 1.6741931289434433\n","49: 1.4562219977378845 0.2682735025882721 1.7244955003261566\n","50: 1.0190858989953995 0.26911526173353195 1.2882011607289314\n","51: 1.1573818027973175 0.2681698426604271 1.4255516454577446\n","52: 0.8245351389050484 0.2585439011454582 1.0830790400505066\n","53: 1.2448972761631012 0.27102329581975937 1.5159205719828606\n","54: 0.8754086047410965 0.24874146282672882 1.1241500675678253\n","55: 1.3292611837387085 0.2673467621207237 1.5966079458594322\n","56: 1.1681642830371857 0.24785704910755157 1.4160213321447372\n","57: 1.0125123113393784 0.24710724502801895 1.2596195563673973\n","58: 1.4152562618255615 0.2505454048514366 1.6658016666769981\n","59: 1.1704474836587906 0.2449992597103119 1.4154467433691025\n","60: 1.3012244999408722 0.23814239725470543 1.5393668971955776\n","61: 0.9525337293744087 0.24083102121949196 1.1933647505939007\n","62: 1.2178621739149094 0.2532195374369621 1.4710817113518715\n","63: 1.2702818810939789 0.23658305034041405 1.506864931434393\n","64: 1.1949872970581055 0.23983439058065414 1.4348216876387596\n","65: 1.4447572827339172 0.2545202113687992 1.6992774941027164\n","66: 1.3209066689014435 0.23359746858477592 1.5545041374862194\n","67: 1.2036013901233673 0.25581803172826767 1.459419421851635\n","68: 0.8099875897169113 0.2565648853778839 1.0665524750947952\n","69: 1.142610102891922 0.23359473422169685 1.3762048371136189\n","70: 1.1223353147506714 0.24283917248249054 1.365174487233162\n","71: 0.9406789541244507 0.2433621510863304 1.184041105210781\n","72: 1.622421532869339 0.23071053251624107 1.85313206538558\n","73: 1.1051927953958511 0.2401304729282856 1.3453232683241367\n","74: 1.2757776379585266 0.23245976865291595 1.5082374066114426\n","75: 1.0538819581270218 0.24654296040534973 1.3004249185323715\n","76: 1.5185817182064056 0.24588316679000854 1.7644648849964142\n","77: 1.4437181949615479 0.22743844613432884 1.6711566410958767\n","78: 1.4287740886211395 0.23166489601135254 1.660438984632492\n","79: 1.1836416199803352 0.25027449429035187 1.433916114270687\n","80: 1.460252285003662 0.23045921325683594 1.690711498260498\n","81: 1.267295092344284 0.22623083367943764 1.4935259260237217\n","82: 0.6576042473316193 0.22385555878281593 0.8814598061144352\n","83: 0.7707216292619705 0.24160459637641907 1.0123262256383896\n","84: 1.1900453716516495 0.2276185043156147 1.4176638759672642\n","85: 1.1671724617481232 0.24362653121352196 1.4107989929616451\n","86: 1.0434782058000565 0.21846219152212143 1.2619403973221779\n","87: 1.5967805981636047 0.2438829056918621 1.8406635038554668\n","88: 1.0435553789138794 0.23691889643669128 1.2804742753505707\n","89: 1.4449691772460938 0.22403200343251228 1.669001180678606\n","90: 1.0133908689022064 0.21922648325562477 1.2326173521578312\n","91: 1.0787057280540466 0.2455085664987564 1.324214294552803\n","92: 1.2444797605276108 0.22465519607067108 1.4691349565982819\n","93: 0.7531361505389214 0.22174369916319847 0.9748798497021198\n","94: 1.2956517040729523 0.21933317556977272 1.514984879642725\n","95: 1.3387612104415894 0.21647664532065392 1.5552378557622433\n","96: 1.0080552101135254 0.22550466656684875 1.2335598766803741\n","97: 0.9032720029354095 0.22714050859212875 1.1304125115275383\n","98: 0.9477390050888062 0.22657952457666397 1.1743185296654701\n","99: 1.2870878279209137 0.23346882313489914 1.5205566510558128\n","100: 1.4436360895633698 0.21848509460687637 1.6621211841702461\n","101: 1.09368896484375 0.2069801315665245 1.3006690964102745\n","102: 1.19952791929245 0.22804761677980423 1.4275755360722542\n","103: 1.2429287433624268 0.22892941907048225 1.471858162432909\n","104: 0.8137250691652298 0.23092272877693176 1.0446477979421616\n","105: 1.4367497861385345 0.21272745728492737 1.649477243423462\n","106: 0.9764644503593445 0.21255461126565933 1.1890190616250038\n","107: 0.93657186627388 0.2413674183189869 1.177939284592867\n","108: 0.9648947566747665 0.22587717697024345 1.19077193364501\n","109: 0.8360598981380463 0.2094498686492443 1.0455097667872906\n","110: 1.236644208431244 0.2084302455186844 1.4450744539499283\n","111: 1.3308629840612411 0.2136359065771103 1.5444988906383514\n","112: 1.270311564207077 0.2056596502661705 1.4759712144732475\n","113: 1.2523868083953857 0.20964279770851135 1.462029606103897\n","114: 1.410769522190094 0.20465703308582306 1.615426555275917\n","115: 0.9310540109872818 0.20930086821317673 1.1403548792004585\n","116: 0.9446569383144379 0.21768171340227127 1.1623386517167091\n","117: 0.7434239536523819 0.20781072601675987 0.9512346796691418\n","118: 1.5496684312820435 0.211209237575531 1.7608776688575745\n","119: 0.9405000954866409 0.21465501189231873 1.1551551073789597\n","120: 0.5615281388163567 0.21220197528600693 0.7737301141023636\n","121: 1.0377218276262283 0.2133152149617672 1.2510370425879955\n","122: 1.2916789948940277 0.22177395969629288 1.5134529545903206\n","123: 1.0129086524248123 0.209051251411438 1.2219599038362503\n","124: 0.4315185695886612 0.2291836515069008 0.660702221095562\n","125: 1.6014705002307892 0.19945598766207695 1.8009264878928661\n","126: 0.866259902715683 0.1963941566646099 1.062654059380293\n","127: 1.7100083827972412 0.19918342307209969 1.909191805869341\n","128: 0.889076367020607 0.20123371109366417 1.0903100781142712\n","129: 1.3999381959438324 0.20334464311599731 1.6032828390598297\n","130: 1.582660973072052 0.21067605167627335 1.7933370247483253\n","131: 1.2424810230731964 0.22396274656057358 1.46644376963377\n","132: 1.686124563217163 0.20712468028068542 1.8932492434978485\n","133: 1.2019521296024323 0.22820032760500908 1.4301524572074413\n","134: 1.1064624190330505 0.2043035700917244 1.310765989124775\n","135: 1.5014722347259521 0.21240133047103882 1.713873565196991\n","136: 1.3702101707458496 0.20616081357002258 1.5763709843158722\n","137: 1.3879885077476501 0.19946955516934395 1.587458062916994\n","138: 1.2735251784324646 0.21017635241150856 1.4837015308439732\n","139: 1.3116120994091034 0.20543532446026802 1.5170474238693714\n","140: 1.5183802545070648 0.21973209828138351 1.7381123527884483\n","141: 1.146951124072075 0.2093925103545189 1.3563436344265938\n","142: 1.1052559614181519 0.20949212089180946 1.3147480823099613\n","143: 1.249284714460373 0.20313859730958939 1.4524233117699623\n","144: 1.1372973918914795 0.22759198024868965 1.3648893721401691\n","145: 0.9969339407980442 0.20297116413712502 1.1999051049351692\n","146: 0.8194605559110641 0.20539885386824608 1.0248594097793102\n","147: 0.6836377382278442 0.21681976318359375 0.900457501411438\n","148: 1.491794615983963 0.20841042697429657 1.7002050429582596\n","149: 1.0320066511631012 0.19979852065443993 1.2318051718175411\n","150: 0.8534215092658997 0.21849704533815384 1.0719185546040535\n","151: 1.4500494301319122 0.2168448381125927 1.666894268244505\n","152: 1.492942452430725 0.20742571353912354 1.7003681659698486\n","153: 1.0790809988975525 0.20993884652853012 1.2890198454260826\n","154: 1.3384515047073364 0.20136286318302155 1.539814367890358\n","155: 1.2662800550460815 0.20150429755449295 1.4677843526005745\n","156: 1.3840186595916748 0.20051151141524315 1.584530171006918\n","157: 0.9276812374591827 0.2052057422697544 1.1328869797289371\n","158: 1.2307473719120026 0.2331988476216793 1.4639462195336819\n","159: 1.5472960472106934 0.2061029188334942 1.7533989660441875\n","160: 1.3299122750759125 0.2053890973329544 1.5353013724088669\n","161: 1.2073030769824982 0.2020733803510666 1.4093764573335648\n","162: 0.835251972079277 0.21848813071846962 1.0537401027977467\n","163: 1.3145053386688232 0.2120441347360611 1.5265494734048843\n","164: 0.9408491104841232 0.20441946759819984 1.145268578082323\n","165: 0.6269167400896549 0.20147785544395447 0.8283945955336094\n","166: 0.7955967932939529 0.1965712532401085 0.9921680465340614\n","167: 1.356543391942978 0.2115389034152031 1.568082295358181\n","168: 0.7944225072860718 0.2054225243628025 0.9998450316488743\n","169: 1.30570387840271 0.21424122527241707 1.519945103675127\n","170: 1.169909618794918 0.19491749815642834 1.3648271169513464\n","171: 1.2229630947113037 0.1999564841389656 1.4229195788502693\n","172: 0.8420703560113907 0.20742134004831314 1.0494916960597038\n","173: 0.8473372459411621 0.19781498610973358 1.0451522320508957\n","174: 1.6177387535572052 0.209626242518425 1.8273649960756302\n","175: 1.3739725351333618 0.20275815576314926 1.576730690896511\n","176: 0.921829417347908 0.22224152460694313 1.1440709419548512\n","177: 0.719476506114006 0.21833154559135437 0.9378080517053604\n","178: 1.4681323170661926 0.22174234315752983 1.6898746602237225\n","179: 0.6653244197368622 0.1972423829138279 0.8625668026506901\n","180: 1.5612069368362427 0.20649243891239166 1.7676993757486343\n","181: 1.238973319530487 0.2176370657980442 1.4566103853285313\n","182: 0.9663394689559937 0.21024327725172043 1.176582746207714\n","183: 0.529093861579895 0.23454487323760986 0.7636387348175049\n","184: 1.456931471824646 0.20596469938755035 1.6628961712121964\n","185: 1.2711573839187622 0.2022159919142723 1.4733733758330345\n","186: 0.4902639761567116 0.22227151319384575 0.7125354893505573\n","187: 1.4424203932285309 0.19990716502070427 1.6423275582492352\n","188: 1.4884874522686005 0.21458067744970322 1.7030681297183037\n","189: 1.1231878101825714 0.21341253817081451 1.336600348353386\n","190: 1.5312885344028473 0.2189129963517189 1.7502015307545662\n","191: 1.4252262115478516 0.20918618887662888 1.6344124004244804\n","192: 0.5473979115486145 0.20662587881088257 0.7540237903594971\n","193: 0.9154272377490997 0.2217651680111885 1.1371924057602882\n","194: 1.2604934573173523 0.2044190652668476 1.4649125225842\n","195: 1.4412596225738525 0.2099771723151207 1.6512367948889732\n","196: 1.428680032491684 0.20036570355296135 1.6290457360446453\n","197: 1.3625394403934479 0.20603439956903458 1.5685738399624825\n","198: 0.6073066145181656 0.20154798030853271 0.8088545948266983\n","199: 1.2327613830566406 0.2026355341076851 1.4353969171643257\n","200: 0.7950945422053337 0.20304857194423676 0.9981431141495705\n","201: 1.2737941890954971 0.20386390388011932 1.4776580929756165\n","202: 1.4382745027542114 0.20094868540763855 1.63922318816185\n","203: 0.9881518185138702 0.20195015892386436 1.1901019774377346\n","204: 1.469973087310791 0.20502740144729614 1.6750004887580872\n","205: 1.296699196100235 0.2098006010055542 1.5064997971057892\n","206: 1.2087863236665726 0.22058643400669098 1.4293727576732635\n","207: 0.6064359992742538 0.20792876183986664 0.8143647611141205\n","208: 1.5584643185138702 0.20002426952123642 1.7584885880351067\n","209: 1.2006679028272629 0.22546035051345825 1.4261282533407211\n","210: 1.0227870792150497 0.20974203199148178 1.2325291112065315\n","211: 1.0609600245952606 0.1952974870800972 1.2562575116753578\n","212: 0.6180146485567093 0.22264030948281288 0.8406549580395222\n","213: 1.0009929090738297 0.1984909549355507 1.1994838640093803\n","214: 1.7692314386367798 0.20393803343176842 1.9731694720685482\n","215: 0.8466073870658875 0.19620609283447266 1.04281347990036\n","216: 0.8773305416107178 0.21889174729585648 1.0962222889065742\n","217: 1.5106686651706696 0.19538437575101852 1.706053040921688\n","218: 0.8270738273859024 0.22783537581562996 1.0549092032015324\n","219: 0.9339667856693268 0.2063344344496727 1.1403012201189995\n","220: 1.333655595779419 0.22079998254776 1.554455578327179\n","221: 1.2141345143318176 0.19877102226018906 1.4129055365920067\n","222: 1.043171688914299 0.20438988506793976 1.2475615739822388\n","223: 0.8969879001379013 0.1936056036502123 1.0905935037881136\n","224: 1.2677663564682007 0.2078024409711361 1.4755687974393368\n","225: 1.20138680934906 0.21904144436120987 1.42042825371027\n","226: 1.211278885602951 0.1976219043135643 1.4089007899165154\n","227: 1.3840570449829102 0.21706092357635498 1.6011179685592651\n","228: 0.9636417925357819 0.2087978571653366 1.1724396497011185\n","229: 1.2415486574172974 0.2165975272655487 1.458146184682846\n","230: 0.5525144524872303 0.1998332515358925 0.7523477040231228\n","231: 0.8764390349388123 0.21186595410108566 1.088304989039898\n","232: 0.9680856466293335 0.215756643563509 1.1838422901928425\n","233: 0.858172819018364 0.1901518888771534 1.0483247078955173\n","234: 0.3785383999347687 0.21312812715768814 0.5916665270924568\n","235: 0.8125714808702469 0.19817933812737465 1.0107508189976215\n","236: 1.2042215168476105 0.19941771775484085 1.4036392346024513\n","237: 0.8445348739624023 0.2200470305979252 1.0645819045603275\n","238: 1.176492154598236 0.21939318627119064 1.3958853408694267\n","239: 1.6523809432983398 0.21209315210580826 1.864474095404148\n","240: 1.0083359330892563 0.1960090473294258 1.204344980418682\n","241: 1.6030192077159882 0.19710949063301086 1.800128698348999\n","242: 1.207245111465454 0.19563696160912514 1.4028820730745792\n","243: 1.6156723201274872 0.19727520644664764 1.8129475265741348\n","244: 1.5697388350963593 0.1951669156551361 1.7649057507514954\n","245: 0.9294609129428864 0.2135857492685318 1.1430466622114182\n","246: 0.8165440112352371 0.22427047789096832 1.0408144891262054\n","247: 1.4629052579402924 0.21288872510194778 1.6757939830422401\n","248: 1.0984665900468826 0.2105957381427288 1.3090623281896114\n","249: 1.3998191356658936 0.19485031440854073 1.5946694500744343\n","250: 1.3025836050510406 0.21397561579942703 1.5165592208504677\n","251: 1.1675457954406738 0.21852421015501022 1.386070005595684\n","252: 1.2472093999385834 0.19490882009267807 1.4421182200312614\n","253: 1.148448571562767 0.21123478189110756 1.3596833534538746\n","254: 0.8906784653663635 0.20885733515024185 1.0995358005166054\n","255: 1.1553072929382324 0.21244660019874573 1.3677538931369781\n","256: 1.5071063339710236 0.19527681544423103 1.7023831494152546\n","257: 1.463898479938507 0.21276520565152168 1.6766636855900288\n","258: 1.4606983959674835 0.20365188270807266 1.6643502786755562\n","259: 1.3140339255332947 0.1990673467516899 1.5131012722849846\n","260: 0.9750916957855225 0.2129117101430893 1.1880034059286118\n","261: 0.6532048508524895 0.20211513340473175 0.8553199842572212\n","262: 1.0948742926120758 0.2068803682923317 1.3017546609044075\n","263: 1.3527618050575256 0.20704302191734314 1.5598048269748688\n","264: 1.6430757641792297 0.19827179610729218 1.841347560286522\n","265: 1.157898023724556 0.19399390742182732 1.3518919311463833\n","266: 1.4684243500232697 0.19914161413908005 1.6675659641623497\n","267: 1.1855473220348358 0.20428939908742905 1.3898367211222649\n","268: 1.4083597660064697 0.22094081342220306 1.6293005794286728\n","269: 1.071466326713562 0.2065093144774437 1.2779756411910057\n","270: 1.115946777164936 0.19566603377461433 1.3116128109395504\n","271: 0.5647213011980057 0.19973383098840714 0.7644551321864128\n","272: 0.3797408863902092 0.2140839621424675 0.5938248485326767\n","273: 1.2519281059503555 0.22304614260792732 1.4749742485582829\n","274: 1.3483794927597046 0.22061999514698982 1.5689994879066944\n","275: 1.3640857636928558 0.21530380845069885 1.5793895721435547\n","276: 0.6789887845516205 0.19810577481985092 0.8770945593714714\n","277: 1.2472899705171585 0.20231200382113457 1.449601974338293\n","278: 0.9294141381978989 0.2063276171684265 1.1357417553663254\n","279: 1.4618107676506042 0.22036225348711014 1.6821730211377144\n","280: 1.3622013330459595 0.2208472490310669 1.5830485820770264\n","281: 0.8292279541492462 0.19648420810699463 1.0257121622562408\n","282: 1.5356882214546204 0.20881635323166847 1.7445045746862888\n","283: 0.8833891004323959 0.2211565300822258 1.1045456305146217\n","284: 1.3747127950191498 0.19652416929602623 1.571236964315176\n","285: 1.0696117728948593 0.1993149146437645 1.2689266875386238\n","286: 1.3373763263225555 0.21058233454823494 1.5479586608707905\n","287: 0.6210633963346481 0.20053600892424583 0.821599405258894\n","288: 1.3525558412075043 0.20709051191806793 1.5596463531255722\n","289: 1.1925666630268097 0.20125852897763252 1.3938251920044422\n","290: 1.3088048696517944 0.19788547605276108 1.5066903457045555\n","291: 0.9717779755592346 0.19352425634860992 1.1653022319078445\n","292: 1.023871548473835 0.21418282017111778 1.2380543686449528\n","293: 0.9077281951904297 0.20088429749011993 1.1086124926805496\n","294: 1.3546773195266724 0.21270287036895752 1.5673801898956299\n","295: 1.054999515414238 0.19819384068250656 1.2531933560967445\n","296: 1.3899264931678772 0.2245253138244152 1.6144518069922924\n","297: 0.6045425236225128 0.2060253694653511 0.8105678930878639\n","298: 1.8249112367630005 0.2214103601872921 2.0463215969502926\n","299: 1.500184565782547 0.20616410672664642 1.7063486725091934\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1624971465553,"user_tz":-60,"elapsed":13,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1624971465553,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"4ebb2160-e58e-4011-8f08-a130ed0c3488"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9736842105263158\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1624971465554,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"69f0a4ec-7b2e-467e-e3b5-4b96001d8983"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9882697947214076\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1624971465554,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":12,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN aug WPL0.75.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1624971630420,"user_tz":-60,"elapsed":17302,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"8693d548-6e31-486a-9f4a-058058da510d"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5"},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W"},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1624971631573,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"a5c142b0-e258-4f5b-c554-af46efb6a063"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","perturb_loss_weight = 0.75\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fc077dbda50>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule"},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1624971637994,"user_tz":-60,"elapsed":6147,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"ad27f39c-f820-492d-c2e9-ccb8063ca801"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module"},"source":["def mixup_breast(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    batch_size = labels.size(0)\n","    idx = torch.randperm(batch_size).to('cuda')\n","    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n","    labels_b = labels[idx]\n","    return mixup_inputs, labels, labels_b, lmbda"],"id":"quiet-module","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions"},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1624971640958,"user_tz":-60,"elapsed":2967,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"b94b9648-5b2a-4700-ed6c-4c754ec0e3bc"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup perturbation #\n","        mixup_inputs, mixup_labels_a, mixup_labels_b, lmbda = mixup_breast(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs = augment_outputs[original_num:]\n","        mixup_loss = mixup_criterion(criterion, mixup_outputs, mixup_labels_a, mixup_labels_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.135200798511505 2.1458818316459656 4.281082630157471\n","1: 2.113309860229492 2.121008515357971 4.234318375587463\n","2: 2.083314001560211 2.0759646892547607 4.159278690814972\n","3: 2.031737208366394 2.018240749835968 4.049977958202362\n","4: 1.9791727662086487 1.9495183229446411 3.92869108915329\n","5: 1.941588044166565 1.8768898248672485 3.8184778690338135\n","6: 1.8657068014144897 1.7940656542778015 3.6597724556922913\n","7: 1.799124300479889 1.7160258293151855 3.5151501297950745\n","8: 1.7418190836906433 1.6305504441261292 3.3723695278167725\n","9: 1.6707698702812195 1.5512408018112183 3.2220106720924377\n","10: 1.5751186609268188 1.4659130573272705 3.0410317182540894\n","11: 1.4565399885177612 1.360917717218399 2.8174577057361603\n","12: 1.4572466909885406 1.2642936706542969 2.7215403616428375\n","13: 1.3765036463737488 1.1727685928344727 2.5492722392082214\n","14: 1.4466383457183838 1.0928407609462738 2.5394791066646576\n","15: 1.2712818682193756 1.0029760897159576 2.2742579579353333\n","16: 1.1776841878890991 0.922114223241806 2.099798411130905\n","17: 1.4774929285049438 0.8745445311069489 2.3520374596118927\n","18: 1.2005797326564789 0.8116535246372223 2.012233257293701\n","19: 1.2014100253582 0.7563896924257278 1.957799717783928\n","20: 1.0378258228302002 0.7173977792263031 1.7552236020565033\n","21: 1.2514312863349915 0.660589337348938 1.9120206236839294\n","22: 1.234011471271515 0.645918145775795 1.8799296170473099\n","23: 1.2454476058483124 0.6228376775979996 1.868285283446312\n","24: 1.221842497587204 0.5911012291908264 1.8129437267780304\n","25: 1.3175092339515686 0.5829265117645264 1.900435745716095\n","26: 1.08237886428833 0.5592420101165771 1.6416208744049072\n","27: 0.8222487270832062 0.5608648210763931 1.3831135481595993\n","28: 0.8371254503726959 0.5208451151847839 1.3579705655574799\n","29: 0.9894205182790756 0.5113497525453568 1.5007702708244324\n","30: 0.9520378559827805 0.4907076507806778 1.4427455067634583\n","31: 1.165311574935913 0.49062395095825195 1.655935525894165\n","32: 1.1258573532104492 0.4762825667858124 1.6021399199962616\n","33: 1.0367874503135681 0.44785597175359726 1.4846434220671654\n","34: 1.0713326185941696 0.44858261942863464 1.5199152380228043\n","35: 1.1316072344779968 0.4494738131761551 1.581081047654152\n","36: 0.9151433855295181 0.4408601373434067 1.3560035228729248\n","37: 1.07359080016613 0.4254412055015564 1.4990320056676865\n","38: 0.8454222232103348 0.43623800575733185 1.2816602289676666\n","39: 1.066884994506836 0.4368574768304825 1.5037424713373184\n","40: 1.0413305908441544 0.41396614164114 1.4552967324852943\n","41: 0.7422594726085663 0.42481907457113266 1.167078547179699\n","42: 1.2154686152935028 0.42022230476140976 1.6356909200549126\n","43: 1.1315809190273285 0.4171614348888397 1.5487423539161682\n","44: 0.9159521907567978 0.41139644384384155 1.3273486346006393\n","45: 0.8564799129962921 0.4084427207708359 1.264922633767128\n","46: 1.0980119109153748 0.3936240077018738 1.4916359186172485\n","47: 0.9345306754112244 0.3836844936013222 1.3182151690125465\n","48: 1.3234201669692993 0.38387513160705566 1.707295298576355\n","49: 1.2294281125068665 0.40510454028844833 1.6345326527953148\n","50: 0.9750561416149139 0.40118902921676636 1.3762451708316803\n","51: 1.23183012008667 0.39407970011234283 1.6259098201990128\n","52: 0.721698984503746 0.4002523794770241 1.1219513639807701\n","53: 1.0874556303024292 0.39770713448524475 1.485162764787674\n","54: 0.8632911443710327 0.387871615588665 1.2511627599596977\n","55: 1.2303257584571838 0.39469826221466064 1.6250240206718445\n","56: 1.175428867340088 0.39216701686382294 1.5675958842039108\n","57: 1.032241091132164 0.40171197056770325 1.4339530616998672\n","58: 1.2382668256759644 0.4063206985592842 1.6445875242352486\n","59: 1.1113347709178925 0.3897830992937088 1.5011178702116013\n","60: 1.358960509300232 0.4071011021733284 1.7660616114735603\n","61: 1.051391750574112 0.39813172817230225 1.4495234787464142\n","62: 1.171485275030136 0.39722641557455063 1.5687116906046867\n","63: 1.1886359751224518 0.4040764942765236 1.5927124693989754\n","64: 1.0133829712867737 0.39923592656850815 1.4126188978552818\n","65: 1.2038739621639252 0.4034954532980919 1.607369415462017\n","66: 1.20492285490036 0.40376412868499756 1.6086869835853577\n","67: 1.121071457862854 0.4046540856361389 1.525725543498993\n","68: 0.9182606339454651 0.4001722186803818 1.3184328526258469\n","69: 1.0774821117520332 0.42808088660240173 1.505562998354435\n","70: 1.0691969096660614 0.4168948829174042 1.4860917925834656\n","71: 1.017869845032692 0.39792417734861374 1.4157940223813057\n","72: 1.3810162842273712 0.41087400168180466 1.7918902859091759\n","73: 0.9670161455869675 0.39439211785793304 1.3614082634449005\n","74: 1.2012232840061188 0.3993396833539009 1.6005629673600197\n","75: 0.883334755897522 0.389295369386673 1.272630125284195\n","76: 1.3469447493553162 0.3918573707342148 1.738802120089531\n","77: 1.305763304233551 0.39627350866794586 1.7020368129014969\n","78: 1.2324629724025726 0.4002791866660118 1.6327421590685844\n","79: 1.0607010573148727 0.41272419691085815 1.473425254225731\n","80: 1.3050639927387238 0.4145243316888809 1.7195883244276047\n","81: 1.1125361919403076 0.40843476355075836 1.520970955491066\n","82: 0.7244393080472946 0.40566106885671616 1.1301003769040108\n","83: 0.7739917039871216 0.3945092037320137 1.1685009077191353\n","84: 1.1215973943471909 0.4048120975494385 1.5264094918966293\n","85: 1.051658570766449 0.4085763618350029 1.4602349326014519\n","86: 0.9059715270996094 0.3974485844373703 1.3034201115369797\n","87: 1.3180756866931915 0.3917417526245117 1.7098174393177032\n","88: 0.9280924797058105 0.3975612074136734 1.325653687119484\n","89: 1.2623300552368164 0.39601171016693115 1.6583417654037476\n","90: 0.869718074798584 0.38505861163139343 1.2547766864299774\n","91: 1.067606270313263 0.3966861069202423 1.4642923772335052\n","92: 1.1344256103038788 0.38300132006406784 1.5174269303679466\n","93: 0.7337641417980194 0.3829071968793869 1.1166713386774063\n","94: 1.1152851581573486 0.37148958444595337 1.486774742603302\n","95: 1.1605508029460907 0.3895217552781105 1.5500725582242012\n","96: 0.9429995864629745 0.37300920486450195 1.3160087913274765\n","97: 0.8445965200662613 0.3718697130680084 1.2164662331342697\n","98: 0.874136358499527 0.37809552252292633 1.2522318810224533\n","99: 1.1641829013824463 0.37336769700050354 1.5375505983829498\n","100: 1.202221542596817 0.36710184067487717 1.5693233832716942\n","101: 1.0614992380142212 0.3697727918624878 1.431272029876709\n","102: 1.084202691912651 0.38167454302310944 1.4658772349357605\n","103: 1.345494955778122 0.38548336923122406 1.730978325009346\n","104: 0.7444752305746078 0.3742513731122017 1.1187266036868095\n","105: 1.3691493570804596 0.3745720908045769 1.7437214478850365\n","106: 0.976658433675766 0.3743475005030632 1.3510059341788292\n","107: 0.9380942285060883 0.389784999191761 1.3278792276978493\n","108: 1.0053864866495132 0.38503194600343704 1.3904184326529503\n","109: 0.901493176817894 0.3742848411202431 1.275778017938137\n","110: 1.1064902245998383 0.3698011338710785 1.4762913584709167\n","111: 1.1140207201242447 0.3712606877088547 1.4852814078330994\n","112: 1.2185040414333344 0.3751816377043724 1.5936856791377068\n","113: 1.1777142733335495 0.3707464933395386 1.548460766673088\n","114: 1.1952673196792603 0.3852256089448929 1.5804929286241531\n","115: 0.884035587310791 0.3699241280555725 1.2539597153663635\n","116: 0.9746017605066299 0.3864004537463188 1.3610022142529488\n","117: 0.8101779818534851 0.4008480831980705 1.2110260650515556\n","118: 1.420593410730362 0.37721578031778336 1.7978091910481453\n","119: 0.8908476233482361 0.3776247203350067 1.2684723436832428\n","120: 0.6615321487188339 0.38089054822921753 1.0424226969480515\n","121: 1.028988242149353 0.3672122359275818 1.3962004780769348\n","122: 1.1692583858966827 0.36475950479507446 1.5340178906917572\n","123: 0.9015566110610962 0.3729993924498558 1.274556003510952\n","124: 0.4909704178571701 0.35443856567144394 0.845408983528614\n","125: 1.194975197315216 0.3764728456735611 1.5714480429887772\n","126: 0.8686709553003311 0.3767155259847641 1.2453864812850952\n","127: 1.3300188779830933 0.3503403589129448 1.680359236896038\n","128: 1.0097190737724304 0.3442700728774071 1.3539891466498375\n","129: 1.1254733800888062 0.34997569024562836 1.4754490703344345\n","130: 1.1203536987304688 0.37079425901174545 1.4911479577422142\n","131: 1.094622641801834 0.37102558463811874 1.4656482264399529\n","132: 1.3692266047000885 0.38023029267787933 1.7494568973779678\n","133: 1.1080393642187119 0.3787670284509659 1.4868063926696777\n","134: 1.0512982904911041 0.3896987661719322 1.4409970566630363\n","135: 1.3545755743980408 0.3904963955283165 1.7450719699263573\n","136: 1.1868254840373993 0.3749807998538017 1.561806283891201\n","137: 1.2958170175552368 0.3767482042312622 1.672565221786499\n","138: 1.3335627913475037 0.39808277040719986 1.7316455617547035\n","139: 1.3230290710926056 0.38637538254261017 1.7094044536352158\n","140: 1.312660813331604 0.40004801005125046 1.7127088233828545\n","141: 0.9823464155197144 0.40545546263456345 1.3878018781542778\n","142: 0.8948653340339661 0.40900540351867676 1.3038707375526428\n","143: 1.1525121331214905 0.4032537639141083 1.5557658970355988\n","144: 1.0126532316207886 0.3985505476593971 1.4112037792801857\n","145: 0.9590576440095901 0.41175956279039383 1.370817206799984\n","146: 0.8237973004579544 0.388597771525383 1.2123950719833374\n","147: 0.7342543005943298 0.3885131701827049 1.1227674707770348\n","148: 1.2291347980499268 0.39379724115133286 1.6229320392012596\n","149: 0.962255522608757 0.38416340202093124 1.3464189246296883\n","150: 0.8484440296888351 0.38916561007499695 1.237609639763832\n","151: 1.2703573405742645 0.3741471767425537 1.6445045173168182\n","152: 1.4009833633899689 0.3696632385253906 1.7706466019153595\n","153: 0.9563278555870056 0.3696630969643593 1.325990952551365\n","154: 1.168264001607895 0.3718324676156044 1.5400964692234993\n","155: 1.098450019955635 0.3793163374066353 1.4777663573622704\n","156: 1.3272518813610077 0.3909052684903145 1.7181571498513222\n","157: 0.8696072399616241 0.37511370331048965 1.2447209432721138\n","158: 1.1142558157444 0.3881072327494621 1.5023630484938622\n","159: 1.2829382717609406 0.3858257085084915 1.668763980269432\n","160: 1.0592117309570312 0.3743458017706871 1.4335575327277184\n","161: 1.0146894752979279 0.38226862996816635 1.3969581052660942\n","162: 0.78090038895607 0.3829079791903496 1.1638083681464195\n","163: 1.1340623199939728 0.37564563751220703 1.5097079575061798\n","164: 0.9583725035190582 0.37622666358947754 1.3345991671085358\n","165: 0.6872815415263176 0.37781716138124466 1.0650987029075623\n","166: 0.8123460114002228 0.3733948767185211 1.185740888118744\n","167: 1.1723554134368896 0.3823374882340431 1.5546929016709328\n","168: 0.7410523444414139 0.3706991374492645 1.1117514818906784\n","169: 1.2008944749832153 0.3858465328812599 1.5867410078644753\n","170: 1.0418912768363953 0.376603402197361 1.4184946790337563\n","171: 1.0703925788402557 0.37742479145526886 1.4478173702955246\n","172: 0.8840867727994919 0.3817613497376442 1.265848122537136\n","173: 0.848993219435215 0.3929389715194702 1.2419321909546852\n","174: 1.2984532117843628 0.3757270723581314 1.6741802841424942\n","175: 1.2159565091133118 0.37575894594192505 1.5917154550552368\n","176: 0.8610532283782959 0.38038139045238495 1.2414346188306808\n","177: 0.7709187120199203 0.38960615545511246 1.1605248674750328\n","178: 1.2500464022159576 0.39658231288194656 1.6466287150979042\n","179: 0.7821155488491058 0.37811553478240967 1.1602310836315155\n","180: 1.3949881792068481 0.3814169615507126 1.7764051407575607\n","181: 1.1315141022205353 0.36678237468004227 1.4982964769005775\n","182: 0.9806942939758301 0.3698694258928299 1.35056371986866\n","183: 0.6541779637336731 0.37354161590337753 1.0277195796370506\n","184: 1.2371816635131836 0.3611603081226349 1.5983419716358185\n","185: 1.128807544708252 0.37392323464155197 1.502730779349804\n","186: 0.5702383816242218 0.38085076957941055 0.9510891512036324\n","187: 1.2962075173854828 0.37612690031528473 1.6723344177007675\n","188: 1.1726529896259308 0.373019203543663 1.5456721931695938\n","189: 1.0126057118177414 0.3880508244037628 1.4006565362215042\n","190: 1.271815299987793 0.37740595638751984 1.6492212563753128\n","191: 1.0895815193653107 0.3700414299964905 1.4596229493618011\n","192: 0.5767945647239685 0.3666001856327057 0.9433947503566742\n","193: 0.9125558137893677 0.3740701302886009 1.2866259440779686\n","194: 1.0481141209602356 0.37354084104299545 1.421654962003231\n","195: 1.1775679886341095 0.3719993829727173 1.5495673716068268\n","196: 1.1916665732860565 0.39084309339523315 1.5825096666812897\n","197: 1.1702613830566406 0.3821582645177841 1.5524196475744247\n","198: 0.6624044477939606 0.3653194010257721 1.0277238488197327\n","199: 0.8861652314662933 0.3795503303408623 1.2657155618071556\n","200: 0.8085216283798218 0.38266848027706146 1.1911901086568832\n","201: 1.0346430242061615 0.3837119936943054 1.418355017900467\n","202: 1.2362658977508545 0.3635287582874298 1.5997946560382843\n","203: 0.9244136810302734 0.36924364417791367 1.293657325208187\n","204: 1.065119594335556 0.3687286972999573 1.4338482916355133\n","205: 1.1731703281402588 0.3707307502627373 1.543901078402996\n","206: 1.040010929107666 0.3738027662038803 1.4138136953115463\n","207: 0.6540617793798447 0.3785281181335449 1.0325898975133896\n","208: 1.3107716143131256 0.36519062519073486 1.6759622395038605\n","209: 1.0683228373527527 0.353714294731617 1.4220371320843697\n","210: 0.9001157879829407 0.3672671839594841 1.2673829719424248\n","211: 1.2251554429531097 0.38079919666051865 1.6059546396136284\n","212: 0.6903006732463837 0.38334444910287857 1.0736451223492622\n","213: 0.990972101688385 0.35655855387449265 1.3475306555628777\n","214: 1.333352506160736 0.36642177402973175 1.6997742801904678\n","215: 0.7955485731363297 0.3820968121290207 1.1776453852653503\n","216: 0.7872674763202667 0.380091167986393 1.1673586443066597\n","217: 1.153809368610382 0.3752511739730835 1.5290605425834656\n","218: 0.8629759550094604 0.3705609142780304 1.2335368692874908\n","219: 0.8963543474674225 0.3784162402153015 1.274770587682724\n","220: 1.2066084444522858 0.3669530898332596 1.5735615342855453\n","221: 1.1985156536102295 0.38635360449552536 1.5848692581057549\n","222: 0.9478863179683685 0.3606371060013771 1.3085234239697456\n","223: 0.8020621538162231 0.37127673625946045 1.1733388900756836\n","224: 1.2324674427509308 0.3649425655603409 1.5974100083112717\n","225: 1.0666986107826233 0.3680865615606308 1.434785172343254\n","226: 1.1716138124465942 0.3818359076976776 1.5534497201442719\n","227: 1.194841980934143 0.38697266578674316 1.5818146467208862\n","228: 0.9711750745773315 0.38768111169338226 1.3588561862707138\n","229: 1.0183655619621277 0.37110187113285065 1.3894674330949783\n","230: 0.6187971085309982 0.36936917901039124 0.9881662875413895\n","231: 0.8592159748077393 0.36311763525009155 1.2223336100578308\n","232: 0.8850338906049728 0.35808176547288895 1.2431156560778618\n","233: 0.9085376262664795 0.36857443302869797 1.2771120592951775\n","234: 0.5065299272537231 0.3815438598394394 0.8880737870931625\n","235: 0.8607529997825623 0.36424150317907333 1.2249945029616356\n","236: 1.059271216392517 0.36367230117321014 1.4229435175657272\n","237: 0.8494224846363068 0.374444916844368 1.2238674014806747\n","238: 1.0550034642219543 0.3539240434765816 1.408927507698536\n","239: 1.2906512022018433 0.354168102145195 1.6448193043470383\n","240: 0.9185848981142044 0.36601897329092026 1.2846038714051247\n","241: 1.3049847185611725 0.357881560921669 1.6628662794828415\n","242: 1.0003513097763062 0.370436392724514 1.3707877025008202\n","243: 1.2419981062412262 0.3670147955417633 1.6090129017829895\n","244: 1.1907010078430176 0.36582551151514053 1.5565265193581581\n","245: 0.8787897229194641 0.373745821416378 1.2525355443358421\n","246: 0.7638380527496338 0.37944498658180237 1.1432830393314362\n","247: 1.2068061530590057 0.3691237345337868 1.5759298875927925\n","248: 0.9499862343072891 0.3790011405944824 1.3289873749017715\n","249: 1.2938511967658997 0.36584313958883286 1.6596943363547325\n","250: 1.2064908146858215 0.3634127229452133 1.5699035376310349\n","251: 0.9708972126245499 0.37154508382081985 1.3424422964453697\n","252: 1.1435132324695587 0.3614993467926979 1.5050125792622566\n","253: 1.0631925612688065 0.36580271273851395 1.4289952740073204\n","254: 0.9163091033697128 0.3687693625688553 1.2850784659385681\n","255: 1.129417434334755 0.36753176152706146 1.4969491958618164\n","256: 1.170971542596817 0.3818162605166435 1.5527878031134605\n","257: 1.1600678861141205 0.3711281269788742 1.5311960130929947\n","258: 1.2648227214813232 0.36083922535181046 1.6256619468331337\n","259: 1.14915069937706 0.380998432636261 1.530149132013321\n","260: 0.9346567988395691 0.37186406552791595 1.306520864367485\n","261: 0.6532678157091141 0.3690120354294777 1.0222798511385918\n","262: 1.1382025182247162 0.3688697889447212 1.5070723071694374\n","263: 1.331338495016098 0.3681332916021347 1.6994717866182327\n","264: 1.3612151145935059 0.360995776951313 1.7222108915448189\n","265: 0.8992180824279785 0.35813207924366 1.2573501616716385\n","266: 1.141081541776657 0.3790566697716713 1.5201382115483284\n","267: 1.0542017221450806 0.37388889491558075 1.4280906170606613\n","268: 1.289847493171692 0.3746340945363045 1.6644815877079964\n","269: 0.9217871278524399 0.35453343391418457 1.2763205617666245\n","270: 1.1342942714691162 0.3686532527208328 1.502947524189949\n","271: 0.6164236813783646 0.3670690581202507 0.9834927394986153\n","272: 0.4985071122646332 0.3703540340065956 0.8688611462712288\n","273: 0.9889684319496155 0.35455797612667084 1.3435264080762863\n","274: 1.1778810918331146 0.36774516850709915 1.5456262603402138\n","275: 1.1867249310016632 0.3673193082213402 1.5540442392230034\n","276: 0.730913482606411 0.37102650851011276 1.1019399911165237\n","277: 1.086330622434616 0.3666951060295105 1.4530257284641266\n","278: 0.8764459490776062 0.3696836829185486 1.2461296319961548\n","279: 1.2535902857780457 0.3618542551994324 1.615444540977478\n","280: 1.0893144607543945 0.3616516962647438 1.4509661570191383\n","281: 0.8629438132047653 0.3702861815690994 1.2332299947738647\n","282: 1.354131817817688 0.36082910746335983 1.7149609252810478\n","283: 0.8559471219778061 0.3635048493742943 1.2194519713521004\n","284: 1.2836702167987823 0.37266603857278824 1.6563362553715706\n","285: 1.0197761058807373 0.36425454914569855 1.3840306550264359\n","286: 1.1844907402992249 0.3756280094385147 1.5601187497377396\n","287: 0.6670943796634674 0.3736433833837509 1.0407377630472183\n","288: 1.0482394099235535 0.3706044703722 1.4188438802957535\n","289: 1.003647804260254 0.37416692078113556 1.3778147250413895\n","290: 1.0800076127052307 0.3655157536268234 1.4455233663320541\n","291: 0.8595681935548782 0.3661388009786606 1.2257069945335388\n","292: 0.8442098051309586 0.35394078493118286 1.1981505900621414\n","293: 0.9307725727558136 0.3771839141845703 1.307956486940384\n","294: 1.243203043937683 0.36006879061460495 1.603271834552288\n","295: 1.0470918715000153 0.37099404633045197 1.4180859178304672\n","296: 1.2385783195495605 0.35842274874448776 1.5970010682940483\n","297: 0.7365690767765045 0.3674219846725464 1.103991061449051\n","298: 1.5244685113430023 0.3683313727378845 1.8927998840808868\n","299: 1.2008310556411743 0.37304675579071045 1.5738778114318848\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details"},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1624971640959,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"4976c61c-bcdd-44df-ab9d-8897f0376e43"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":null,"outputs":[{"output_type":"stream","text":["0.9780701754385965\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1624971640959,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"84e37674-2f7a-43c2-bfd1-166c2aa870df"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":null,"outputs":[{"output_type":"stream","text":["0.9824046920821115\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy"},"source":[""],"id":"preceding-galaxy","execution_count":null,"outputs":[]}]}
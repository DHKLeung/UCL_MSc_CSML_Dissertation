{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN aug WPL0.99.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1624971772362,"user_tz":-60,"elapsed":31008,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"3155d718-eb53-4b4f-f99a-74bf8f393cd6"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1624971773131,"user_tz":-60,"elapsed":772,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1624971773470,"user_tz":-60,"elapsed":340,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1624971773471,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"2da212bb-c2ab-4b04-a670-c11a5797db63"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","perturb_loss_weight = 0.99\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7faf1ab35a70>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1624971773471,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1624971780421,"user_tz":-60,"elapsed":6952,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"fff2d84b-4bb7-40fa-d40c-267337be08d8"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1624971780422,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    batch_size = labels.size(0)\n","    idx = torch.randperm(batch_size).to('cuda')\n","    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n","    labels_b = labels[idx]\n","    return mixup_inputs, labels, labels_b, lmbda"],"id":"quiet-module","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1624971780423,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1624971784061,"user_tz":-60,"elapsed":3642,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"defef445-7633-4c27-f1d6-2e1156d5b931"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup perturbation #\n","        mixup_inputs, mixup_labels_a, mixup_labels_b, lmbda = mixup_breast(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs = augment_outputs[original_num:]\n","        mixup_loss = mixup_criterion(criterion, mixup_outputs, mixup_labels_a, mixup_labels_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.143701136112213 2.150623857975006 4.294324994087219\n","1: 2.1241751313209534 2.1279155015945435 4.252090632915497\n","2: 2.0954782366752625 2.0876649022102356 4.183143138885498\n","3: 2.0459943413734436 2.031179964542389 4.0771743059158325\n","4: 1.9960755109786987 1.9682888984680176 3.9643644094467163\n","5: 1.9461207389831543 1.8934651017189026 3.839585840702057\n","6: 1.8866071105003357 1.8200817108154297 3.7066888213157654\n","7: 1.7952818274497986 1.7423062324523926 3.537588059902191\n","8: 1.7556187510490417 1.6611897349357605 3.4168084859848022\n","9: 1.675018548965454 1.5809034705162048 3.255922019481659\n","10: 1.6247117519378662 1.4952608048915863 3.1199725568294525\n","11: 1.4825010895729065 1.4005809128284454 2.883082002401352\n","12: 1.4648220539093018 1.2980517446994781 2.76287379860878\n","13: 1.403311401605606 1.2158343493938446 2.6191457509994507\n","14: 1.4372864663600922 1.118940532207489 2.556226998567581\n","15: 1.2912065088748932 1.0529418289661407 2.344148337841034\n","16: 1.1977302134037018 0.9627841413021088 2.1605143547058105\n","17: 1.336136817932129 0.9028747379779816 2.2390115559101105\n","18: 1.2403727769851685 0.8259457349777222 2.0663185119628906\n","19: 1.1839077472686768 0.7733772695064545 1.9572850167751312\n","20: 1.0143892467021942 0.7211139798164368 1.735503226518631\n","21: 1.2020542621612549 0.6747908890247345 1.8768451511859894\n","22: 1.1762621700763702 0.6415920406579971 1.8178542107343674\n","23: 1.1318967938423157 0.6057933419942856 1.7376901358366013\n","24: 1.0972584187984467 0.5849619209766388 1.6822203397750854\n","25: 1.280061274766922 0.5716104507446289 1.851671725511551\n","26: 1.092673271894455 0.549852043390274 1.642525315284729\n","27: 0.8563287854194641 0.5451408177614212 1.4014696031808853\n","28: 0.8031783699989319 0.5238460302352905 1.3270244002342224\n","29: 0.9967729449272156 0.5211207866668701 1.5178937315940857\n","30: 0.8921345770359039 0.49333474040031433 1.3854693174362183\n","31: 1.0876122117042542 0.4796188920736313 1.5672311037778854\n","32: 1.0821792483329773 0.4620717763900757 1.544251024723053\n","33: 1.0508024096488953 0.45072057843208313 1.5015229880809784\n","34: 1.001597598195076 0.45321130752563477 1.4548089057207108\n","35: 1.0935718715190887 0.439970463514328 1.5335423350334167\n","36: 0.937227800488472 0.4421238750219345 1.3793516755104065\n","37: 1.0874071419239044 0.4394434094429016 1.526850551366806\n","38: 0.7420936524868011 0.4314291179180145 1.1735227704048157\n","39: 1.1496820747852325 0.4273146986961365 1.576996773481369\n","40: 0.9245423674583435 0.4378765970468521 1.3624189645051956\n","41: 0.7299017310142517 0.4215029776096344 1.151404708623886\n","42: 1.234763354063034 0.41801978647708893 1.652783140540123\n","43: 0.9266873747110367 0.42093774676322937 1.347625121474266\n","44: 0.9340838938951492 0.41527020931243896 1.3493541032075882\n","45: 0.8973256796598434 0.41462818533182144 1.3119538649916649\n","46: 1.1393223106861115 0.41216422617435455 1.551486536860466\n","47: 0.9348697066307068 0.41264304518699646 1.3475127518177032\n","48: 1.2044251561164856 0.4056207835674286 1.6100459396839142\n","49: 1.1339292526245117 0.41616786271333694 1.5500971153378487\n","50: 0.9242282211780548 0.4155556485056877 1.3397838696837425\n","51: 1.1936479806900024 0.40800100564956665 1.601648986339569\n","52: 0.7757544368505478 0.4122793525457382 1.188033789396286\n","53: 0.9536450803279877 0.4149811640381813 1.368626244366169\n","54: 0.7040367424488068 0.39522115141153336 1.0992578938603401\n","55: 1.1828932464122772 0.3995833545923233 1.5824766010046005\n","56: 1.1569787859916687 0.39819444715976715 1.5551732331514359\n","57: 0.928119957447052 0.40381672978401184 1.3319366872310638\n","58: 1.1450228691101074 0.39676810055971146 1.5417909696698189\n","59: 1.0064674764871597 0.39843903481960297 1.4049065113067627\n","60: 1.094195157289505 0.4094207212328911 1.503615878522396\n","61: 0.9152004271745682 0.41337502002716064 1.3285754472017288\n","62: 1.0819911658763885 0.3924953565001488 1.4744865223765373\n","63: 1.0904927253723145 0.4057876169681549 1.4962803423404694\n","64: 0.9403883516788483 0.3980918675661087 1.338480219244957\n","65: 1.1018649637699127 0.4049592912197113 1.506824254989624\n","66: 1.1617085337638855 0.40358714759349823 1.5652956813573837\n","67: 1.1141494512557983 0.41539181023836136 1.5295412614941597\n","68: 0.8952213674783707 0.3991387411952019 1.2943601086735725\n","69: 0.9792481735348701 0.4026927798986435 1.3819409534335136\n","70: 1.0100509524345398 0.40853235125541687 1.4185833036899567\n","71: 0.917815774679184 0.41947560012340546 1.3372913748025894\n","72: 1.2639082968235016 0.40584561228752136 1.669753909111023\n","73: 0.9126799702644348 0.40583500266075134 1.3185149729251862\n","74: 1.0673415660858154 0.40382495522499084 1.4711665213108063\n","75: 0.8840523958206177 0.3938020169734955 1.2778544127941132\n","76: 1.2892652750015259 0.39658425748348236 1.6858495324850082\n","77: 1.1434473991394043 0.40912365913391113 1.5525710582733154\n","78: 1.1448631882667542 0.405721053481102 1.5505842417478561\n","79: 0.9791310131549835 0.41280795633792877 1.3919389694929123\n","80: 1.2089874744415283 0.4006913974881172 1.6096788719296455\n","81: 1.1213552504777908 0.4177500158548355 1.5391052663326263\n","82: 0.7090539485216141 0.4275387227535248 1.1365926712751389\n","83: 0.7430194914340973 0.40635306388139725 1.1493725553154945\n","84: 0.9999390989542007 0.4106644093990326 1.4106035083532333\n","85: 0.8435661494731903 0.40739937126636505 1.2509655207395554\n","86: 0.867482453584671 0.41042254120111465 1.2779049947857857\n","87: 1.3277996182441711 0.3932588994503021 1.7210585176944733\n","88: 0.8578319251537323 0.3992927819490433 1.2571247071027756\n","89: 1.1490382254123688 0.3857196494936943 1.534757874906063\n","90: 0.8241555690765381 0.3827330619096756 1.2068886309862137\n","91: 1.0169117450714111 0.3924935385584831 1.4094052836298943\n","92: 1.0461408495903015 0.3826146200299263 1.4287554696202278\n","93: 0.6983535885810852 0.38934890925884247 1.0877024978399277\n","94: 1.1408574879169464 0.3810301870107651 1.5218876749277115\n","95: 1.0285938382148743 0.3809661269187927 1.409559965133667\n","96: 0.903265506029129 0.37512414902448654 1.2783896550536156\n","97: 0.8343030363321304 0.37749846279621124 1.2118014991283417\n","98: 0.8293078541755676 0.37881961464881897 1.2081274688243866\n","99: 1.1007898449897766 0.3790466785430908 1.4798365235328674\n","100: 1.108882561326027 0.3800172284245491 1.488899789750576\n","101: 1.0909847915172577 0.3745182901620865 1.4655030816793442\n","102: 0.9509316384792328 0.3771398589015007 1.3280714973807335\n","103: 1.1058831810951233 0.3815435543656349 1.4874267354607582\n","104: 0.7538902312517166 0.37109214067459106 1.1249823719263077\n","105: 1.151870310306549 0.3732396811246872 1.5251099914312363\n","106: 0.8670596480369568 0.3707377761602402 1.237797424197197\n","107: 0.8586587607860565 0.3745826706290245 1.233241431415081\n","108: 0.8757911175489426 0.3687618002295494 1.244552917778492\n","109: 0.8504062294960022 0.37541136145591736 1.2258175909519196\n","110: 0.9584923088550568 0.3692832663655281 1.3277755752205849\n","111: 1.0304204523563385 0.3624877631664276 1.3929082155227661\n","112: 1.1167868375778198 0.3664037510752678 1.4831905886530876\n","113: 1.1502056121826172 0.37383002787828445 1.5240356400609016\n","114: 1.0559810400009155 0.37241795659065247 1.428398996591568\n","115: 0.8530997037887573 0.3765765279531479 1.2296762317419052\n","116: 0.9478333592414856 0.37655532360076904 1.3243886828422546\n","117: 0.7349070310592651 0.37997229397296906 1.1148793250322342\n","118: 1.1763128340244293 0.37815696746110916 1.5544698014855385\n","119: 0.7740573287010193 0.3696308434009552 1.1436881721019745\n","120: 0.5883029997348785 0.3759313225746155 0.964234322309494\n","121: 0.9231351912021637 0.37857113033533096 1.3017063215374947\n","122: 1.0008013248443604 0.3572876453399658 1.3580889701843262\n","123: 0.765719935297966 0.35782384872436523 1.1235437840223312\n","124: 0.49073147028684616 0.35453273355960846 0.8452642038464546\n","125: 1.2523212432861328 0.34536219388246536 1.5976834371685982\n","126: 0.7980916202068329 0.34738801419734955 1.1454796344041824\n","127: 1.1969485878944397 0.3462396711111069 1.5431882590055466\n","128: 0.8171949684619904 0.3573877885937691 1.1745827570557594\n","129: 1.113939642906189 0.33695514500141144 1.4508947879076004\n","130: 1.2110663950443268 0.33908867835998535 1.5501550734043121\n","131: 1.057841181755066 0.35001667588949203 1.407857857644558\n","132: 1.1380109190940857 0.3597484454512596 1.4977593645453453\n","133: 1.011130303144455 0.36840350925922394 1.379533812403679\n","134: 0.9495254158973694 0.3599097207188606 1.30943513661623\n","135: 1.301500827074051 0.3705800026655197 1.6720808297395706\n","136: 1.018139898777008 0.3747037947177887 1.3928436934947968\n","137: 1.1855226755142212 0.38736051321029663 1.5728831887245178\n","138: 1.1575933992862701 0.40108872205018997 1.5586821213364601\n","139: 1.0567196011543274 0.3952722102403641 1.4519918113946915\n","140: 1.2528848052024841 0.4116860032081604 1.6645708084106445\n","141: 0.9244376420974731 0.40984979271888733 1.3342874348163605\n","142: 0.8477533161640167 0.39781317114830017 1.245566487312317\n","143: 1.069009155035019 0.41126787662506104 1.48027703166008\n","144: 1.026678204536438 0.4180673882365227 1.4447455927729607\n","145: 0.8848230987787247 0.4009355679154396 1.2857586666941643\n","146: 0.8134431168437004 0.3927154839038849 1.2061586007475853\n","147: 0.6788877844810486 0.3947794511914253 1.073667235672474\n","148: 1.2267591655254364 0.39076942205429077 1.6175285875797272\n","149: 0.8332269787788391 0.38106807321310043 1.2142950519919395\n","150: 0.7940221428871155 0.37387897819280624 1.1679011210799217\n","151: 1.2368056774139404 0.36315641552209854 1.599962092936039\n","152: 1.1540168225765228 0.37173668295145035 1.5257535055279732\n","153: 0.8525896966457367 0.3760991245508194 1.228688821196556\n","154: 1.0562145709991455 0.378415122628212 1.4346296936273575\n","155: 1.0961577743291855 0.3725782334804535 1.468736007809639\n","156: 1.0498612225055695 0.3675401285290718 1.4174013510346413\n","157: 0.8547351956367493 0.3757801055908203 1.2305153012275696\n","158: 0.9820434153079987 0.3733570799231529 1.3554004952311516\n","159: 1.1780399680137634 0.37378326058387756 1.551823228597641\n","160: 0.9195785820484161 0.3677269369363785 1.2873055189847946\n","161: 1.0150448083877563 0.3745401054620743 1.3895849138498306\n","162: 0.8023174107074738 0.37031687051057816 1.172634281218052\n","163: 1.1217879056930542 0.37970492243766785 1.501492828130722\n","164: 0.909832164645195 0.3853386715054512 1.2951708361506462\n","165: 0.6718826144933701 0.377762109041214 1.049644723534584\n","166: 0.7328581362962723 0.3773512467741966 1.110209383070469\n","167: 1.0796163082122803 0.3803401291370392 1.4599564373493195\n","168: 0.7110676318407059 0.3767843022942543 1.0878519341349602\n","169: 1.0811011493206024 0.3746650665998459 1.4557662159204483\n","170: 0.9859573543071747 0.3763387128710747 1.3622960671782494\n","171: 1.0361942946910858 0.37026356160640717 1.406457856297493\n","172: 0.7872965931892395 0.3649219274520874 1.152218520641327\n","173: 0.7560515403747559 0.37724095582962036 1.1332924962043762\n","174: 1.1139949262142181 0.3857187032699585 1.4997136294841766\n","175: 1.185751959681511 0.36169084906578064 1.5474428087472916\n","176: 0.8681361228227615 0.3685976415872574 1.236733764410019\n","177: 0.6906283497810364 0.3738110065460205 1.0644393563270569\n","178: 1.232239007949829 0.38120226562023163 1.6134412735700607\n","179: 0.7308099567890167 0.3664604350924492 1.097270391881466\n","180: 1.2195540964603424 0.3638792335987091 1.5834333300590515\n","181: 0.9664364904165268 0.37089671194553375 1.3373332023620605\n","182: 0.9520925581455231 0.37497401237487793 1.327066570520401\n","183: 0.5970356464385986 0.3708181008696556 0.9678537473082542\n","184: 1.239321768283844 0.37609846144914627 1.6154202297329903\n","185: 1.1091224551200867 0.379937008023262 1.4890594631433487\n","186: 0.5651268661022186 0.36902108043432236 0.934147946536541\n","187: 1.166483998298645 0.3622004836797714 1.5286844819784164\n","188: 1.059836983680725 0.3642222583293915 1.4240592420101166\n","189: 1.008175790309906 0.36589619517326355 1.3740719854831696\n","190: 1.2196314632892609 0.3728233054280281 1.592454768717289\n","191: 1.034245878458023 0.36204811930656433 1.3962939977645874\n","192: 0.6279873251914978 0.3777126595377922 1.00569998472929\n","193: 0.8237763345241547 0.37494678795337677 1.1987231224775314\n","194: 0.9589967429637909 0.36380382627248764 1.3228005692362785\n","195: 1.14748215675354 0.3721664175391197 1.5196485742926598\n","196: 1.1316299736499786 0.3818878084421158 1.5135177820920944\n","197: 1.092635452747345 0.3734697997570038 1.4661052525043488\n","198: 0.6269379407167435 0.3745695650577545 1.001507505774498\n","199: 1.0040989071130753 0.37023767828941345 1.3743365854024887\n","200: 0.796910896897316 0.37038087099790573 1.1672917678952217\n","201: 0.8874428570270538 0.371850810945034 1.2592936679720879\n","202: 1.1645543575286865 0.367221437394619 1.5317757949233055\n","203: 0.91814786195755 0.36814800649881363 1.2862958684563637\n","204: 1.056280642747879 0.3819340541958809 1.43821469694376\n","205: 1.0910914242267609 0.3712204545736313 1.4623118788003922\n","206: 0.9697687923908234 0.36892982572317123 1.3386986181139946\n","207: 0.5979277044534683 0.3743666484951973 0.9722943529486656\n","208: 1.1621878743171692 0.35848525166511536 1.5206731259822845\n","209: 1.0879177749156952 0.37656381726264954 1.4644815921783447\n","210: 0.9060104489326477 0.3751014322042465 1.2811118811368942\n","211: 0.9901270270347595 0.36951491981744766 1.3596419468522072\n","212: 0.6273267716169357 0.38753532618284225 1.014862097799778\n","213: 1.0008493512868881 0.37009193003177643 1.3709412813186646\n","214: 1.3373962044715881 0.36445170640945435 1.7018479108810425\n","215: 0.8578994572162628 0.36190247535705566 1.2198019325733185\n","216: 0.7374378889799118 0.3619479015469551 1.099385790526867\n","217: 1.1542795598506927 0.3699537366628647 1.5242332965135574\n","218: 0.8159627318382263 0.37586383521556854 1.1918265670537949\n","219: 0.8613718897104263 0.3556521385908127 1.217024028301239\n","220: 1.0659110844135284 0.36832499504089355 1.434236079454422\n","221: 1.0914892256259918 0.3653797507286072 1.456868976354599\n","222: 0.8392398953437805 0.3636820316314697 1.2029219269752502\n","223: 0.7495648413896561 0.3773607760667801 1.1269256174564362\n","224: 1.1579575836658478 0.3779859244823456 1.5359435081481934\n","225: 0.9961740374565125 0.36343494802713394 1.3596089854836464\n","226: 1.028100699186325 0.3800335079431534 1.4081342071294785\n","227: 1.0448114573955536 0.363518550992012 1.4083300083875656\n","228: 0.8727912902832031 0.3586588725447655 1.2314501628279686\n","229: 1.0283633470535278 0.36295071244239807 1.391314059495926\n","230: 0.6552870869636536 0.3720736429095268 1.0273607298731804\n","231: 0.8445502817630768 0.3677630126476288 1.2123132944107056\n","232: 0.8637528419494629 0.3720083683729172 1.23576121032238\n","233: 0.8170029520988464 0.37084661424160004 1.1878495663404465\n","234: 0.49841736257076263 0.37635327130556107 0.8747706338763237\n","235: 0.8034840375185013 0.3706200122833252 1.1741040498018265\n","236: 0.9232165962457657 0.3647363483905792 1.287952944636345\n","237: 0.8187685459852219 0.36627981811761856 1.1850483641028404\n","238: 1.020934209227562 0.362271249294281 1.383205458521843\n","239: 1.1870290637016296 0.37097425013780594 1.5580033138394356\n","240: 0.8678280413150787 0.37010657787323 1.2379346191883087\n","241: 1.192508041858673 0.3654487654566765 1.5579568073153496\n","242: 0.923620268702507 0.3644924908876419 1.288112759590149\n","243: 1.1414574682712555 0.3652336820960045 1.50669115036726\n","244: 1.1525400876998901 0.35082487016916275 1.5033649578690529\n","245: 0.8126628696918488 0.35352660715579987 1.1661894768476486\n","246: 0.7123599275946617 0.356283463537693 1.0686433911323547\n","247: 1.1099077761173248 0.3734161779284477 1.4833239540457726\n","248: 0.9472509175539017 0.3708089739084244 1.318059891462326\n","249: 1.1386966705322266 0.3718349486589432 1.5105316191911697\n","250: 1.1500076353549957 0.3569776266813278 1.5069852620363235\n","251: 0.9463904798030853 0.37310582399368286 1.3194963037967682\n","252: 1.099476471543312 0.3651935085654259 1.464669980108738\n","253: 0.9355920255184174 0.3658888638019562 1.3014808893203735\n","254: 0.862795889377594 0.3569619804620743 1.2197578698396683\n","255: 1.0539699494838715 0.3612992540001869 1.4152692034840584\n","256: 1.0741179287433624 0.3588871657848358 1.4330050945281982\n","257: 1.2822706699371338 0.37454114109277725 1.656811811029911\n","258: 1.1618764400482178 0.3571716770529747 1.5190481171011925\n","259: 1.0680806338787079 0.37408534437417984 1.4421659782528877\n","260: 0.8919177353382111 0.3614821881055832 1.2533999234437943\n","261: 0.6336706429719925 0.37007025629282 1.0037408992648125\n","262: 1.0097694993019104 0.3637649491429329 1.3735344484448433\n","263: 1.2181623876094818 0.36606214195489883 1.5842245295643806\n","264: 1.2473130822181702 0.35578104108572006 1.6030941233038902\n","265: 0.9262386411428452 0.3692049905657768 1.295443631708622\n","266: 1.0696191191673279 0.36690156906843185 1.4365206882357597\n","267: 1.0748129487037659 0.3676968291401863 1.4425097778439522\n","268: 1.1911804378032684 0.3742004260420799 1.5653808638453484\n","269: 0.8656775206327438 0.36473551392555237 1.2304130345582962\n","270: 1.026989921927452 0.38210756331682205 1.4090974852442741\n","271: 0.6187694668769836 0.36572548747062683 0.9844949543476105\n","272: 0.4705671891570091 0.3638601079583168 0.8344272971153259\n","273: 0.9243506640195847 0.36852776259183884 1.2928784266114235\n","274: 1.1057128757238388 0.3685939237475395 1.4743067994713783\n","275: 1.1495917439460754 0.37203119695186615 1.5216229408979416\n","276: 0.6735780462622643 0.3580671474337578 1.031645193696022\n","277: 1.0409823954105377 0.3847000449895859 1.4256824404001236\n","278: 0.7856041491031647 0.3716197609901428 1.1572239100933075\n","279: 1.1560331583023071 0.36467308551073074 1.5207062438130379\n","280: 1.1162649393081665 0.3743961751461029 1.4906611144542694\n","281: 0.7321352660655975 0.3751405254006386 1.1072757914662361\n","282: 1.2342019379138947 0.35636264085769653 1.5905645787715912\n","283: 0.8212091326713562 0.3774106726050377 1.198619805276394\n","284: 1.1235195696353912 0.37217454612255096 1.4956941157579422\n","285: 0.9183256924152374 0.3767196089029312 1.2950453013181686\n","286: 1.1832297146320343 0.36576085537672043 1.5489905700087547\n","287: 0.6447670757770538 0.37595749646425247 1.0207245722413063\n","288: 1.0109025537967682 0.377256840467453 1.3881593942642212\n","289: 0.9623415172100067 0.3671577572822571 1.3294992744922638\n","290: 1.0872202217578888 0.3707148879766464 1.4579351097345352\n","291: 0.8836986571550369 0.3713507503271103 1.2550494074821472\n","292: 0.7732640355825424 0.3683517798781395 1.141615815460682\n","293: 0.8807386159896851 0.36375271528959274 1.2444913312792778\n","294: 1.0080626010894775 0.36652912199497223 1.3745917230844498\n","295: 0.9013132601976395 0.3664342761039734 1.2677475363016129\n","296: 1.0879240334033966 0.35345330834388733 1.441377341747284\n","297: 0.673007920384407 0.3656124621629715 1.0386203825473785\n","298: 1.3056174516677856 0.36982812732458115 1.6754455789923668\n","299: 1.2063478231430054 0.3724760264158249 1.5788238495588303\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1624971784062,"user_tz":-60,"elapsed":15,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1624971784062,"user_tz":-60,"elapsed":14,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"f2835e59-dc65-4c35-ce66-23f754363dd5"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9736842105263158\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1624971784063,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"c1473ad7-843b-4989-d8a0-2eb18d679c4c"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9882697947214076\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1624971784063,"user_tz":-60,"elapsed":7,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":12,"outputs":[]}]}
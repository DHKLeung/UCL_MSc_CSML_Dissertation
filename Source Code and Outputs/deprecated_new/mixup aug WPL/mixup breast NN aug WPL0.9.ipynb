{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN aug WPL0.9.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1624971725465,"user_tz":-60,"elapsed":16000,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"2983def9-e48d-4a75-b6b8-f80202c08d7f"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5"},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W"},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1624971726689,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"e6617f66-773e-47c4-c758-7bb41a2d6756"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","perturb_loss_weight = 0.9\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7ff13ced0a50>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule"},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1624971733108,"user_tz":-60,"elapsed":6421,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"e413b57e-a981-4901-9398-98d91c951d75"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module"},"source":["def mixup_breast(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    batch_size = labels.size(0)\n","    idx = torch.randperm(batch_size).to('cuda')\n","    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n","    labels_b = labels[idx]\n","    return mixup_inputs, labels, labels_b, lmbda"],"id":"quiet-module","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions"},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1624971735956,"user_tz":-60,"elapsed":2850,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"cfb83fc0-4c87-4df0-dace-b4afeec22947"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup perturbation #\n","        mixup_inputs, mixup_labels_a, mixup_labels_b, lmbda = mixup_breast(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs = augment_outputs[original_num:]\n","        mixup_loss = mixup_criterion(criterion, mixup_outputs, mixup_labels_a, mixup_labels_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.1336777806282043 2.138392448425293 4.272070229053497\n","1: 2.114541232585907 2.1136826872825623 4.228223919868469\n","2: 2.0801615715026855 2.073118507862091 4.153280079364777\n","3: 2.0343751907348633 2.01762056350708 4.051995754241943\n","4: 1.9810308814048767 1.9524022340774536 3.9334331154823303\n","5: 1.9376389384269714 1.8774563670158386 3.81509530544281\n","6: 1.881745994091034 1.8030694723129272 3.684815466403961\n","7: 1.807210624217987 1.726330280303955 3.533540904521942\n","8: 1.7574201226234436 1.6533096432685852 3.410729765892029\n","9: 1.67957204580307 1.5664142370224 3.24598628282547\n","10: 1.62049400806427 1.475210040807724 3.095704048871994\n","11: 1.4670150578022003 1.389530211687088 2.8565452694892883\n","12: 1.4763184189796448 1.2918388843536377 2.7681573033332825\n","13: 1.400887817144394 1.2085337936878204 2.6094216108322144\n","14: 1.453137755393982 1.106365978717804 2.559503734111786\n","15: 1.2673243582248688 1.022609680891037 2.2899340391159058\n","16: 1.2279779016971588 0.9590457677841187 2.1870236694812775\n","17: 1.3387555181980133 0.8725378513336182 2.2112933695316315\n","18: 1.2489672303199768 0.8131720125675201 2.062139242887497\n","19: 1.2252044379711151 0.7674661576747894 1.9926705956459045\n","20: 1.0540654957294464 0.7048611342906952 1.7589266300201416\n","21: 1.2390904128551483 0.6686552166938782 1.9077456295490265\n","22: 1.233971506357193 0.6361096352338791 1.870081141591072\n","23: 1.1814872026443481 0.6136311441659927 1.7951183468103409\n","24: 1.1547607779502869 0.5805180817842484 1.7352788597345352\n","25: 1.3494446575641632 0.5587783306837082 1.9082229882478714\n","26: 1.0665838420391083 0.5472128838300705 1.6137967258691788\n","27: 0.8802762925624847 0.5315234810113907 1.4117997735738754\n","28: 0.8428919315338135 0.524066150188446 1.3669580817222595\n","29: 0.9323112964630127 0.5035749822854996 1.4358862787485123\n","30: 0.9851470291614532 0.49638064205646515 1.4815276712179184\n","31: 1.0697960257530212 0.468194842338562 1.5379908680915833\n","32: 1.1035145223140717 0.4665158540010452 1.5700303763151169\n","33: 1.0524441301822662 0.46936507523059845 1.5218092054128647\n","34: 1.0612354576587677 0.4474620074033737 1.5086974650621414\n","35: 1.172515630722046 0.4452793449163437 1.6177949756383896\n","36: 0.9126035571098328 0.43499717116355896 1.3476007282733917\n","37: 1.0866553485393524 0.4336724877357483 1.5203278362751007\n","38: 0.7836724668741226 0.42584458738565445 1.209517054259777\n","39: 1.1194685995578766 0.41611435264348984 1.5355829522013664\n","40: 1.0773826986551285 0.4107035771012306 1.488086275756359\n","41: 0.7285849004983902 0.41057001054286957 1.1391549110412598\n","42: 1.1201401054859161 0.41930752992630005 1.5394476354122162\n","43: 1.0967951565980911 0.4040200710296631 1.5008152276277542\n","44: 0.916283518075943 0.39173048734664917 1.3080140054225922\n","45: 0.7926159054040909 0.39857184886932373 1.1911877542734146\n","46: 1.1271971464157104 0.4037664830684662 1.5309636294841766\n","47: 0.9513646364212036 0.3882461339235306 1.3396107703447342\n","48: 1.1389869451522827 0.3930964097380638 1.5320833548903465\n","49: 1.3223875164985657 0.3865973949432373 1.708984911441803\n","50: 0.910255029797554 0.3879028409719467 1.2981578707695007\n","51: 1.251935750246048 0.39621323347091675 1.6481489837169647\n","52: 0.7628884017467499 0.40154115855693817 1.164429560303688\n","53: 1.0000418722629547 0.39246556907892227 1.392507441341877\n","54: 0.8380263447761536 0.3868725523352623 1.2248988971114159\n","55: 1.2805266380310059 0.39310799539089203 1.6736346334218979\n","56: 1.1616151332855225 0.3880046606063843 1.5496197938919067\n","57: 1.0043996274471283 0.39755305647850037 1.4019526839256287\n","58: 1.216955840587616 0.38849740475416183 1.6054532453417778\n","59: 0.9841383695602417 0.4016723334789276 1.3858107030391693\n","60: 1.1694048643112183 0.3924276828765869 1.5618325471878052\n","61: 0.948181688785553 0.4000413939356804 1.3482230827212334\n","62: 1.1833471357822418 0.39986203610897064 1.5832091718912125\n","63: 1.0753443241119385 0.39345820248126984 1.4688025265932083\n","64: 0.9710453152656555 0.3966720476746559 1.3677173629403114\n","65: 1.1085903644561768 0.3967171683907509 1.5053075328469276\n","66: 1.0820971727371216 0.3881509527564049 1.4702481254935265\n","67: 1.0623424351215363 0.3956127017736435 1.4579551368951797\n","68: 0.8405092805624008 0.39828525483608246 1.2387945353984833\n","69: 0.9236605763435364 0.40511637181043625 1.3287769481539726\n","70: 1.000889152288437 0.3932139128446579 1.3941030651330948\n","71: 0.9196668863296509 0.39135052263736725 1.3110174089670181\n","72: 1.1402824521064758 0.39029738306999207 1.530579835176468\n","73: 1.0348621904850006 0.39386287331581116 1.4287250638008118\n","74: 1.1468809843063354 0.3889830410480499 1.5358640253543854\n","75: 0.9283305555582047 0.3803764432668686 1.3087069988250732\n","76: 1.3344886004924774 0.3875578045845032 1.7220464050769806\n","77: 1.1346329152584076 0.38754531741142273 1.5221782326698303\n","78: 1.1884836852550507 0.3867521733045578 1.5752358585596085\n","79: 1.028053179383278 0.3891156390309334 1.4171688184142113\n","80: 1.2568629086017609 0.39776183664798737 1.6546247452497482\n","81: 1.1594959050416946 0.3929984122514725 1.5524943172931671\n","82: 0.6665008813142776 0.3988102003931999 1.0653110817074776\n","83: 0.7815296351909637 0.4012669697403908 1.1827966049313545\n","84: 1.0026125609874725 0.3856569603085518 1.3882695212960243\n","85: 0.9844514727592468 0.39278432726860046 1.3772358000278473\n","86: 0.9493474066257477 0.3782406747341156 1.3275880813598633\n","87: 1.2276675701141357 0.3832612782716751 1.6109288483858109\n","88: 0.8539015799760818 0.38793279230594635 1.2418343722820282\n","89: 1.2165250182151794 0.37634214758872986 1.5928671658039093\n","90: 0.8260261118412018 0.3685730919241905 1.1945992037653923\n","91: 1.0262196511030197 0.3794017881155014 1.4056214392185211\n","92: 1.0412684082984924 0.3650129586458206 1.406281366944313\n","93: 0.6952722072601318 0.376952700316906 1.0722249075770378\n","94: 1.0931775569915771 0.3734544366598129 1.46663199365139\n","95: 1.125743806362152 0.3655983433127403 1.4913421496748924\n","96: 0.9646106064319611 0.3766072392463684 1.3412178456783295\n","97: 0.8644525706768036 0.36148054897785187 1.2259331196546555\n","98: 0.8212441653013229 0.3604954183101654 1.1817395836114883\n","99: 1.1670110821723938 0.3597906306385994 1.5268017128109932\n","100: 1.2021440267562866 0.36025356501340866 1.5623975917696953\n","101: 1.079201340675354 0.3698439970612526 1.4490453377366066\n","102: 1.0175787210464478 0.365636482834816 1.3832152038812637\n","103: 1.1283179819583893 0.37069886922836304 1.4990168511867523\n","104: 0.7586106359958649 0.36948875337839127 1.1280993893742561\n","105: 1.2868878841400146 0.36224471032619476 1.6491325944662094\n","106: 0.8227278590202332 0.37408388406038284 1.196811743080616\n","107: 0.9242942631244659 0.3637605309486389 1.2880547940731049\n","108: 0.9142767488956451 0.369039848446846 1.2833165973424911\n","109: 0.8163376152515411 0.3592120632529259 1.175549678504467\n","110: 1.04653000831604 0.36016735434532166 1.4066973626613617\n","111: 1.003537341952324 0.3525438606739044 1.3560812026262283\n","112: 1.1401174068450928 0.3572982922196388 1.4974156990647316\n","113: 1.0366742014884949 0.36365874856710434 1.4003329500555992\n","114: 1.1445470750331879 0.36421995609998703 1.508767031133175\n","115: 0.8826375007629395 0.3654298782348633 1.2480673789978027\n","116: 0.9251176565885544 0.3580838292837143 1.2832014858722687\n","117: 0.7523394376039505 0.3564211279153824 1.1087605655193329\n","118: 1.2741667032241821 0.3548004478216171 1.6289671510457993\n","119: 0.7741875499486923 0.3510431796312332 1.1252307295799255\n","120: 0.5520275458693504 0.34000804275274277 0.8920355886220932\n","121: 0.985316202044487 0.356405571103096 1.341721773147583\n","122: 1.0951748043298721 0.34233879297971725 1.4375135973095894\n","123: 0.864061713218689 0.3408983424305916 1.2049600556492805\n","124: 0.4689723029732704 0.3378053605556488 0.8067776635289192\n","125: 1.3826530873775482 0.337415374815464 1.7200684621930122\n","126: 0.8258565068244934 0.331478513777256 1.1573350206017494\n","127: 1.1460468769073486 0.3364078253507614 1.48245470225811\n","128: 0.8332407772541046 0.3236997649073601 1.1569405421614647\n","129: 1.1794852912425995 0.3304600194096565 1.509945310652256\n","130: 1.2081518471240997 0.3463209718465805 1.5544728189706802\n","131: 1.0226263999938965 0.34132101386785507 1.3639474138617516\n","132: 1.2565814852714539 0.33870013803243637 1.5952816233038902\n","133: 1.0292736887931824 0.3569362312555313 1.3862099200487137\n","134: 1.013491153717041 0.35516469925642014 1.3686558529734612\n","135: 1.228057324886322 0.3605140298604965 1.5885713547468185\n","136: 1.1963421702384949 0.37572238594293594 1.5720645561814308\n","137: 1.148363620042801 0.3679225966334343 1.5162862166762352\n","138: 1.234428495168686 0.3726441189646721 1.607072614133358\n","139: 1.204599142074585 0.3793126046657562 1.5839117467403412\n","140: 1.2402290999889374 0.3841693475842476 1.624398447573185\n","141: 0.9013102799654007 0.40241266041994095 1.3037229403853416\n","142: 0.8548923283815384 0.4030004069209099 1.2578927353024483\n","143: 1.0510459244251251 0.3803269490599632 1.4313728734850883\n","144: 1.0053540468215942 0.3965509831905365 1.4019050300121307\n","145: 0.8574980273842812 0.3920198380947113 1.2495178654789925\n","146: 0.7249342799186707 0.3783326745033264 1.103266954421997\n","147: 0.6549128144979477 0.36943210661411285 1.0243449211120605\n","148: 1.1426298916339874 0.3587747663259506 1.501404657959938\n","149: 0.8384910225868225 0.3570050522685051 1.1954960748553276\n","150: 0.7889596074819565 0.3560503348708153 1.1450099423527718\n","151: 1.2929843068122864 0.3438306376338005 1.6368149444460869\n","152: 1.1338828206062317 0.34794799983501434 1.481830820441246\n","153: 0.9639159440994263 0.36534979194402695 1.3292657360434532\n","154: 1.0616251826286316 0.34668806940317154 1.4083132520318031\n","155: 1.1059933006763458 0.36012157797813416 1.46611487865448\n","156: 1.2177525758743286 0.3540630340576172 1.5718156099319458\n","157: 0.8405888080596924 0.36486538499593735 1.2054541930556297\n","158: 1.0551887452602386 0.3504195064306259 1.4056082516908646\n","159: 1.2959850132465363 0.35429243743419647 1.6502774506807327\n","160: 1.075051873922348 0.3593524470925331 1.4344043210148811\n","161: 1.000211626291275 0.3626174181699753 1.3628290444612503\n","162: 0.736771360039711 0.363361194729805 1.100132554769516\n","163: 1.0791830718517303 0.3666333109140396 1.44581638276577\n","164: 1.000162661075592 0.3621673732995987 1.3623300343751907\n","165: 0.6362278908491135 0.3525596410036087 0.9887875318527222\n","166: 0.7283568531274796 0.36345284432172775 1.0918096974492073\n","167: 1.0919274389743805 0.3510354310274124 1.442962870001793\n","168: 0.6638487130403519 0.3602498918771744 1.0240986049175262\n","169: 1.2331616282463074 0.3420000374317169 1.5751616656780243\n","170: 1.040171429514885 0.35331354290246964 1.3934849724173546\n","171: 1.0352135300636292 0.3482278510928154 1.3834413811564445\n","172: 0.7892786115407944 0.36001601070165634 1.1492946222424507\n","173: 0.7668890953063965 0.3486053943634033 1.1154944896697998\n","174: 1.2278540432453156 0.35194727033376694 1.5798013135790825\n","175: 1.1048577427864075 0.3545755073428154 1.4594332501292229\n","176: 0.8376867473125458 0.3456406593322754 1.1833274066448212\n","177: 0.6916042864322662 0.3491230905056 1.0407273769378662\n","178: 1.202702820301056 0.3511774390935898 1.5538802593946457\n","179: 0.7025641202926636 0.34694528579711914 1.0495094060897827\n","180: 1.2953860461711884 0.3542204648256302 1.6496065109968185\n","181: 1.002492070198059 0.3508750721812248 1.353367142379284\n","182: 0.9785613715648651 0.3547337055206299 1.333295077085495\n","183: 0.5760969519615173 0.3508053198456764 0.9269022718071938\n","184: 1.2082927525043488 0.3500802144408226 1.5583729669451714\n","185: 0.9484705179929733 0.3506413698196411 1.2991118878126144\n","186: 0.5529336482286453 0.34419112652540207 0.8971247747540474\n","187: 1.1911218464374542 0.34976714104413986 1.540888987481594\n","188: 1.059584230184555 0.3598715290427208 1.4194557592272758\n","189: 1.069593071937561 0.3586578518152237 1.4282509237527847\n","190: 1.2731792628765106 0.3426903635263443 1.615869626402855\n","191: 1.162854164838791 0.36177996546030045 1.5246341302990913\n","192: 0.5929863005876541 0.36103949695825577 0.9540257975459099\n","193: 0.784353494644165 0.3512670323252678 1.1356205269694328\n","194: 1.057104006409645 0.353740394115448 1.410844400525093\n","195: 1.1629460155963898 0.3611530065536499 1.5240990221500397\n","196: 1.1491103172302246 0.34114182740449905 1.4902521446347237\n","197: 1.056456983089447 0.3505578637123108 1.4070148468017578\n","198: 0.6029275059700012 0.3607565611600876 0.9636840671300888\n","199: 0.9744281768798828 0.355237141251564 1.3296653181314468\n","200: 0.772571325302124 0.34837618470191956 1.1209475100040436\n","201: 1.006986916065216 0.3604590743780136 1.3674459904432297\n","202: 1.1504591405391693 0.3526509255170822 1.5031100660562515\n","203: 0.8336488306522369 0.34463145583868027 1.1782802864909172\n","204: 1.0150585174560547 0.35084276646375656 1.3659012839198112\n","205: 1.147676557302475 0.35632045567035675 1.5039970129728317\n","206: 0.9347268640995026 0.359133742749691 1.2938606068491936\n","207: 0.6352031081914902 0.3586144745349884 0.9938175827264786\n","208: 1.231812745332718 0.3561941608786583 1.5880069062113762\n","209: 1.041224166750908 0.34405288845300674 1.3852770552039146\n","210: 0.8600548505783081 0.3488014191389084 1.2088562697172165\n","211: 1.0446984320878983 0.35299913585186005 1.3976975679397583\n","212: 0.6311332881450653 0.34406473487615585 0.9751980230212212\n","213: 0.9154807031154633 0.35629820823669434 1.2717789113521576\n","214: 1.2466259896755219 0.3527207225561142 1.599346712231636\n","215: 0.80785071849823 0.3506055325269699 1.1584562510252\n","216: 0.8189230859279633 0.3421218544244766 1.1610449403524399\n","217: 1.1648471653461456 0.3559758812189102 1.5208230465650558\n","218: 0.796490877866745 0.35283228754997253 1.1493231654167175\n","219: 0.9453313499689102 0.3529009222984314 1.2982322722673416\n","220: 1.125343233346939 0.3500448539853096 1.4753880873322487\n","221: 1.2027204036712646 0.34685108810663223 1.5495714917778969\n","222: 0.8950323760509491 0.3407968357205391 1.2358292117714882\n","223: 0.7744561731815338 0.34116265177726746 1.1156188249588013\n","224: 1.1029827892780304 0.3479508012533188 1.4509335905313492\n","225: 1.073691576719284 0.3569049686193466 1.4305965453386307\n","226: 0.9722955822944641 0.3379255384206772 1.3102211207151413\n","227: 1.0765671133995056 0.3437856361269951 1.4203527495265007\n","228: 0.9897145330905914 0.35563182830810547 1.345346361398697\n","229: 1.1615168452262878 0.34855135530233383 1.5100682005286217\n","230: 0.5948988795280457 0.35911641269922256 0.9540152922272682\n","231: 0.8544852435588837 0.345174603164196 1.1996598467230797\n","232: 0.8498578071594238 0.3469005897641182 1.196758396923542\n","233: 0.8640503287315369 0.34639979898929596 1.2104501277208328\n","234: 0.4537670463323593 0.3416922837495804 0.7954593300819397\n","235: 0.8189554661512375 0.35771431028842926 1.1766697764396667\n","236: 1.0678998306393623 0.35611946880817413 1.4240192994475365\n","237: 0.7869216650724411 0.35246044397354126 1.1393821090459824\n","238: 1.0169971585273743 0.34647002816200256 1.3634671866893768\n","239: 1.2415370643138885 0.35749849677085876 1.5990355610847473\n","240: 1.0131048634648323 0.34293293207883835 1.3560377955436707\n","241: 1.2643440067768097 0.3445020169019699 1.6088460236787796\n","242: 0.9045939743518829 0.34113337844610214 1.245727352797985\n","243: 1.1074443459510803 0.3440561518073082 1.4515004977583885\n","244: 1.2334973812103271 0.3443208783864975 1.5778182595968246\n","245: 0.8308098614215851 0.34430889785289764 1.1751187592744827\n","246: 0.6654442399740219 0.34035176038742065 1.0057960003614426\n","247: 1.1693341732025146 0.34193164855241776 1.5112658217549324\n","248: 0.9841509908437729 0.35034143179655075 1.3344924226403236\n","249: 1.2164206504821777 0.3470083475112915 1.5634289979934692\n","250: 1.2588368952274323 0.35304585844278336 1.6118827536702156\n","251: 0.9262131601572037 0.3484024628996849 1.2746156230568886\n","252: 0.9562195092439651 0.336327962577343 1.2925474718213081\n","253: 0.9612545073032379 0.3474538028240204 1.3087083101272583\n","254: 0.8928504139184952 0.35353169590234756 1.2463821098208427\n","255: 0.9458368420600891 0.3449602723121643 1.2907971143722534\n","256: 1.180134117603302 0.35662128776311874 1.5367554053664207\n","257: 1.0871500968933105 0.35588308423757553 1.443033181130886\n","258: 1.168200820684433 0.34833384305238724 1.5165346637368202\n","259: 1.1283428370952606 0.35113096982240677 1.4794738069176674\n","260: 0.8649977296590805 0.3456944078207016 1.210692137479782\n","261: 0.6664955168962479 0.34969740360975266 1.0161929205060005\n","262: 1.0798166394233704 0.3524032235145569 1.4322198629379272\n","263: 1.2668262422084808 0.34671445935964584 1.6135407015681267\n","264: 1.2517964243888855 0.34484005719423294 1.5966364815831184\n","265: 0.9259348809719086 0.35121776908636093 1.2771526500582695\n","266: 1.1885162889957428 0.34829141199588776 1.5368077009916306\n","267: 1.1957954168319702 0.3670775443315506 1.5628729611635208\n","268: 1.1111049056053162 0.3511017933487892 1.4622066989541054\n","269: 0.9218506366014481 0.34678614884614944 1.2686367854475975\n","270: 0.9292771220207214 0.35652901977300644 1.2858061417937279\n","271: 0.617137223482132 0.3530260771512985 0.9701633006334305\n","272: 0.4663738012313843 0.34897010028362274 0.815343901515007\n","273: 1.0272821187973022 0.35744039714336395 1.3847225159406662\n","274: 1.09194415807724 0.3482406586408615 1.4401848167181015\n","275: 1.125468522310257 0.353009931743145 1.478478454053402\n","276: 0.7072805985808372 0.3580823391675949 1.0653629377484322\n","277: 1.0178321301937103 0.3427940830588341 1.3606262132525444\n","278: 0.8227389454841614 0.3420882970094681 1.1648272424936295\n","279: 1.225244164466858 0.3430013731122017 1.5682455375790596\n","280: 1.2147472500801086 0.36050422489643097 1.5752514749765396\n","281: 0.8172653317451477 0.3380858302116394 1.155351161956787\n","282: 1.1787749528884888 0.34213926643133163 1.5209142193198204\n","283: 0.7680097669363022 0.34572525322437286 1.113735020160675\n","284: 1.1650833487510681 0.3484208881855011 1.5135042369365692\n","285: 0.9587906002998352 0.34442563354969025 1.3032162338495255\n","286: 1.1112821400165558 0.35268667340278625 1.463968813419342\n","287: 0.644699901342392 0.34064508229494095 0.9853449836373329\n","288: 1.0872099101543427 0.34934548288583755 1.4365553930401802\n","289: 1.0604912638664246 0.3379049599170685 1.398396223783493\n","290: 1.1929564774036407 0.36054517328739166 1.5535016506910324\n","291: 0.9032765626907349 0.3445001542568207 1.2477767169475555\n","292: 0.8267153203487396 0.3484552353620529 1.1751705557107925\n","293: 0.8039335310459137 0.34615443646907806 1.1500879675149918\n","294: 1.0862370729446411 0.3500041216611862 1.4362411946058273\n","295: 0.9081692546606064 0.3524296134710312 1.2605988681316376\n","296: 1.2909823954105377 0.35084863752126694 1.6418310329318047\n","297: 0.6459494978189468 0.34101099520921707 0.9869604930281639\n","298: 1.4017595648765564 0.34698486328125 1.7487444281578064\n","299: 1.1089325845241547 0.3476537615060806 1.4565863460302353\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details"},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1624971735957,"user_tz":-60,"elapsed":11,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"493805ec-a3c9-4c51-9198-c5101ad18f61"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":null,"outputs":[{"output_type":"stream","text":["0.9736842105263158\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1624971735957,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"60d8e2cd-ce23-4bcf-cb37-300745143797"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":null,"outputs":[{"output_type":"stream","text":["0.9853372434017595\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy"},"source":[""],"id":"preceding-galaxy","execution_count":null,"outputs":[]}]}
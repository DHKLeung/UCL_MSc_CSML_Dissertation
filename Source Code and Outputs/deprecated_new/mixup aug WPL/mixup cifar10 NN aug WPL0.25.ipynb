{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup cifar10 NN aug WPL0.25.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_b2Nvqg3Oifz"},"source":["# Import Libraries"],"id":"_b2Nvqg3Oifz"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"romantic-purchase","executionInfo":{"status":"ok","timestamp":1624993364845,"user_tz":-60,"elapsed":18612,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"a2a831e1-0127-401c-9c2a-e61be1b5fae4"},"source":["import torch\n","from torchvision import transforms, datasets\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"romantic-purchase","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qiPvZSTKO0Pk"},"source":["# Import outside code"],"id":"qiPvZSTKO0Pk"},{"cell_type":"code","metadata":{"id":"kw0NscKvO2bI","executionInfo":{"status":"ok","timestamp":1624993365845,"user_tz":-60,"elapsed":1003,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"kw0NscKvO2bI","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMYYosNOO2V3","executionInfo":{"status":"ok","timestamp":1624993366500,"user_tz":-60,"elapsed":657,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"sMYYosNOO2V3","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pg95DAmvO8un"},"source":["# Configuration"],"id":"pg95DAmvO8un"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silver-clear","executionInfo":{"status":"ok","timestamp":1624993366501,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"9864c2c1-c394-4fb1-8928-06f5556f95e3"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),  # can omit\n","    transforms.RandomHorizontalFlip(),  # can omit\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","batch_size = 128\n","step_size = 0.1\n","random_seed = 0\n","epochs = 200\n","L2_decay = 1e-4\n","alpha = 1.\n","perturb_loss_weight = 0.25\n","\n","torch.manual_seed(random_seed)"],"id":"silver-clear","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f4f3d47eab0>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"uDMInb7zPT3v"},"source":["# Data"],"id":"uDMInb7zPT3v"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"relative-mobility","executionInfo":{"status":"ok","timestamp":1624993377048,"user_tz":-60,"elapsed":10549,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"fdee453f-4e79-4a73-b02e-7dacadb4fe6c"},"source":["\"\"\"\n","Data\n","\"\"\"\n","train_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=True, download=True, transform=transform_train)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=False, download=True, transform=transform_test)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"relative-mobility","execution_count":5,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0-R2uhkAPjkv"},"source":["# Models, Loss, Optimiser"],"id":"0-R2uhkAPjkv"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"starting-chancellor","executionInfo":{"status":"ok","timestamp":1624993382916,"user_tz":-60,"elapsed":5870,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"018318ac-651e-4095-ce92-af7c2a5fb034"},"source":["model = ResNet18()\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"starting-chancellor","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"1qFqkwNmQgeO"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"1qFqkwNmQgeO"},{"cell_type":"code","metadata":{"id":"contemporary-gross","executionInfo":{"status":"ok","timestamp":1624993382917,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_cifar10(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    batch_size = labels.size(0)\n","    idx = torch.randperm(batch_size).to('cuda')\n","    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n","    labels_b = labels[idx]\n","    return mixup_inputs, labels, labels_b, lmbda"],"id":"contemporary-gross","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"right-spending","executionInfo":{"status":"ok","timestamp":1624993382917,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"right-spending","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TlGDo8vIQoZk"},"source":["# Training"],"id":"TlGDo8vIQoZk"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"written-bookmark","executionInfo":{"status":"ok","timestamp":1625003181486,"user_tz":-60,"elapsed":9798572,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"7af73499-42c2-4ede-bca6-6c65686a0620"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","\n","        # Mixup perturbation #\n","        mixup_inputs, mixup_labels_a, mixup_labels_b, lmbda = mixup_cifar10(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs = augment_outputs[original_num:]\n","        mixup_loss = mixup_criterion(criterion, mixup_outputs, mixup_labels_a, mixup_labels_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"written-bookmark","execution_count":9,"outputs":[{"output_type":"stream","text":["0: 787.4170030355453 653.6367110013962 1441.0537140369415\n","1: 673.2479578256607 443.7641751766205 1117.0121330022812\n","2: 623.061054289341 339.0588132739067 962.1198675632477\n","3: 585.330440402031 266.6050142645836 851.9354546666145\n","4: 564.9289778470993 222.78302523493767 787.712003082037\n","5: 541.7559588849545 192.7963505089283 734.5523093938828\n","6: 517.0943786203861 170.8449724316597 687.9393510520458\n","7: 518.8398333787918 153.28203366696835 672.1218670457602\n","8: 496.86894100904465 138.83215859532356 635.7010996043682\n","9: 481.8761647939682 128.05427691340446 609.9304417073727\n","10: 493.4937511384487 115.99306087195873 609.4868120104074\n","11: 493.90359777212143 108.94922144711018 602.8528192192316\n","12: 478.12851333618164 101.06077907979488 579.1892924159765\n","13: 462.8308353126049 95.08077016472816 557.9116054773331\n","14: 476.9469761252403 90.12921616435051 567.0761922895908\n","15: 463.29848892986774 83.56324995309114 546.8617388829589\n","16: 477.0989359021187 80.41715806722641 557.5160939693451\n","17: 437.4746487736702 77.35381831228733 514.8284670859575\n","18: 457.03308464586735 71.27275504171848 528.3058396875858\n","19: 449.79929292201996 67.65474946051836 517.4540423825383\n","20: 448.41787898540497 67.01247514784336 515.4303541332483\n","21: 450.0381031483412 64.2219965159893 514.2600996643305\n","22: 449.20630626380444 60.57719209417701 509.78349835798144\n","23: 442.02400824427605 59.90497602894902 501.92898427322507\n","24: 452.14922021329403 57.69674323871732 509.84596345201135\n","25: 432.00326439738274 55.44706493988633 487.45032933726907\n","26: 447.7103930711746 54.51972083747387 502.2301139086485\n","27: 432.07701206207275 52.86031062155962 484.9373226836324\n","28: 429.868861541152 51.90716143511236 481.77602297626436\n","29: 448.9887179508805 50.46791201084852 499.45662996172905\n","30: 441.1506813764572 49.36221493408084 490.51289631053805\n","31: 436.55484043061733 47.619821425527334 484.17466185614467\n","32: 420.7602906227112 46.2838029935956 467.0440936163068\n","33: 426.8316679298878 46.94449816830456 473.77616609819233\n","34: 443.55232948064804 43.74328211508691 487.29561159573495\n","35: 429.9891395121813 45.71131628938019 475.7004558015615\n","36: 437.97677950561047 42.24191473610699 480.21869424171746\n","37: 425.06867779046297 42.15064803697169 467.21932582743466\n","38: 433.1790791749954 43.47340129688382 476.65248047187924\n","39: 421.84535138309 42.113722793757915 463.95907417684793\n","40: 413.508959248662 42.5662629455328 456.0752221941948\n","41: 428.34930704534054 39.97766141593456 468.3269684612751\n","42: 427.48110234737396 39.350281823426485 466.83138417080045\n","43: 427.5453834980726 40.025000063702464 467.5703835617751\n","44: 438.5895470418036 38.56629582867026 477.15584287047386\n","45: 430.5822182893753 40.18620702251792 470.7684253118932\n","46: 417.45990812033415 37.86390162073076 455.3238097410649\n","47: 415.76954524219036 36.95462794601917 452.72417318820953\n","48: 424.0831877216697 39.00977077335119 463.09295849502087\n","49: 414.2138693854213 36.96232644841075 451.176195833832\n","50: 425.86963564157486 37.02882496267557 462.89846060425043\n","51: 408.50725737959146 36.27869840338826 444.7859557829797\n","52: 420.92201685905457 35.43398936651647 456.35600622557104\n","53: 423.0187966749072 35.857009725645185 458.8758064005524\n","54: 420.688258305192 35.93141816556454 456.61967647075653\n","55: 412.36641328036785 37.53872167877853 449.9051349591464\n","56: 434.6337066590786 33.15488204266876 467.78858870174736\n","57: 422.0296384692192 35.502989226952195 457.5326276961714\n","58: 419.18409141898155 34.41714107990265 453.6012324988842\n","59: 419.28261268883944 34.424361856654286 453.7069745454937\n","60: 410.6042592599988 34.16380730830133 444.7680665683001\n","61: 424.8287965208292 34.03186954930425 458.86066607013345\n","62: 413.2288628295064 34.306373009458184 447.5352358389646\n","63: 419.13110687583685 35.76342302374542 454.89452989958227\n","64: 406.855567060411 35.38370734080672 442.2392744012177\n","65: 431.80310076475143 32.936407666653395 464.73950843140483\n","66: 408.73282665759325 34.672212675213814 443.40503933280706\n","67: 415.9159360229969 33.04351313225925 448.95944915525615\n","68: 409.1396686807275 32.68750057555735 441.82716925628483\n","69: 434.9741358682513 33.486300053074956 468.4604359213263\n","70: 415.87093748152256 30.9180101249367 446.78894760645926\n","71: 408.0777152553201 33.63455478474498 441.71227004006505\n","72: 403.1445716768503 34.609191907569766 437.7537635844201\n","73: 400.49784256517887 33.284601740539074 433.78244430571795\n","74: 421.3509579747915 33.58687327615917 454.9378312509507\n","75: 409.6950686722994 32.17411652486771 441.8691851971671\n","76: 419.59124913066626 32.886801635846496 452.47805076651275\n","77: 416.78366236388683 30.92875530384481 447.71241766773164\n","78: 409.47163231670856 32.4317786693573 441.90341098606586\n","79: 419.0293139144778 31.914207449182868 450.9435213636607\n","80: 421.6966470107436 32.61848733201623 454.31513434275985\n","81: 415.4882988706231 30.216293633915484 445.7045925045386\n","82: 411.1057514026761 30.73449228145182 441.8402436841279\n","83: 408.8234085217118 32.02114346716553 440.84455198887736\n","84: 407.5278125181794 31.499696891754866 439.0275094099343\n","85: 417.79618913680315 30.081879205070436 447.8780683418736\n","86: 424.1901028454304 32.23502071388066 456.42512355931103\n","87: 408.7819683626294 30.914190863259137 439.69615922588855\n","88: 421.8399232700467 31.831501180306077 453.6714244503528\n","89: 414.0166569054127 30.222131399437785 444.23878830485046\n","90: 420.30497436597943 30.369679931551218 450.67465429753065\n","91: 414.6816394776106 32.577114356681705 447.2587538342923\n","92: 406.8176531419158 29.89478532690555 436.71243846882135\n","93: 415.46818444132805 29.8774586096406 445.34564305096865\n","94: 402.39120199531317 31.267881328240037 433.6590833235532\n","95: 424.42227464541793 31.162941458635032 455.58521610405296\n","96: 402.3054902255535 31.991765807382762 434.2972560329363\n","97: 404.3165307752788 31.9554271902889 436.2719579655677\n","98: 401.9066835194826 32.14645288605243 434.05313640553504\n","99: 394.47268257290125 29.793313102796674 424.2659956756979\n","100: 399.3300815448165 12.509272768627852 411.83935431344435\n","101: 387.36881103087217 6.222138301935047 393.5909493328072\n","102: 367.00602504983544 5.008044841932133 372.0140698917676\n","103: 372.2745994273573 4.064793979050592 376.3393934064079\n","104: 368.83596348389983 3.630031363805756 372.4659948477056\n","105: 357.77683360502124 3.0696318718837574 360.846465476905\n","106: 372.42634416371584 3.0751195778138936 375.50146374152973\n","107: 366.4416679870337 2.8458827289287 369.2875507159624\n","108: 369.3577534304932 2.6058130810270086 371.9635665115202\n","109: 362.158779039979 2.70041035534814 364.8591893953271\n","110: 355.9889134671539 2.421399690094404 358.4103131572483\n","111: 363.27792473882437 2.406725594191812 365.6846503330162\n","112: 362.3341325307265 2.2562903793295845 364.5904229100561\n","113: 361.74337667785585 2.1350819086655974 363.87845858652145\n","114: 352.62131740618497 2.0720048191724345 354.6933222253574\n","115: 365.3271794989705 2.024490454583429 367.35166995355394\n","116: 357.4369272207841 2.120381658896804 359.55730887968093\n","117: 360.6058369195089 1.9983492783503607 362.60418619785924\n","118: 373.60132721811533 2.0659890344832093 375.66731625259854\n","119: 346.6464785290882 1.827099344925955 348.47357787401415\n","120: 348.695946238935 1.8365922824013978 350.5325385213364\n","121: 355.99976672045887 1.7284680570592172 357.7282347775181\n","122: 363.4825494289398 1.8950750915100798 365.3776245204499\n","123: 359.0948490202427 1.7538063824176788 360.84865540266037\n","124: 347.61323830857873 1.6168691600905731 349.2301074686693\n","125: 350.22714827023447 1.7359874412650242 351.9631357114995\n","126: 367.28976290114224 1.6565026441821828 368.9462655453244\n","127: 351.8556195124984 1.6552312895655632 353.51085080206394\n","128: 359.50827987492085 1.6907958257943392 361.1990757007152\n","129: 350.79838277958333 1.610631683259271 352.4090144628426\n","130: 333.37464414630085 1.6088570061838254 334.9835011524847\n","131: 364.8610589839518 1.6121674951864406 366.47322647913825\n","132: 347.08038364350796 1.5584841407835484 348.6388677842915\n","133: 345.77924979873933 1.6605038781417534 347.4397536768811\n","134: 354.97541653737426 1.5993475667200983 356.57476410409436\n","135: 347.31693939492106 1.61125078657642 348.9281901814975\n","136: 349.5600930042565 1.537442578584887 351.0975355828414\n","137: 334.7742418535054 1.493583042640239 336.2678248961456\n","138: 332.6298763155937 1.5935428806114942 334.2234191962052\n","139: 349.5413781218231 1.5205319812521338 351.0619101030752\n","140: 339.1858393829316 1.599873734288849 340.78571311722044\n","141: 349.05723352264613 1.5305042408872396 350.58773776353337\n","142: 344.52040931768715 1.505569325061515 346.02597864274867\n","143: 341.57667821645737 1.427707208902575 343.00438542535994\n","144: 336.0832298733294 1.4267492611543275 337.50997913448373\n","145: 338.93954370729625 1.4917499723378569 340.4312936796341\n","146: 358.726226137951 1.4578661745181307 360.18409231246915\n","147: 352.11642775684595 1.414763371925801 353.53119112877175\n","148: 342.10675852838904 1.4436229148413986 343.55038144323044\n","149: 339.02424260182306 1.445726883248426 340.4699694850715\n","150: 351.53975507058203 1.5113408028846607 353.0510958734667\n","151: 342.30375478602946 1.353193171788007 343.65694795781747\n","152: 351.8403586503118 1.4596996031468734 353.3000582534587\n","153: 335.4919503936544 1.441001973580569 336.932952367235\n","154: 341.59984754305333 1.3451130017638206 342.94496054481715\n","155: 333.1217912794091 1.3398214703192934 334.4616127497284\n","156: 341.6898734997958 1.3120068165007979 343.0018803162966\n","157: 344.8673158530146 1.3480931809172034 346.2154090339318\n","158: 342.8047881666571 1.3590134651167318 344.1638016317738\n","159: 345.57350757624954 1.3237785269739106 346.89728610322345\n","160: 330.4545393148437 1.2994962272932753 331.754035542137\n","161: 330.93088390119374 1.3600996586028486 332.2909835597966\n","162: 349.9816562458873 1.3299242358189076 351.3115804817062\n","163: 341.63762786611915 1.2889959311578423 342.926623797277\n","164: 331.29587332718074 1.310161188361235 332.606034515542\n","165: 336.31016530795023 1.3454831254202873 337.6556484333705\n","166: 344.8880567923188 1.3195861338172108 346.20764292613603\n","167: 340.04142409563065 1.2828083405038342 341.3242324361345\n","168: 335.94885707460344 1.2738364057149738 337.2226934803184\n","169: 335.747541946359 1.2822838047286496 337.0298257510876\n","170: 344.9553747288883 1.3906497835414484 346.3460245124297\n","171: 334.2354988832958 1.2852116042049602 335.52071048750076\n","172: 325.17498062271625 1.2326481424388476 326.4076287651551\n","173: 317.3441293127835 1.2210695806425065 318.565198893426\n","174: 340.82120894640684 1.2743427401874214 342.09555168659426\n","175: 335.0868363426998 1.2436701818951406 336.33050652459497\n","176: 329.43843414261937 1.3100416873348877 330.74847582995426\n","177: 335.08740100171417 1.3013078105868772 336.38870881230105\n","178: 346.2325517712161 1.2479051928967237 347.4804569641128\n","179: 335.7478192169219 1.3453131810529158 337.09313239797484\n","180: 333.5755852609873 1.2200823887251318 334.7956676497124\n","181: 343.75575689412653 1.234977637825068 344.9907345319516\n","182: 339.2870389558375 1.232650769467 340.5196897253045\n","183: 338.5573899401352 1.2067456220975146 339.7641355622327\n","184: 319.9390995623544 1.203770320629701 321.1428698829841\n","185: 340.12374290823936 1.2685740772867575 341.3923169855261\n","186: 332.4399212785065 1.1657459727139212 333.60566725122044\n","187: 330.9405185766518 1.171709794551134 332.11222837120295\n","188: 318.43550707027316 1.3057954218238592 319.741302492097\n","189: 332.08407257683575 1.1984179808059707 333.2824905576417\n","190: 337.552223354578 1.1791139635606669 338.7313373181387\n","191: 325.7446173406206 1.1209640087326989 326.8655813493533\n","192: 325.55265116505325 1.1135869272984564 326.6662380923517\n","193: 329.00081711634994 1.1811799620627426 330.1819970784127\n","194: 317.95181175507605 1.1233491310849786 319.07516088616103\n","195: 332.3484146222472 1.1439703369396739 333.4923849591869\n","196: 338.5546340383589 1.1738275477546267 339.72846158611355\n","197: 345.54827826749533 1.1016621015151031 346.64994036901044\n","198: 331.3276412989944 1.169896651234012 332.49753795022843\n","199: 331.44081700593233 1.173646197130438 332.61446320306277\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"doR5kS81SW5F"},"source":["# Save model"],"id":"doR5kS81SW5F"},{"cell_type":"code","metadata":{"id":"frozen-damage","executionInfo":{"status":"ok","timestamp":1625003181487,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_cifar10')\n","# model = models.__dict__['ResNet18']()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_cifar10'))"],"id":"frozen-damage","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLlLnmpRSZAl"},"source":["# Test on Test Data"],"id":"BLlLnmpRSZAl"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aboriginal-lafayette","executionInfo":{"status":"ok","timestamp":1625003184772,"user_tz":-60,"elapsed":3290,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"0f6f8e41-5648-456e-ca4e-93742a32fe90"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"aboriginal-lafayette","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9553\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xHj0XxoMScbg"},"source":["# Test on Train Data"],"id":"xHj0XxoMScbg"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"therapeutic-orlando","executionInfo":{"status":"ok","timestamp":1625003205875,"user_tz":-60,"elapsed":21105,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"0462b8fe-b4b9-4796-a19f-b2fb85b08866"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"therapeutic-orlando","execution_count":12,"outputs":[{"output_type":"stream","text":["0.99998\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XCvQUWwMSetI","executionInfo":{"status":"ok","timestamp":1625003205875,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"XCvQUWwMSetI","execution_count":12,"outputs":[]}]}
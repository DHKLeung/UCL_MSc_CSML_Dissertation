{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1624970736970,"user_tz":-60,"elapsed":324,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"2babed2f-5af8-45dd-905a-623a374ed329"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":10,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1624970737772,"user_tz":-60,"elapsed":281,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1624970738154,"user_tz":-60,"elapsed":385,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1624970738154,"user_tz":-60,"elapsed":7,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"19935327-5d9f-4267-a843-da3a1bf06197"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f9170085a90>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1624970738155,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1624970738155,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"6fda2dd0-8742-4a75-f42b-8c8aa565048c"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"quiet-module","executionInfo":{"status":"ok","timestamp":1624970738155,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    batch_size = labels.size(0)\n","    idx = torch.randperm(batch_size).to('cuda')\n","    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n","    labels_b = labels[idx]\n","    return mixup_inputs, labels, labels_b, lmbda"],"id":"quiet-module","execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"opening-sessions","executionInfo":{"status":"ok","timestamp":1624970738156,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"opening-sessions","execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1624970740940,"user_tz":-60,"elapsed":2788,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"9cd4cb5b-7f18-46cd-b5dc-7dce3b4f7322"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup perturbation #\n","        mixup_inputs, mixup_labels_a, mixup_labels_b, lmbda = mixup_breast(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs = augment_outputs[original_num:]\n","        mixup_loss = mixup_criterion(criterion, mixup_outputs, mixup_labels_a, mixup_labels_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        augment_loss = mixup_loss + loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += augment_loss.item()\n","\n","        # Gradient Calculation & Optimisation #\n","        augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":18,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.1265034675598145 2.1323508620262146 4.258854269981384\n","1: 2.093610107898712 2.076211929321289 4.169822096824646\n","2: 2.012036383152008 1.982236623764038 3.9942729473114014\n","3: 1.9117406010627747 1.8590783476829529 3.7708189487457275\n","4: 1.8019902110099792 1.7154844999313354 3.51747465133667\n","5: 1.7272621989250183 1.5562486052513123 3.28351092338562\n","6: 1.5836297273635864 1.3816258013248444 2.9652554988861084\n","7: 1.407281905412674 1.2055354416370392 2.612817347049713\n","8: 1.2994039952754974 1.0302605032920837 2.329664468765259\n","9: 1.1808987259864807 0.8801208138465881 2.061019539833069\n","10: 1.109409898519516 0.7408319711685181 1.8502418994903564\n","11: 0.8254320621490479 0.6177297532558441 1.4431617856025696\n","12: 1.0424788296222687 0.5585745722055435 1.6010533571243286\n","13: 0.9711514413356781 0.48359163105487823 1.45474311709404\n","14: 1.2848126590251923 0.4358385354280472 1.7206512093544006\n","15: 0.8528616726398468 0.3822798728942871 1.2351415753364563\n","16: 0.9031426310539246 0.34889186173677444 1.2520345151424408\n","17: 1.4520753026008606 0.3381629213690758 1.7902382612228394\n","18: 1.0993620455265045 0.31979794055223465 1.4191599637269974\n","19: 1.1559682488441467 0.3019682690501213 1.4579365253448486\n","20: 0.7370217591524124 0.3009288012981415 1.037950575351715\n","21: 1.2382773756980896 0.2788842171430588 1.5171615779399872\n","22: 1.2337464094161987 0.2973547503352165 1.5311011373996735\n","23: 1.3526397943496704 0.27727537602186203 1.629915177822113\n","24: 1.1372366845607758 0.2850622981786728 1.4222989976406097\n","25: 1.4652235805988312 0.28971803188323975 1.7549415826797485\n","26: 1.0387122333049774 0.2730518653988838 1.311764121055603\n","27: 0.7838784903287888 0.2782386615872383 1.0621171593666077\n","28: 0.7344545423984528 0.27712704986333847 1.0115815699100494\n","29: 0.9502501636743546 0.26315775886178017 1.2134079188108444\n","30: 0.9856717735528946 0.2698177322745323 1.2554895281791687\n","31: 1.1851151883602142 0.2692650407552719 1.4543802440166473\n","32: 1.099214494228363 0.2565840184688568 1.3557985126972198\n","33: 0.9998297244310379 0.2465229146182537 1.246352642774582\n","34: 1.0769593566656113 0.25342830270528793 1.330387607216835\n","35: 1.1042831540107727 0.25251659005880356 1.3567997813224792\n","36: 0.9602191373705864 0.26003968715667725 1.2202588021755219\n","37: 1.1831762790679932 0.24051383882761002 1.423690140247345\n","38: 0.6560284644365311 0.25022290647029877 0.906251385807991\n","39: 1.0691447854042053 0.24724096804857254 1.3163857758045197\n","40: 1.095853939652443 0.2484743520617485 1.3443282842636108\n","41: 0.7406206354498863 0.24289186298847198 0.9835124909877777\n","42: 1.2441079020500183 0.2404720075428486 1.4845799207687378\n","43: 1.136691838502884 0.2318623811006546 1.3685542047023773\n","44: 0.8602034524083138 0.24285271018743515 1.103056162595749\n","45: 0.7644382417201996 0.23758796229958534 1.0020262002944946\n","46: 1.2036145329475403 0.2328997440636158 1.4365142583847046\n","47: 0.9253434538841248 0.24394187331199646 1.1692853271961212\n","48: 1.3060670495033264 0.23269692063331604 1.5387639701366425\n","49: 1.3908070027828217 0.23295801132917404 1.623764991760254\n","50: 0.9210731983184814 0.23176209628582 1.1528353095054626\n","51: 1.1846765577793121 0.2328675463795662 1.4175440669059753\n","52: 0.7543020769953728 0.2322300300002098 0.986532136797905\n","53: 1.016804352402687 0.24526632577180862 1.2620707154273987\n","54: 0.7300167307257652 0.22884520888328552 0.9588619470596313\n","55: 1.3028526902198792 0.23168446868658066 1.534537136554718\n","56: 1.2361660599708557 0.2295350767672062 1.4657011330127716\n","57: 0.9631385803222656 0.22582948952913284 1.1889680325984955\n","58: 1.4617214798927307 0.2324698492884636 1.6941913068294525\n","59: 1.040873035788536 0.22603943943977356 1.2669124901294708\n","60: 1.2652828097343445 0.23476503044366837 1.5000478625297546\n","61: 0.8877237215638161 0.2364524006843567 1.1241761445999146\n","62: 1.2262146472930908 0.24705440551042557 1.4732690453529358\n","63: 1.2029560804367065 0.2367699071764946 1.439725935459137\n","64: 1.0911322832107544 0.25023869425058365 1.3413709700107574\n","65: 1.2576609551906586 0.24083086848258972 1.4984918236732483\n","66: 1.1030773520469666 0.23985768854618073 1.342935025691986\n","67: 1.1821896135807037 0.23952854052186012 1.421718180179596\n","68: 0.801369309425354 0.246149443089962 1.0475187450647354\n","69: 0.9666683822870255 0.24404481798410416 1.2107132077217102\n","70: 1.1177291870117188 0.24645697325468063 1.3641861975193024\n","71: 0.9714350402355194 0.23838504403829575 1.2098200619220734\n","72: 1.2910663187503815 0.24149128794670105 1.5325576663017273\n","73: 1.0043883323669434 0.2462933585047722 1.2506817281246185\n","74: 1.1605085134506226 0.23674841970205307 1.3972569704055786\n","75: 0.9637790396809578 0.23303351551294327 1.1968125402927399\n","76: 1.453432947397232 0.23999448865652084 1.6934274435043335\n","77: 1.2615642547607422 0.23448022454977036 1.4960444867610931\n","78: 1.1790077984333038 0.24240876734256744 1.42141655087471\n","79: 0.9939909130334854 0.23855948448181152 1.2325504124164581\n","80: 1.368051528930664 0.24105389416217804 1.609105408191681\n","81: 1.1933850049972534 0.25469573587179184 1.4480807483196259\n","82: 0.5530413314700127 0.24333671852946281 0.7963780462741852\n","83: 0.6899534910917282 0.2502656802535057 0.9402191638946533\n","84: 0.9529869258403778 0.24379084259271622 1.1967777907848358\n","85: 1.052549660205841 0.22902869060635567 1.28157839179039\n","86: 0.9151566326618195 0.22234710678458214 1.1375037133693695\n","87: 1.4987337589263916 0.22276946157217026 1.72150319814682\n","88: 0.8026717230677605 0.22766801714897156 1.0303397178649902\n","89: 1.3913970589637756 0.21971027553081512 1.611107349395752\n","90: 0.9640979170799255 0.2253497615456581 1.1894477009773254\n","91: 1.01591295003891 0.22219378501176834 1.2381067276000977\n","92: 1.0318703651428223 0.2315986156463623 1.2634689807891846\n","93: 0.6575784683227539 0.21975407004356384 0.8773325532674789\n","94: 1.133978694677353 0.21890459954738617 1.3528832495212555\n","95: 1.1943625211715698 0.22055687010288239 1.4149194061756134\n","96: 0.9728061258792877 0.21466632932424545 1.1874724626541138\n","97: 0.8372208252549171 0.2286464348435402 1.0658672600984573\n","98: 0.8150896430015564 0.21617531776428223 1.031264990568161\n","99: 1.1449724435806274 0.21408100053668022 1.3590534627437592\n","100: 1.3125920593738556 0.21230946481227875 1.5249015092849731\n","101: 1.1539984345436096 0.20922937244176865 1.3632278144359589\n","102: 1.0893467664718628 0.21607810258865356 1.305424839258194\n","103: 1.1713274717330933 0.21181998774409294 1.3831474781036377\n","104: 0.8209018260240555 0.22556552663445473 1.046467363834381\n","105: 1.5165596008300781 0.21716828644275665 1.7337278723716736\n","106: 0.8790404796600342 0.21634525805711746 1.095385730266571\n","107: 0.8732361644506454 0.2174244187772274 1.0906605869531631\n","108: 0.9146204888820648 0.2288282960653305 1.1434487998485565\n","109: 0.7979492545127869 0.22159577906131744 1.0195450484752655\n","110: 1.1209252774715424 0.21020053699612617 1.3311257809400558\n","111: 1.1846172362565994 0.21470840275287628 1.399325668811798\n","112: 1.2508930563926697 0.21280226111412048 1.4636953175067902\n","113: 1.167009174823761 0.2168978452682495 1.3839070200920105\n","114: 1.1238975822925568 0.21650701016187668 1.3404045701026917\n","115: 0.8977996557950974 0.21346992999315262 1.1112695634365082\n","116: 0.9689522534608841 0.22308291494846344 1.1920351684093475\n","117: 0.7167602032423019 0.2150217443704605 0.9317819327116013\n","118: 1.3839066624641418 0.2151038646697998 1.5990105271339417\n","119: 0.7379344999790192 0.21256173402071 0.9504962265491486\n","120: 0.47475726157426834 0.21773066371679306 0.6924879252910614\n","121: 0.9667448997497559 0.199813574552536 1.1665584743022919\n","122: 0.9645467549562454 0.2032712735235691 1.1678180396556854\n","123: 0.8919391334056854 0.20429528877139091 1.0962344110012054\n","124: 0.39830954372882843 0.19909602403640747 0.5974055826663971\n","125: 1.3399419486522675 0.19534727558493614 1.535289227962494\n","126: 0.8266182243824005 0.19270887598395348 1.0193271040916443\n","127: 1.4147330224514008 0.18153389915823936 1.5962669849395752\n","128: 0.965160146355629 0.19066891819238663 1.1558290868997574\n","129: 1.0878734588623047 0.1889813430607319 1.2768548429012299\n","130: 1.2266027331352234 0.18953771144151688 1.4161404371261597\n","131: 1.1039775609970093 0.19771166145801544 1.3016892075538635\n","132: 1.3570471405982971 0.19816390424966812 1.555211067199707\n","133: 1.089715838432312 0.2017761431634426 1.291491985321045\n","134: 1.0933483839035034 0.20527403056621552 1.2986223995685577\n","135: 1.3986696004867554 0.21700580790638924 1.6156753897666931\n","136: 1.154877781867981 0.21862119063735008 1.3734989762306213\n","137: 1.2300682663917542 0.2197127342224121 1.4497810006141663\n","138: 1.2590168714523315 0.23147085309028625 1.4904877543449402\n","139: 1.2483900487422943 0.23219409584999084 1.4805840849876404\n","140: 1.249804049730301 0.24440867453813553 1.4942127168178558\n","141: 0.9175186008214951 0.23627203702926636 1.153790608048439\n","142: 0.7803271114826202 0.23606327176094055 1.0163903832435608\n","143: 1.1058368682861328 0.23317451030015945 1.3390113711357117\n","144: 1.137571394443512 0.23088820278644562 1.3684596419334412\n","145: 0.9132967367768288 0.229722797870636 1.1430195271968842\n","146: 0.783549502491951 0.22046104073524475 1.0040105432271957\n","147: 0.6400344967842102 0.2214628830552101 0.8614974021911621\n","148: 1.236489713191986 0.2186737060546875 1.4551633894443512\n","149: 0.8231301456689835 0.20559877157211304 1.0287289321422577\n","150: 0.6906342208385468 0.20880843326449394 0.8994426429271698\n","151: 1.4028364717960358 0.2056829072535038 1.6085194051265717\n","152: 1.2731698155403137 0.2001422382891178 1.4733120501041412\n","153: 0.9360217303037643 0.21121516823768616 1.1472368836402893\n","154: 1.2026784718036652 0.20856928080320358 1.4112477600574493\n","155: 1.1660796999931335 0.2029326632618904 1.3690123558044434\n","156: 1.2530229091644287 0.21102596819400787 1.4640488624572754\n","157: 0.7889759540557861 0.20269249007105827 0.9916684329509735\n","158: 1.1049707531929016 0.21278756111860275 1.317758321762085\n","159: 1.3582909107208252 0.2174728363752365 1.5757637321949005\n","160: 1.076281100511551 0.2085392251610756 1.284820318222046\n","161: 0.9901462495326996 0.20066694542765617 1.190813198685646\n","162: 0.7424152344465256 0.20617971569299698 0.9485949277877808\n","163: 1.1404855847358704 0.20718354731798172 1.3476691246032715\n","164: 0.9194168001413345 0.20685099437832832 1.1262677907943726\n","165: 0.575406514108181 0.20764897018671036 0.7830554842948914\n","166: 0.6808209866285324 0.2075515165925026 0.8883725255727768\n","167: 1.0999670624732971 0.21244779974222183 1.3124148845672607\n","168: 0.6627101898193359 0.2074679508805275 0.8701781630516052\n","169: 1.1723319590091705 0.20836865156888962 1.3807005882263184\n","170: 1.2908779010176659 0.20574172586202621 1.4966196119785309\n","171: 1.1026004254817963 0.20683041960000992 1.309430867433548\n","172: 0.8551094233989716 0.20302791148424149 1.0581373572349548\n","173: 0.8186705112457275 0.20547613501548767 1.0241466611623764\n","174: 1.3517916202545166 0.20002122968435287 1.5518128871917725\n","175: 1.285144031047821 0.20607032626867294 1.4912143349647522\n","176: 0.830958366394043 0.21147648990154266 1.0424348711967468\n","177: 0.6199145019054413 0.2159293182194233 0.8358438163995743\n","178: 1.348927915096283 0.20403828099370003 1.5529661774635315\n","179: 0.6504857540130615 0.21350626647472382 0.8639920055866241\n","180: 1.5009260177612305 0.21195178478956223 1.7128777503967285\n","181: 1.2014569640159607 0.20804492384195328 1.4095018357038498\n","182: 0.9811870157718658 0.2148399017751217 1.1960269212722778\n","183: 0.4581339880824089 0.19964777678251266 0.6577817648649216\n","184: 1.3304031491279602 0.2054767720401287 1.535879909992218\n","185: 1.2664439976215363 0.2071058601140976 1.4735498428344727\n","186: 0.4473104327917099 0.2052256315946579 0.6525360494852066\n","187: 1.3666671514511108 0.2090074047446251 1.5756745338439941\n","188: 1.2304616868495941 0.21026811748743057 1.440729796886444\n","189: 0.8973164260387421 0.20323551073670387 1.1005519330501556\n","190: 1.441123127937317 0.20798925310373306 1.6491124033927917\n","191: 1.2720011621713638 0.21016643196344376 1.4821676313877106\n","192: 0.5077896788716316 0.21300482004880905 0.7207944989204407\n","193: 0.7361635714769363 0.2175956442952156 0.9537592232227325\n","194: 1.0374112129211426 0.20196835324168205 1.2393795549869537\n","195: 1.2953058779239655 0.20721890777349472 1.5025248229503632\n","196: 1.2027623355388641 0.2048797570168972 1.4076420664787292\n","197: 1.1522516012191772 0.2092188484966755 1.3614704310894012\n","198: 0.5718259662389755 0.22028233110904694 0.7921082973480225\n","199: 0.9753098487854004 0.20840102434158325 1.1837108433246613\n","200: 0.6893499791622162 0.20529238134622574 0.8946423381567001\n","201: 1.0328571051359177 0.20346637442708015 1.2363234907388687\n","202: 1.325891375541687 0.20341632887721062 1.5293077230453491\n","203: 0.9481080770492554 0.20817364752292633 1.156281739473343\n","204: 1.026357650756836 0.20282898098230362 1.2291866540908813\n","205: 1.150176152586937 0.20340009778738022 1.3535762131214142\n","206: 1.0689097046852112 0.20322579145431519 1.2721355259418488\n","207: 0.5388857573270798 0.21239561587572098 0.7512813806533813\n","208: 1.2765464186668396 0.20921583473682404 1.4857622385025024\n","209: 1.0659040212631226 0.21090605854988098 1.276810109615326\n","210: 0.841954842209816 0.20507077127695084 1.0470256209373474\n","211: 1.211447313427925 0.20193789154291153 1.413385197520256\n","212: 0.5398276895284653 0.21032508835196495 0.7501527667045593\n","213: 0.8886405825614929 0.20132878050208092 1.0899693667888641\n","214: 1.5062530636787415 0.21049650758504868 1.7167495489120483\n","215: 0.8600465059280396 0.20653395354747772 1.066580444574356\n","216: 0.7314459308981895 0.2142040655016899 0.9456500262022018\n","217: 1.3853780031204224 0.20476222783327103 1.5901402235031128\n","218: 0.7579646706581116 0.21069705486297607 0.9686617255210876\n","219: 0.9128460139036179 0.20606423541903496 1.1189102530479431\n","220: 1.1863642632961273 0.20716659724712372 1.3935308754444122\n","221: 1.1384674310684204 0.20837167650461197 1.3468391299247742\n","222: 0.908730074763298 0.21027617156505585 1.1190062463283539\n","223: 0.7506006807088852 0.2029844932258129 0.953585147857666\n","224: 1.2096875309944153 0.20752115920186043 1.4172086715698242\n","225: 1.15276700258255 0.20596333593130112 1.3587303161621094\n","226: 1.0998302698135376 0.20225775986909866 1.3020879924297333\n","227: 1.0385838150978088 0.19738338515162468 1.2359671890735626\n","228: 0.9874958693981171 0.20753832161426544 1.1950342059135437\n","229: 1.0742220282554626 0.20303939282894135 1.2772614359855652\n","230: 0.5402221381664276 0.21559705212712288 0.7558192014694214\n","231: 0.8067473024129868 0.19954270496964455 1.006290003657341\n","232: 1.020860180258751 0.2086802013218403 1.2295404225587845\n","233: 0.8873811662197113 0.19853132590651512 1.0859124958515167\n","234: 0.3617284297943115 0.2028726115822792 0.5646010339260101\n","235: 0.8528956919908524 0.20349537581205368 1.0563910901546478\n","236: 1.0047556161880493 0.20075631514191628 1.205511897802353\n","237: 0.7301015257835388 0.19821980968117714 0.9283213019371033\n","238: 1.0665543377399445 0.20020662993192673 1.2667609751224518\n","239: 1.2513188123703003 0.20117932558059692 1.4524981081485748\n","240: 1.048516035079956 0.199433833360672 1.2479498982429504\n","241: 1.2679995000362396 0.20037713646888733 1.4683766663074493\n","242: 0.963043600320816 0.20010574162006378 1.1631493419408798\n","243: 1.2568352222442627 0.2019520252943039 1.4587872326374054\n","244: 1.1560252010822296 0.200822614133358 1.3568477928638458\n","245: 0.7395280450582504 0.1988823264837265 0.9384103715419769\n","246: 0.6640964969992638 0.20921896770596504 0.8733154833316803\n","247: 1.2485873997211456 0.1942913644015789 1.4428787529468536\n","248: 0.918313056230545 0.207971453666687 1.1262844949960709\n","249: 1.1333682537078857 0.19899696484208107 1.3323652148246765\n","250: 1.3735429346561432 0.19995351508259773 1.5734964311122894\n","251: 0.9089311212301254 0.19959142804145813 1.1085225343704224\n","252: 1.0578755140304565 0.20506520569324493 1.262940764427185\n","253: 1.0624423697590828 0.20053696259856224 1.2629793137311935\n","254: 0.8376810848712921 0.19874699041247368 1.036428079009056\n","255: 1.082497239112854 0.2081899531185627 1.290687158703804\n","256: 1.1884199678897858 0.20501535013318062 1.3934353590011597\n","257: 1.1464969515800476 0.20029238611459732 1.3467893302440643\n","258: 1.2319143116474152 0.2005729265511036 1.4324872493743896\n","259: 1.1262482702732086 0.19788770750164986 1.3241359889507294\n","260: 0.9090347141027451 0.2093355506658554 1.1183702647686005\n","261: 0.5759909525513649 0.20160897821187973 0.7775999307632446\n","262: 1.0011521577835083 0.20559991151094437 1.2067520916461945\n","263: 1.5537026524543762 0.21128346771001816 1.7649860978126526\n","264: 1.3486945629119873 0.2032502330839634 1.5519448220729828\n","265: 0.9492799341678619 0.19885441660881042 1.1481343507766724\n","266: 1.2993475198745728 0.2057247906923294 1.5050723254680634\n","267: 1.128328561782837 0.20845702663064003 1.336785614490509\n","268: 1.129090130329132 0.20046952366828918 1.3295596241950989\n","269: 0.9411490857601166 0.20830673351883888 1.1494558155536652\n","270: 1.0749267637729645 0.20817922800779343 1.2831059992313385\n","271: 0.5222719311714172 0.20486686378717422 0.7271387875080109\n","272: 0.36620286107063293 0.21349716559052467 0.5797000378370285\n","273: 1.0278321504592896 0.20801852270960808 1.2358506917953491\n","274: 1.1793642938137054 0.20328742638230324 1.3826517462730408\n","275: 1.3029478788375854 0.2041717916727066 1.5071196854114532\n","276: 0.6587101146578789 0.2028302252292633 0.8615403473377228\n","277: 1.112936407327652 0.20457785576581955 1.3175142705440521\n","278: 0.8749340027570724 0.20234131440520287 1.0772753357887268\n","279: 1.3595016300678253 0.1988062970340252 1.5583079159259796\n","280: 1.2648184597492218 0.2017746940255165 1.4665931165218353\n","281: 0.7944014966487885 0.2012135423719883 0.9956150352954865\n","282: 1.3619197010993958 0.20069464668631554 1.5626143217086792\n","283: 0.745315283536911 0.20270933210849762 0.9480246007442474\n","284: 1.3183815777301788 0.20530615001916885 1.523687720298767\n","285: 1.064131110906601 0.20673348754644394 1.2708646059036255\n","286: 1.1725876480340958 0.20741752535104752 1.3800051808357239\n","287: 0.5548525303602219 0.20127597078680992 0.7561284899711609\n","288: 0.9638928771018982 0.2108958512544632 1.1747887432575226\n","289: 1.1842689663171768 0.21050814166665077 1.3947771191596985\n","290: 1.146848350763321 0.20956698060035706 1.356415331363678\n","291: 0.915494441986084 0.20231791958212852 1.1178123652935028\n","292: 0.8362779766321182 0.20309797301888466 1.0393759310245514\n","293: 0.838058739900589 0.21156036108732224 1.0496190786361694\n","294: 1.2316851317882538 0.2002822607755661 1.4319673776626587\n","295: 0.8388577848672867 0.20252245292067528 1.0413802415132523\n","296: 1.200322449207306 0.20224858447909355 1.402571052312851\n","297: 0.6215456873178482 0.20215455442667007 0.8237002491950989\n","298: 1.4957790076732635 0.20336271822452545 1.6991417407989502\n","299: 1.1903684735298157 0.2087317779660225 1.39910027384758\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1624970740941,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1624970741329,"user_tz":-60,"elapsed":391,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"5232a241-ff16-485c-a9c1-7148e1740027"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":20,"outputs":[{"output_type":"stream","text":["0.9649122807017544\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1624970741330,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"9d05d940-ddf3-44a7-81b2-77230f99b88b"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":21,"outputs":[{"output_type":"stream","text":["0.9941348973607038\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1624970741330,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":21,"outputs":[]}]}
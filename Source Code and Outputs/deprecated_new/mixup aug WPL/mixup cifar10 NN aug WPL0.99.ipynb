{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup cifar10 NN aug WPL0.99.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_b2Nvqg3Oifz"},"source":["# Import Libraries"],"id":"_b2Nvqg3Oifz"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"romantic-purchase","executionInfo":{"status":"ok","timestamp":1625005234015,"user_tz":-60,"elapsed":17592,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"d0bc253f-9c19-489c-e847-80aa71b6b2a5"},"source":["import torch\n","from torchvision import transforms, datasets\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"romantic-purchase","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qiPvZSTKO0Pk"},"source":["# Import outside code"],"id":"qiPvZSTKO0Pk"},{"cell_type":"code","metadata":{"id":"kw0NscKvO2bI","executionInfo":{"status":"ok","timestamp":1625005235312,"user_tz":-60,"elapsed":1299,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"kw0NscKvO2bI","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMYYosNOO2V3","executionInfo":{"status":"ok","timestamp":1625005235972,"user_tz":-60,"elapsed":663,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"sMYYosNOO2V3","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pg95DAmvO8un"},"source":["# Configuration"],"id":"pg95DAmvO8un"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silver-clear","executionInfo":{"status":"ok","timestamp":1625005235972,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"ee201da6-f0cd-4d6c-d4c7-265ad4030461"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),  # can omit\n","    transforms.RandomHorizontalFlip(),  # can omit\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","batch_size = 128\n","step_size = 0.1\n","random_seed = 0\n","epochs = 200\n","L2_decay = 1e-4\n","alpha = 1.\n","perturb_loss_weight = 0.99\n","\n","torch.manual_seed(random_seed)"],"id":"silver-clear","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f7288742a90>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"uDMInb7zPT3v"},"source":["# Data"],"id":"uDMInb7zPT3v"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"relative-mobility","executionInfo":{"status":"ok","timestamp":1625005249359,"user_tz":-60,"elapsed":13389,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"4e3cd9f9-2bd7-4f87-b710-bb40075126bf"},"source":["\"\"\"\n","Data\n","\"\"\"\n","train_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=True, download=True, transform=transform_train)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=False, download=True, transform=transform_test)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"relative-mobility","execution_count":5,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0-R2uhkAPjkv"},"source":["# Models, Loss, Optimiser"],"id":"0-R2uhkAPjkv"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"starting-chancellor","executionInfo":{"status":"ok","timestamp":1625005256630,"user_tz":-60,"elapsed":7272,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"752c905b-ab45-44bc-d574-63cd49a909fe"},"source":["model = ResNet18()\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"starting-chancellor","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"1qFqkwNmQgeO"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"1qFqkwNmQgeO"},{"cell_type":"code","metadata":{"id":"contemporary-gross","executionInfo":{"status":"ok","timestamp":1625005256630,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_cifar10(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    batch_size = labels.size(0)\n","    idx = torch.randperm(batch_size).to('cuda')\n","    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n","    labels_b = labels[idx]\n","    return mixup_inputs, labels, labels_b, lmbda"],"id":"contemporary-gross","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"right-spending","executionInfo":{"status":"ok","timestamp":1625005256631,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"right-spending","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TlGDo8vIQoZk"},"source":["# Training"],"id":"TlGDo8vIQoZk"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"written-bookmark","executionInfo":{"status":"ok","timestamp":1625015142364,"user_tz":-60,"elapsed":9885737,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"41e00158-abe2-4ccb-f4b5-e55383e67043"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","\n","        # Mixup perturbation #\n","        mixup_inputs, mixup_labels_a, mixup_labels_b, lmbda = mixup_cifar10(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs = augment_outputs[original_num:]\n","        mixup_loss = mixup_criterion(criterion, mixup_outputs, mixup_labels_a, mixup_labels_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"written-bookmark","execution_count":9,"outputs":[{"output_type":"stream","text":["0: 812.88740837574 849.9223599433899 1662.80976831913\n","1: 726.5244642496109 636.1745927333832 1362.699056982994\n","2: 672.2577512264252 532.1246441602707 1204.3823953866959\n","3: 626.6694341897964 457.5703845024109 1084.2398186922073\n","4: 592.4918067455292 394.79475408792496 987.2865608334541\n","5: 555.9104881882668 337.9047282934189 893.8152164816856\n","6: 530.3823844194412 303.93882125616074 834.321205675602\n","7: 521.7440506219864 273.69056355953217 795.4346141815186\n","8: 499.35159623622894 249.38968041539192 748.7412766516209\n","9: 483.04993802309036 231.36683037877083 714.4167684018612\n","10: 489.3200581371784 220.8068546950817 710.1269128322601\n","11: 488.6413472890854 212.64687424898148 701.2882215380669\n","12: 471.3331320285797 200.5930678844452 671.9261999130249\n","13: 458.39829206466675 192.0170014500618 650.4152935147285\n","14: 468.63206297159195 189.66194394230843 658.2940069139004\n","15: 456.4684672653675 179.33878907561302 635.8072563409805\n","16: 465.75926730036736 178.03484186530113 643.7941091656685\n","17: 431.8592674434185 166.79292330145836 598.6521907448769\n","18: 446.08003744482994 165.92501352727413 612.0050509721041\n","19: 440.65175372362137 163.35870456695557 604.0104582905769\n","20: 435.9295542836189 157.64955236017704 593.579106643796\n","21: 437.52399548888206 154.1599230915308 591.6839185804129\n","22: 434.41413763165474 152.13590873777866 586.5500463694334\n","23: 429.16986268758774 151.63127517700195 580.8011378645897\n","24: 434.77233220636845 150.15888845920563 584.9312206655741\n","25: 417.75850707292557 146.89307682216167 564.6515838950872\n","26: 429.97581201791763 145.1286020874977 575.1044141054153\n","27: 415.58501026034355 141.1826217919588 556.7676320523024\n","28: 412.63320526480675 140.73144252598286 553.3646477907896\n","29: 427.6693344116211 141.1981878876686 568.8675222992897\n","30: 421.79161520302296 139.28359998762608 561.075215190649\n","31: 418.2115507423878 135.92719154059887 554.1387422829866\n","32: 402.69382908940315 133.1668944656849 535.860723555088\n","33: 406.24827022850513 132.98156020790339 539.2298304364085\n","34: 421.6392779350281 134.94535240530968 556.5846303403378\n","35: 407.8063093870878 130.46266753971577 538.2689769268036\n","36: 414.6462898403406 131.98248507082462 546.6287749111652\n","37: 403.22093023359776 129.85618935525417 533.0771195888519\n","38: 409.9109427034855 126.21697650849819 536.1279192119837\n","39: 399.09782733023167 125.42390620708466 524.5217335373163\n","40: 392.6150806695223 122.07258335500956 514.6876640245318\n","41: 404.73771515488625 124.64993199706078 529.387647151947\n","42: 402.0893260240555 124.51878393441439 526.6081099584699\n","43: 401.53722493350506 120.5066244751215 522.0438494086266\n","44: 411.7303131520748 121.8020728379488 533.5323859900236\n","45: 401.82110349833965 118.13207387179136 519.953177370131\n","46: 395.07273054122925 115.16704393923283 510.2397744804621\n","47: 389.7241581529379 114.02863372117281 503.7527918741107\n","48: 395.1341313421726 117.8456824272871 512.9798137694597\n","49: 388.4175747334957 114.36386984586716 502.78144457936287\n","50: 395.7278423309326 115.08632031083107 510.8141626417637\n","51: 382.7812839895487 112.31146944314241 495.0927534326911\n","52: 392.431936070323 112.93052258342505 505.36245865374804\n","53: 392.6604615598917 110.38894911855459 503.0494106784463\n","54: 392.69859206676483 111.23906560242176 503.9376576691866\n","55: 383.8742405176163 109.63130635768175 493.505546875298\n","56: 402.08979384601116 114.73356620967388 516.823360055685\n","57: 390.7387783527374 109.48018896579742 500.21896731853485\n","58: 390.2234551012516 108.57502620667219 498.7984813079238\n","59: 390.6858888119459 107.96313425898552 498.64902307093143\n","60: 382.26515275239944 108.9840620085597 491.24921476095915\n","61: 393.2496944665909 110.99589598923922 504.2455904558301\n","62: 381.7559676915407 107.41076810657978 489.1667357981205\n","63: 385.92803855240345 108.24436052888632 494.17239908128977\n","64: 375.2656126022339 104.83490787446499 480.1005204766989\n","65: 396.7596453130245 110.1040618494153 506.8637071624398\n","66: 376.98062236607075 106.91557053476572 483.89619290083647\n","67: 383.36613468825817 105.26136909425259 488.62750378251076\n","68: 377.2138275206089 101.95741732418537 479.1712448447943\n","69: 397.8009983599186 107.84352820366621 505.6445265635848\n","70: 383.3013048097491 105.01805131882429 488.3193561285734\n","71: 372.7615397423506 101.98319640010595 474.74473614245653\n","72: 371.1725520044565 103.58496649563313 474.75751850008965\n","73: 370.812173768878 103.49106632173061 474.3032400906086\n","74: 386.7914588749409 104.88326561450958 491.67472448945045\n","75: 374.77968914061785 103.37303297966719 478.15272212028503\n","76: 382.601301163435 107.6887923553586 490.2900935187936\n","77: 383.0873148292303 102.4104620590806 485.4977768883109\n","78: 375.53538101911545 99.1337800398469 474.66916105896235\n","79: 381.98488487303257 98.02371073514223 480.0085956081748\n","80: 382.7358528673649 101.74935930222273 484.4852121695876\n","81: 378.74861396849155 100.03616267442703 478.7847766429186\n","82: 375.7120811790228 102.81237978488207 478.52446096390486\n","83: 372.9779584556818 100.92837280035019 473.906331256032\n","84: 372.8015447854996 99.17553965002298 471.97708443552256\n","85: 380.5233014822006 100.93179021030664 481.45509169250727\n","86: 384.6001138910651 102.2491509243846 486.8492648154497\n","87: 375.584330573678 97.70412012189627 473.2884506955743\n","88: 382.07994869351387 98.74147480726242 480.8214235007763\n","89: 377.1324710845947 100.64462453871965 477.7770956233144\n","90: 383.71846739947796 101.18853353708982 484.9070009365678\n","91: 375.1173407584429 100.36670301109552 475.4840437695384\n","92: 371.64084730297327 100.05863311886787 471.69948042184114\n","93: 379.4993888363242 102.83969466388226 482.33908350020647\n","94: 365.8972172290087 95.40410627052188 461.30132349953055\n","95: 384.32396027445793 98.27371379733086 482.5976740717888\n","96: 365.4002136439085 95.56461431831121 460.9648279622197\n","97: 366.2186027020216 97.26616422086954 463.48476692289114\n","98: 366.43887144327164 98.75435518100858 465.1932266242802\n","99: 361.8594856187701 93.56795248389244 455.42743810266256\n","100: 354.5568355321884 73.52417213842273 428.08100767061114\n","101: 342.881483130157 67.44486940465868 410.32635253481567\n","102: 325.20472199469805 63.08729089424014 388.2920128889382\n","103: 329.0120766758919 62.95442273840308 391.96649941429496\n","104: 326.7148586884141 60.714058481156826 387.4289171695709\n","105: 317.2195284925401 57.96262679807842 375.18215529061854\n","106: 328.20869387313724 58.690703224390745 386.899397097528\n","107: 322.8760726749897 57.47846559435129 380.354538269341\n","108: 324.6919620297849 56.69262859784067 381.3845906276256\n","109: 319.0077468473464 55.44210289232433 374.44984973967075\n","110: 313.3543394431472 54.51422982476652 367.8685692679137\n","111: 319.70996598154306 53.55415601283312 373.2641219943762\n","112: 319.75098852068186 54.02488852664828 373.77587704733014\n","113: 317.21531552635133 52.989308934658766 370.2046244610101\n","114: 310.45327115803957 51.6390816885978 362.09235284663737\n","115: 319.98438470996916 53.09588241018355 373.0802671201527\n","116: 313.4729984086007 51.93514544516802 365.4081438537687\n","117: 316.4687103740871 51.43451506923884 367.90322544332594\n","118: 327.75546491891146 52.53667332883924 380.2921382477507\n","119: 304.690156808123 48.76311637926847 353.45327318739146\n","120: 306.9698275141418 49.75224093999714 356.72206845413893\n","121: 312.067769439891 50.85326892323792 362.9210383631289\n","122: 319.4411872290075 51.07615416496992 370.5173413939774\n","123: 315.6286704316735 50.143005231395364 365.7716756630689\n","124: 305.4141868054867 48.822710449807346 354.236897255294\n","125: 307.6670708414167 49.11184293869883 356.77891378011554\n","126: 321.69237681105733 50.03386462107301 371.72624143213034\n","127: 309.66560028493404 48.27334281243384 357.9389430973679\n","128: 315.31232604011893 48.342165714129806 363.65449175424874\n","129: 307.4996211193502 47.49264875240624 354.99226987175643\n","130: 294.01672061905265 45.59259069990367 339.6093113189563\n","131: 318.7576317191124 48.04205221310258 366.799683932215\n","132: 304.83436888456345 45.91186739411205 350.7462362786755\n","133: 303.56378701049834 45.341029362753034 348.9048163732514\n","134: 312.972878344357 45.572651291266084 358.5455296356231\n","135: 305.2911246456206 45.276738532818854 350.56786317843944\n","136: 307.00145675614476 46.215711942873895 353.21716869901866\n","137: 295.28301388490945 43.39620332792401 338.67921721283346\n","138: 292.4879419133067 43.64289083238691 336.1308327456936\n","139: 307.7099116053432 45.639956516213715 353.34986812155694\n","140: 297.802467379719 44.064007895067334 341.86647527478635\n","141: 306.7079201824963 44.24591833911836 350.95383852161467\n","142: 303.7628336381167 43.77742947731167 347.5402631154284\n","143: 299.95211639255285 43.92538321763277 343.8774996101856\n","144: 295.5691671464592 43.12865521851927 338.6978223649785\n","145: 298.94963408634067 43.15574646927416 342.10538055561483\n","146: 314.7399413511157 45.80876444932073 360.54870580043644\n","147: 310.17052124999464 45.08257889561355 355.2531001456082\n","148: 301.65486733801663 43.46298567857593 345.11785301659256\n","149: 299.1301068747416 43.52482482045889 342.6549316952005\n","150: 308.93618874810636 44.777214844711125 353.7134035928175\n","151: 302.463621430099 44.615247666835785 347.0788690969348\n","152: 310.1092514526099 44.68105651810765 354.79030797071755\n","153: 296.39014059118927 42.95855254307389 339.34869313426316\n","154: 301.7763750143349 44.078016561456025 345.85439157579094\n","155: 294.36661834735423 43.05283084977418 337.4194491971284\n","156: 302.48535031452775 42.59159583039582 345.07694614492357\n","157: 304.1984183676541 43.094613269902766 347.29303163755685\n","158: 303.3784389011562 43.13361911568791 346.5120580168441\n","159: 306.10198249295354 43.88296318612993 349.98494567908347\n","160: 293.01816936023533 41.00562227796763 334.02379163820297\n","161: 292.47402261570096 41.113747471012175 333.58777008671314\n","162: 309.2961108498275 42.77614216133952 352.07225301116705\n","163: 302.72255197167397 42.85557008907199 345.57812206074595\n","164: 293.2598985154182 41.21793593186885 334.477834447287\n","165: 298.3007791126147 41.06080321315676 339.36158232577145\n","166: 306.44213049672544 43.00895703397691 349.45108753070235\n","167: 301.5454307831824 41.621637932024896 343.1670687152073\n","168: 297.645613187924 42.111241151578724 339.75685433950275\n","169: 297.18698057346046 41.14997752383351 338.336958097294\n","170: 305.6164043433964 42.905678391456604 348.52208273485303\n","171: 297.45895251911134 40.66429983731359 338.1232523564249\n","172: 289.6488566370681 39.76821800787002 329.4170746449381\n","173: 283.45527264568955 39.03338092379272 322.48865356948227\n","174: 302.82925865240395 41.40270888060331 344.23196753300726\n","175: 297.5850883554667 40.72343384101987 338.3085221964866\n","176: 293.39976244419813 39.83612381666899 333.2358862608671\n","177: 297.52009793836623 40.56702331453562 338.08712125290185\n","178: 307.28029773104936 41.037959163077176 348.31825689412653\n","179: 299.00699064694345 39.779044987633824 338.7860356345773\n","180: 297.91967493109405 40.2044233744964 338.12409830559045\n","181: 306.271268915385 40.93568503856659 347.2069539539516\n","182: 302.0764991566539 41.13948409911245 343.21598325576633\n","183: 301.82028036750853 40.53329819440842 342.35357856191695\n","184: 285.1506252102554 38.31977935228497 323.47040456254035\n","185: 303.7525553070009 39.66974244546145 343.4222977524623\n","186: 297.0232355259359 40.31532617285848 337.33856169879436\n","187: 296.2861136160791 40.73647298756987 337.02258660364896\n","188: 285.76537960767746 38.97095406241715 324.7363336700946\n","189: 296.950711902231 40.02350950054824 336.9742214027792\n","190: 301.41107023879886 38.26089199632406 339.6719622351229\n","191: 292.0394060527906 37.40647942852229 329.44588548131287\n","192: 291.59041458182037 37.83336244430393 329.4237770261243\n","193: 295.7173300012946 38.78400098066777 334.5013309819624\n","194: 285.76817521080375 38.08326651528478 323.8514417260885\n","195: 298.1226587705314 38.7162534147501 336.8389121852815\n","196: 305.07453651726246 39.84729711804539 344.92183363530785\n","197: 309.9992082398385 40.728774522431195 350.7279827622697\n","198: 298.8060963880271 39.64966345671564 338.4557598447427\n","199: 298.3401177339256 40.59050953667611 338.9306272706017\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"doR5kS81SW5F"},"source":["# Save model"],"id":"doR5kS81SW5F"},{"cell_type":"code","metadata":{"id":"frozen-damage","executionInfo":{"status":"ok","timestamp":1625015142365,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_cifar10')\n","# model = models.__dict__['ResNet18']()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_cifar10'))"],"id":"frozen-damage","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLlLnmpRSZAl"},"source":["# Test on Test Data"],"id":"BLlLnmpRSZAl"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aboriginal-lafayette","executionInfo":{"status":"ok","timestamp":1625015146687,"user_tz":-60,"elapsed":4327,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"1b0a79d1-eaf2-4087-bc87-489dc71cd3ec"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"aboriginal-lafayette","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9483\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xHj0XxoMScbg"},"source":["# Test on Train Data"],"id":"xHj0XxoMScbg"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"therapeutic-orlando","executionInfo":{"status":"ok","timestamp":1625015174962,"user_tz":-60,"elapsed":28276,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"4e91d550-bf1f-4247-d79f-3265cb1f5140"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"therapeutic-orlando","execution_count":12,"outputs":[{"output_type":"stream","text":["0.99788\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XCvQUWwMSetI","executionInfo":{"status":"ok","timestamp":1625015174962,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"XCvQUWwMSetI","execution_count":12,"outputs":[]}]}
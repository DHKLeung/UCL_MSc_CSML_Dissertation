{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup cifar10 NN aug.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_b2Nvqg3Oifz"},"source":["# Import Libraries"],"id":"_b2Nvqg3Oifz"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"romantic-purchase","executionInfo":{"status":"ok","timestamp":1624971242658,"user_tz":-60,"elapsed":884,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"347433ad-ce4f-426c-a9ec-1718a9ca4701"},"source":["import torch\n","from torchvision import transforms, datasets\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"romantic-purchase","execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qiPvZSTKO0Pk"},"source":["# Import outside code"],"id":"qiPvZSTKO0Pk"},{"cell_type":"code","metadata":{"id":"kw0NscKvO2bI","executionInfo":{"status":"ok","timestamp":1624971243023,"user_tz":-60,"elapsed":366,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"kw0NscKvO2bI","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMYYosNOO2V3","executionInfo":{"status":"ok","timestamp":1624971243413,"user_tz":-60,"elapsed":391,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"sMYYosNOO2V3","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pg95DAmvO8un"},"source":["# Configuration"],"id":"pg95DAmvO8un"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silver-clear","executionInfo":{"status":"ok","timestamp":1624971243414,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"04a90ead-b4f6-4af7-9b5e-ff788e282569"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),  # can omit\n","    transforms.RandomHorizontalFlip(),  # can omit\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","batch_size = 128\n","step_size = 0.1\n","random_seed = 0\n","epochs = 200\n","L2_decay = 1e-4\n","alpha = 1.\n","\n","torch.manual_seed(random_seed)"],"id":"silver-clear","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fc08e423bb0>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"uDMInb7zPT3v"},"source":["# Data"],"id":"uDMInb7zPT3v"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"relative-mobility","executionInfo":{"status":"ok","timestamp":1624971246494,"user_tz":-60,"elapsed":3082,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"58a2e0ad-a2ea-4f67-e033-d2fb313b5f63"},"source":["\"\"\"\n","Data\n","\"\"\"\n","train_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=True, download=True, transform=transform_train)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=False, download=True, transform=transform_test)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"relative-mobility","execution_count":5,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0-R2uhkAPjkv"},"source":["# Models, Loss, Optimiser"],"id":"0-R2uhkAPjkv"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"starting-chancellor","executionInfo":{"status":"ok","timestamp":1624971249107,"user_tz":-60,"elapsed":2615,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"4d2eb105-5455-4404-837c-69b3d4fa5f71"},"source":["model = ResNet18()\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"starting-chancellor","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"1qFqkwNmQgeO"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"1qFqkwNmQgeO"},{"cell_type":"code","metadata":{"id":"contemporary-gross","executionInfo":{"status":"ok","timestamp":1624971249108,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_cifar10(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    batch_size = labels.size(0)\n","    idx = torch.randperm(batch_size).to('cuda')\n","    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n","    labels_b = labels[idx]\n","    return mixup_inputs, labels, labels_b, lmbda"],"id":"contemporary-gross","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"right-spending","executionInfo":{"status":"ok","timestamp":1624971249108,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"right-spending","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TlGDo8vIQoZk"},"source":["# Training"],"id":"TlGDo8vIQoZk"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"written-bookmark","executionInfo":{"status":"ok","timestamp":1624981067581,"user_tz":-60,"elapsed":9818476,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"86a3ac29-ebfe-440f-bbcb-41b07539c336"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","\n","        # Mixup perturbation #\n","        mixup_inputs, mixup_labels_a, mixup_labels_b, lmbda = mixup_cifar10(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs = augment_outputs[original_num:]\n","        mixup_loss = mixup_criterion(criterion, mixup_outputs, mixup_labels_a, mixup_labels_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        augment_loss = mixup_loss + loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += augment_loss.item()\n","\n","        # Gradient Calculation & Optimisation #\n","        augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"written-bookmark","execution_count":9,"outputs":[{"output_type":"stream","text":["0: 1021.7338914871216 1701.316578388214 2723.05047082901\n","1: 902.055810213089 908.7096478939056 1810.765459537506\n","2: 901.4563145637512 902.6972286701202 1804.1535439491272\n","3: 901.4918274879456 900.6558425426483 1802.1476664543152\n","4: 901.5792264938354 900.9099583625793 1802.4891920089722\n","5: 901.5063619613647 900.2433471679688 1801.7497019767761\n","6: 901.4943814277649 900.322794675827 1801.8171706199646\n","7: 901.5438890457153 899.7646524906158 1801.3085398674011\n","8: 901.6841802597046 901.8422403335571 1803.5264148712158\n","9: 901.4109768867493 899.722898721695 1801.1338715553284\n","10: 901.411452293396 899.5998013019562 1801.0112519264221\n","11: 968.2675726413727 1145.807996749878 2114.075562477112\n","12: 925.3947665691376 993.1255524158478 1918.5203218460083\n","13: 901.6673262119293 903.1780142784119 1804.8453440666199\n","14: 901.7085826396942 901.5059673786163 1803.2145419120789\n","15: 901.7899131774902 901.0670521259308 1802.8569650650024\n","16: 901.6027188301086 900.3134853839874 1801.9162073135376\n","17: 906.3486113548279 958.851470708847 1865.2000818252563\n","18: 901.7947480678558 905.4325613975525 1807.2273082733154\n","19: 901.2022581100464 900.6961936950684 1801.8984489440918\n","20: 900.9586243629456 898.76602602005 1799.7246503829956\n","21: 900.8025479316711 897.1352341175079 1797.9377765655518\n","22: 899.9189476966858 894.1698591709137 1794.088800907135\n","23: 894.8114440441132 884.0530774593353 1778.8645224571228\n","24: 858.6381326913834 821.7461603879929 1680.3842923641205\n","25: 815.4401741027832 745.8484448194504 1561.2886216640472\n","26: 797.0655474662781 707.6658892631531 1504.7314324378967\n","27: 766.750453710556 657.9686276912689 1424.7190780639648\n","28: 735.5076878070831 603.679337978363 1339.187024116516\n","29: 719.8235722780228 554.5625842809677 1274.3861570358276\n","30: 690.4351146221161 502.2489867210388 1192.684103012085\n","31: 663.1150442361832 452.63041001558304 1115.7454552650452\n","32: 634.9457371234894 410.34430080652237 1045.2900390625\n","33: 619.97733938694 375.7851192355156 995.7624604701996\n","34: 615.9879378676414 343.94594752788544 959.9338867664337\n","35: 589.3170775175095 316.38707488775253 905.7041553258896\n","36: 578.8041711449623 287.0133346915245 865.8175060749054\n","37: 558.6981425881386 266.3307640552521 825.0289064645767\n","38: 552.6890705823898 247.59767657518387 800.2867479324341\n","39: 536.7129793763161 232.01763013005257 768.7306092977524\n","40: 519.6236125230789 218.67760744690895 738.3012196421623\n","41: 526.4255551695824 206.09935694932938 732.5249127149582\n","42: 519.032207429409 193.301987439394 712.3341938853264\n","43: 512.6085947751999 184.6488843858242 697.2574787139893\n","44: 517.8411230444908 174.0282869040966 691.8694095611572\n","45: 505.20747247338295 166.63060063123703 671.8380734920502\n","46: 491.5922948420048 162.05749933421612 653.6497950553894\n","47: 492.1963261663914 166.48924182355404 658.6855677366257\n","48: 498.05577635765076 165.54374459385872 663.5995219349861\n","49: 482.93768033385277 152.0200459063053 634.9577258825302\n","50: 487.8544165790081 143.19521951675415 631.0496366024017\n","51: 468.25610077381134 137.8732185959816 606.1293188333511\n","52: 478.7303017228842 133.45317593216896 612.1834778189659\n","53: 474.0537991821766 125.47259423136711 599.5263936519623\n","54: 471.00570726394653 121.51814422011375 592.5238516926765\n","55: 459.2173202931881 117.92012351751328 577.1374433636665\n","56: 478.26352247595787 111.90892177820206 590.1724451780319\n","57: 464.1051778793335 110.03435234725475 574.1395301222801\n","58: 461.72125804424286 108.18655899167061 569.9078163802624\n","59: 460.6810694485903 105.14686646312475 565.8279358744621\n","60: 451.8581593185663 104.32251988351345 556.1806787252426\n","61: 464.23064360022545 102.71553008258343 566.94617369771\n","62: 450.686459004879 100.26868636906147 550.955145150423\n","63: 451.90384352207184 94.7333951741457 546.6372382640839\n","64: 440.63707253336906 93.89184417575598 534.5289168059826\n","65: 461.37003454566 88.80928084999323 550.1793144643307\n","66: 438.9568921327591 88.93936789035797 527.8962601423264\n","67: 445.5189517736435 87.20360760390759 532.7225585579872\n","68: 437.28903716802597 85.19050182402134 522.4795394837856\n","69: 459.35495322942734 81.10589008778334 540.4608435928822\n","70: 443.8307770341635 79.65712378174067 523.4879000484943\n","71: 434.53539602458477 81.04073517024517 515.5761312544346\n","72: 428.9321044832468 80.54193819314241 509.47404220700264\n","73: 425.1772305816412 77.95997662097216 503.13720723986626\n","74: 442.90997041761875 74.1153287962079 517.0252995789051\n","75: 430.95519058406353 73.85718116909266 504.81237161159515\n","76: 439.81477700918913 74.01983474195004 513.8346109390259\n","77: 436.57844787836075 70.73344940692186 507.3118983954191\n","78: 427.9120637625456 71.35233864933252 499.26440250873566\n","79: 436.9975001960993 68.25429501384497 505.25179648399353\n","80: 436.875356733799 65.91989620774984 502.7952524870634\n","81: 432.8144185990095 66.29280350357294 499.1072221696377\n","82: 429.80469347536564 68.90656384080648 498.71125742793083\n","83: 425.27305637300014 65.8678846731782 491.14094138145447\n","84: 423.9672481417656 66.04467856884003 490.0119257867336\n","85: 432.4851342588663 64.43220653012395 496.91734075546265\n","86: 437.1776559948921 62.275083996355534 499.452740162611\n","87: 424.58173328638077 61.08583911135793 485.66757172346115\n","88: 435.2185690551996 61.93937212228775 497.15794083476067\n","89: 429.10888313502073 61.96977473050356 491.0786573588848\n","90: 434.50504180043936 61.60491897165775 496.1099605858326\n","91: 424.90354615449905 61.20594569668174 486.1094902455807\n","92: 420.0773550271988 58.39585676416755 478.4732119292021\n","93: 427.3670109808445 56.851019099354744 484.2180309891701\n","94: 413.3623577058315 59.33142765238881 472.69378501176834\n","95: 434.49277541041374 55.604847222566605 490.09762382507324\n","96: 413.2524596899748 58.5018193423748 471.75427953898907\n","97: 412.2720737233758 54.97775499150157 467.2498287707567\n","98: 412.1504960656166 57.06813186779618 469.21862806379795\n","99: 404.84957333654165 56.179546650499105 461.0291203856468\n","100: 405.86694330722094 29.135313129983842 435.0022569000721\n","101: 392.2738085947931 19.519795424304903 411.7936033271253\n","102: 373.88268292322755 17.48161038570106 391.36429324001074\n","103: 378.7094956897199 15.549963482655585 394.25945999473333\n","104: 375.94079488888383 13.812085968442261 389.75287948548794\n","105: 365.3814900778234 13.55168634466827 378.93317579478025\n","106: 379.750730574131 12.947273508645594 392.6980038434267\n","107: 373.16257114335895 11.566797943785787 384.7293684706092\n","108: 376.7486605383456 10.878596974071115 387.62725818157196\n","109: 368.9920786675066 10.229816699866205 379.2218953743577\n","110: 363.08292244561017 10.005232089664787 373.08815510571003\n","111: 371.2337164282799 9.652712411712855 380.8864291161299\n","112: 369.5495222695172 9.227056831587106 378.77657920867205\n","113: 368.38550671748817 8.859993229154497 377.24550054222345\n","114: 360.517391173169 8.245906542055309 368.7632976435125\n","115: 373.2027518451214 8.191333791706711 381.394086483866\n","116: 364.3049650043249 8.062948165927082 372.36791352182627\n","117: 367.8210116978735 7.369802209082991 375.1908139176667\n","118: 381.6784608140588 7.595837965141982 389.27429953962564\n","119: 353.83669350482523 7.19661118555814 361.03330501168966\n","120: 356.41699292883277 7.033376372884959 363.4503684602678\n","121: 363.54237101227045 6.7850342080928385 370.32740557938814\n","122: 370.48364401236176 6.645888110157102 377.12953255325556\n","123: 367.5474244058132 6.212214581668377 373.75963851064444\n","124: 354.9989190623164 6.411255871877074 361.41017547622323\n","125: 358.34175357595086 6.332714922260493 364.6744679585099\n","126: 375.2291241083294 5.763232399243861 380.9923570342362\n","127: 359.496305167675 6.45592256449163 365.9522280842066\n","128: 367.3404791280627 6.335249071475118 373.6757278367877\n","129: 357.81436342559755 5.881671572104096 363.6960347928107\n","130: 340.5796940047294 5.5892016449943185 346.16889595612884\n","131: 372.4206751510501 6.153825825545937 378.5745012536645\n","132: 354.2254835218191 5.9117701961658895 360.1372531540692\n","133: 353.08388352859765 5.739010185003281 358.8228933196515\n","134: 362.9693598821759 5.529048356693238 368.4984082430601\n","135: 354.5479040853679 5.641482215374708 360.18938675150275\n","136: 356.7736471295357 5.408246638253331 362.1818931661546\n","137: 341.6843090467155 5.287857276154682 346.9721668995917\n","138: 339.07209884002805 5.281158936675638 344.3532583657652\n","139: 357.44972200132906 5.5479041282087564 362.9976264163852\n","140: 345.72063522040844 5.291296402923763 351.0119318217039\n","141: 357.33702710457146 5.054218351375312 362.39124531671405\n","142: 353.46558063849807 5.442167746601626 358.90774881467223\n","143: 349.18920915573835 5.293706497177482 354.48291605338454\n","144: 343.39282598719 4.845948474947363 348.2387743014842\n","145: 346.5716713964939 4.924036644864827 351.495708078146\n","146: 366.7450220156461 5.0751508173998445 371.82017409428954\n","147: 360.038834515959 4.81671163556166 364.855546079576\n","148: 350.56696094572544 4.663881283253431 355.23084253817797\n","149: 346.59082259517163 5.028325005667284 351.61914763227105\n","150: 358.9918209835887 4.917286938754842 363.90910779312253\n","151: 350.2040059491992 4.710143820382655 354.9141501970589\n","152: 359.09016151353717 5.222184517420828 364.3123456630856\n","153: 343.5752499215305 4.806627011857927 348.381876161322\n","154: 350.55996227264404 4.952204309403896 355.51216505654156\n","155: 340.78410593792796 4.826147870160639 345.61025487072766\n","156: 349.75715040788054 4.899538769386709 354.65668941661716\n","157: 353.04589395597577 4.629110265523195 357.6750050075352\n","158: 350.83042938634753 4.472269136691466 355.3026986271143\n","159: 354.43061150982976 4.639646266121417 359.0702582448721\n","160: 338.75956903398037 4.641236598603427 343.40080601722\n","161: 338.4416789673269 4.506125585874543 342.94780438765883\n","162: 359.3821591362357 4.549169279867783 363.93132827058434\n","163: 350.18319585360587 4.630790622206405 354.8139862474054\n","164: 338.6365032829344 4.498017521109432 343.1345208324492\n","165: 344.52329003065825 4.8581734276376665 349.38146248459816\n","166: 353.15279026143253 4.312403315445408 357.4651930630207\n","167: 348.1461506485939 4.651071373838931 352.7972221970558\n","168: 344.57738053426147 4.4210207760334015 348.9984003081918\n","169: 343.92962183244526 4.794391839299351 348.7240128517151\n","170: 353.13353027403355 5.114381351741031 358.2479118555784\n","171: 342.9353979304433 4.566691118758172 347.50208972766995\n","172: 334.26818853616714 4.425178040051833 338.69336615130305\n","173: 325.7925770226866 4.351668625837192 330.1442457512021\n","174: 349.94053246825933 4.344537238357589 354.28507033362985\n","175: 343.28159216511995 4.185407727956772 347.46699994429946\n","176: 337.9519584476948 4.465161690255627 342.41712152957916\n","177: 343.80427267216146 4.196434658020735 348.00070705637336\n","178: 355.9352504555136 4.2743976067285985 360.20964907482266\n","179: 344.47443708684295 4.48720030579716 348.9616386592388\n","180: 343.14035428129137 4.632201977074146 347.7725564055145\n","181: 353.4752866383642 4.388871252303943 357.86415799334645\n","182: 349.1031313315034 4.4827769924886525 353.5859085433185\n","183: 347.9351198747754 4.283487645210698 352.2186076082289\n","184: 329.73473060689867 4.0919354415964335 333.8266661968082\n","185: 349.9626042097807 4.345957331126556 354.308561835438\n","186: 341.8234147429466 4.2218937035650015 346.04530826956034\n","187: 340.3360604904592 4.527215790469199 344.86327608674765\n","188: 327.2040244862437 4.636885919608176 331.8409105986357\n","189: 342.12230508588254 4.5580971736926585 346.6804013941437\n","190: 347.85377587843686 4.300806745886803 352.15458358824253\n","191: 335.2579678874463 4.157084102975205 339.41505091637373\n","192: 335.6576460748911 4.301067157415673 339.95871326327324\n","193: 338.62357055954635 4.202304699225351 342.8258747085929\n","194: 329.1020999774337 4.34016838343814 333.442267999053\n","195: 342.4746866915375 3.854072886519134 346.32875917479396\n","196: 350.4427268411964 4.5464922829996794 354.98921940103173\n","197: 357.2504843901843 4.369980420684442 361.6204649247229\n","198: 342.5483419857919 3.9584954087622464 346.50683703646064\n","199: 342.1270435638726 4.286701008444652 346.41374424658716\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"doR5kS81SW5F"},"source":["# Save model"],"id":"doR5kS81SW5F"},{"cell_type":"code","metadata":{"id":"frozen-damage","executionInfo":{"status":"ok","timestamp":1624981067582,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_cifar10')\n","# model = models.__dict__['ResNet18']()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_cifar10'))"],"id":"frozen-damage","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLlLnmpRSZAl"},"source":["# Test on Test Data"],"id":"BLlLnmpRSZAl"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aboriginal-lafayette","executionInfo":{"status":"ok","timestamp":1624981071706,"user_tz":-60,"elapsed":4129,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"cb17aeb2-18fc-4f8e-d1d7-a164d910aa56"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"aboriginal-lafayette","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9401\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xHj0XxoMScbg"},"source":["# Test on Train Data"],"id":"xHj0XxoMScbg"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"therapeutic-orlando","executionInfo":{"status":"ok","timestamp":1624981098982,"user_tz":-60,"elapsed":27278,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"d26fd36c-8f3a-465e-e9df-5bab82b1d900"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"therapeutic-orlando","execution_count":12,"outputs":[{"output_type":"stream","text":["0.9994\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XCvQUWwMSetI","executionInfo":{"status":"ok","timestamp":1624981098982,"user_tz":-60,"elapsed":11,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"XCvQUWwMSetI","execution_count":12,"outputs":[]}]}
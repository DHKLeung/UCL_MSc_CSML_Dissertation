{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup cifar10 NN aug WPL0.9.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_b2Nvqg3Oifz"},"source":["# Import Libraries"],"id":"_b2Nvqg3Oifz"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"romantic-purchase","executionInfo":{"status":"ok","timestamp":1624993380050,"user_tz":-60,"elapsed":29979,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"ae07043c-2992-461f-f421-11f087149f5a"},"source":["import torch\n","from torchvision import transforms, datasets\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"romantic-purchase","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qiPvZSTKO0Pk"},"source":["# Import outside code"],"id":"qiPvZSTKO0Pk"},{"cell_type":"code","metadata":{"id":"kw0NscKvO2bI","executionInfo":{"status":"ok","timestamp":1624993381103,"user_tz":-60,"elapsed":1055,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"kw0NscKvO2bI","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMYYosNOO2V3","executionInfo":{"status":"ok","timestamp":1624993381463,"user_tz":-60,"elapsed":362,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"sMYYosNOO2V3","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pg95DAmvO8un"},"source":["# Configuration"],"id":"pg95DAmvO8un"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silver-clear","executionInfo":{"status":"ok","timestamp":1624993381463,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"4bcd8e6b-f75a-4399-d76e-5b4df70d00dc"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),  # can omit\n","    transforms.RandomHorizontalFlip(),  # can omit\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","batch_size = 128\n","step_size = 0.1\n","random_seed = 0\n","epochs = 200\n","L2_decay = 1e-4\n","alpha = 1.\n","perturb_loss_weight = 0.9\n","\n","torch.manual_seed(random_seed)"],"id":"silver-clear","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f3609f3cad0>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"uDMInb7zPT3v"},"source":["# Data"],"id":"uDMInb7zPT3v"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"relative-mobility","executionInfo":{"status":"ok","timestamp":1624993391598,"user_tz":-60,"elapsed":10136,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"d3feced7-771e-463e-c601-7c9ad5256abd"},"source":["\"\"\"\n","Data\n","\"\"\"\n","train_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=True, download=True, transform=transform_train)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=False, download=True, transform=transform_test)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"relative-mobility","execution_count":5,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0-R2uhkAPjkv"},"source":["# Models, Loss, Optimiser"],"id":"0-R2uhkAPjkv"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"starting-chancellor","executionInfo":{"status":"ok","timestamp":1624993397639,"user_tz":-60,"elapsed":6042,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"c5e52411-b6fd-4ecc-bccc-a2241ec9bbba"},"source":["model = ResNet18()\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"starting-chancellor","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"1qFqkwNmQgeO"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"1qFqkwNmQgeO"},{"cell_type":"code","metadata":{"id":"contemporary-gross","executionInfo":{"status":"ok","timestamp":1624993397639,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_cifar10(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    batch_size = labels.size(0)\n","    idx = torch.randperm(batch_size).to('cuda')\n","    mixup_inputs = lmbda * inputs + (1 - lmbda) * inputs[idx]\n","    labels_b = labels[idx]\n","    return mixup_inputs, labels, labels_b, lmbda"],"id":"contemporary-gross","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"right-spending","executionInfo":{"status":"ok","timestamp":1624993397639,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_criterion(criterion, predicts, labels, labels_b, lmbda):\n","    mixup_loss = lmbda * criterion(predicts, labels) + (1 - lmbda) * criterion(predicts, labels_b)\n","    return mixup_loss"],"id":"right-spending","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TlGDo8vIQoZk"},"source":["# Training"],"id":"TlGDo8vIQoZk"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"written-bookmark","executionInfo":{"status":"ok","timestamp":1625003221401,"user_tz":-60,"elapsed":9823765,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"c8c461f8-a6d1-4089-bc80-86dbc92d6618"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","\n","        # Mixup perturbation #\n","        mixup_inputs, mixup_labels_a, mixup_labels_b, lmbda = mixup_cifar10(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs = augment_outputs[original_num:]\n","        mixup_loss = mixup_criterion(criterion, mixup_outputs, mixup_labels_a, mixup_labels_b, lmbda)\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"written-bookmark","execution_count":9,"outputs":[{"output_type":"stream","text":["0: 774.005110502243 716.6156333684921 1490.6207438707352\n","1: 658.165080666542 509.8077123761177 1167.9727930426598\n","2: 606.9500144720078 406.81051260232925 1013.760527074337\n","3: 569.4769604206085 341.16028249263763 910.6372429132462\n","4: 546.120911359787 292.58871680498123 838.7096281647682\n","5: 520.8043568134308 257.94298458099365 778.7473413944244\n","6: 499.45387133955956 233.16682136058807 732.6206927001476\n","7: 498.41034972667694 212.196639418602 710.6069891452789\n","8: 477.7437073290348 195.7331907749176 673.4768981039524\n","9: 463.9199932515621 183.68625715374947 647.6062504053116\n","10: 473.8254669904709 174.555594176054 648.3810611665249\n","11: 473.53064626455307 163.4656652957201 636.9963115602732\n","12: 457.0865191221237 154.30282402038574 611.3893431425095\n","13: 443.75082528591156 146.82278279960155 590.5736080855131\n","14: 455.2117457985878 142.21395727992058 597.4257030785084\n","15: 442.8873919546604 134.26578968763351 577.1531816422939\n","16: 454.313839673996 129.6753167808056 583.9891564548016\n","17: 418.7011696100235 124.6610064804554 543.3621760904789\n","18: 434.1761802434921 120.15956093370914 554.3357411772013\n","19: 428.4075628519058 118.18234503269196 546.5899078845978\n","20: 425.9135560840368 112.46211721003056 538.3756732940674\n","21: 426.28635634481907 110.18381201475859 536.4701683595777\n","22: 427.8949805647135 111.56825368851423 539.4632342532277\n","23: 418.8528150767088 104.5200049802661 523.3728200569749\n","24: 425.30438324809074 103.044110506773 528.3484937548637\n","25: 407.58313605189323 100.76975462585688 508.3528906777501\n","26: 420.13621847331524 97.22028180211782 517.3565002754331\n","27: 406.00450959801674 95.5012117922306 501.50572139024734\n","28: 404.3193061500788 94.41590844839811 498.7352145984769\n","29: 418.53537537157536 92.50637993216515 511.0417553037405\n","30: 413.54856164753437 90.40990097820759 503.95846262574196\n","31: 408.6980571746826 88.56171391159296 497.2597710862756\n","32: 394.11751855909824 87.1562753841281 481.27379394322634\n","33: 399.63452848792076 87.16173057258129 486.79625906050205\n","34: 412.2442988604307 83.47915224730968 495.7234511077404\n","35: 399.74149891734123 84.13145911693573 483.87295803427696\n","36: 408.85763052105904 84.06576781719923 492.92339833825827\n","37: 397.10656882822514 82.55524131655693 479.66181014478207\n","38: 401.161559894681 79.95040989667177 481.11196979135275\n","39: 393.4554023593664 79.36059510707855 472.81599746644497\n","40: 385.4329634308815 79.47903786227107 464.91200129315257\n","41: 397.57226081192493 77.76345886290073 475.33571967482567\n","42: 395.2256974130869 75.77302219718695 470.99871961027384\n","43: 394.704325273633 75.8500762283802 470.5544015020132\n","44: 405.06513315439224 74.84304462373257 479.9081777781248\n","45: 394.94784174859524 72.73411494493484 467.6819566935301\n","46: 388.2015338912606 73.89289671182632 462.09443060308695\n","47: 384.177828758955 73.3269622027874 457.5047909617424\n","48: 390.09779869019985 71.9097542539239 462.00755294412374\n","49: 381.9482112079859 71.28021039068699 453.22842159867287\n","50: 390.82200787961483 70.46688263863325 461.2888905182481\n","51: 377.82459729909897 71.2025763913989 449.0271736904979\n","52: 386.07046127319336 67.63989404588938 453.71035531908274\n","53: 386.16733457148075 67.19334536790848 453.36067993938923\n","54: 384.5330993384123 67.57206630334258 452.10516564175487\n","55: 377.90699967741966 67.23008961230516 445.1370892897248\n","56: 396.45173777639866 66.22237776219845 462.6741155385971\n","57: 386.0416841059923 67.32243266701698 453.3641167730093\n","58: 384.58816903829575 67.98076013475657 452.5689291730523\n","59: 385.13356633484364 65.98043517768383 451.11400151252747\n","60: 376.02331184595823 65.74855500459671 441.77186685055494\n","61: 386.9797187894583 64.67636490985751 451.6560836993158\n","62: 378.8439806699753 66.58232483267784 445.4263055026531\n","63: 380.3535054922104 63.51859410852194 443.8720996007323\n","64: 370.7011640369892 62.49420680105686 433.1953708380461\n","65: 391.09558457136154 60.61406673491001 451.70965130627155\n","66: 371.5218550413847 61.67381037026644 433.19566541165113\n","67: 378.0431405901909 62.98400817811489 441.0271487683058\n","68: 373.5286588445306 62.62912907451391 436.1577879190445\n","69: 393.02368454635143 60.50120708346367 453.5248916298151\n","70: 380.53008005023 62.0588331669569 442.5889132171869\n","71: 369.9668185785413 60.6723978407681 430.6392164193094\n","72: 367.5003166049719 62.03907618671656 429.53939279168844\n","73: 364.8965870514512 60.99287659302354 425.88946364447474\n","74: 380.8423130661249 59.264781415462494 440.1070944815874\n","75: 371.00818997621536 61.35082434490323 432.3590143211186\n","76: 380.4499722123146 61.1647307947278 441.6147030070424\n","77: 378.8320694863796 60.04301502928138 438.875084515661\n","78: 370.0828428864479 58.629104509949684 428.7119473963976\n","79: 377.7042645588517 57.774209037423134 435.47847359627485\n","80: 379.7943521440029 58.26901828870177 438.0633704327047\n","81: 375.1733369976282 56.42624815180898 431.5995851494372\n","82: 371.52039986103773 57.90680172666907 429.4272015877068\n","83: 367.962059661746 56.83574556186795 424.797805223614\n","84: 369.85694248974323 58.46523232012987 428.3221748098731\n","85: 377.0707662925124 56.77382482588291 433.84459111839533\n","86: 383.8071934878826 58.110206458717585 441.9173999466002\n","87: 370.78239142894745 57.77530666068196 428.5576980896294\n","88: 379.65443079918623 55.7713915258646 435.42582232505083\n","89: 373.81756016612053 56.13801347464323 429.95557364076376\n","90: 379.2598192319274 55.98265551403165 435.24247474595904\n","91: 371.89118729531765 56.754127234220505 428.64531452953815\n","92: 367.3690513819456 55.93337032198906 423.30242170393467\n","93: 375.6924743503332 56.161475993692875 431.8539503440261\n","94: 363.4157616496086 55.87592639029026 419.2916880398989\n","95: 382.5583717748523 54.06014293059707 436.61851470544934\n","96: 362.67507069557905 55.331756979227066 418.0068276748061\n","97: 364.5600220039487 55.764319997280836 420.3243420012295\n","98: 363.14396937191486 55.57918115705252 418.7231505289674\n","99: 356.32898470014334 53.10947897285223 409.43846367299557\n","100: 352.1878853943199 32.045667016878724 384.2335524111986\n","101: 340.2510436754674 25.335998348891735 365.5870420243591\n","102: 322.5067463554442 23.115338487550616 345.6220848429948\n","103: 327.0019978135824 22.218740073032677 349.2207378866151\n","104: 324.292038872838 20.427969376556575 344.7200082493946\n","105: 314.3282653372735 19.6194803211838 333.9477456584573\n","106: 325.3114564232528 19.05350807774812 344.36496450100094\n","107: 320.69942591153085 18.43750947341323 339.1369353849441\n","108: 322.8425597809255 17.3790304614231 340.2215902423486\n","109: 317.29038592241704 17.37498334608972 334.66536926850677\n","110: 310.85245541483164 16.783551637083292 327.63600705191493\n","111: 317.4148741364479 16.198715400882065 333.61358953733\n","112: 316.96681690216064 16.23965131305158 333.2064682152122\n","113: 315.6139860507101 15.851945639587939 331.465931690298\n","114: 308.6061519701034 15.31698658503592 323.9231385551393\n","115: 318.6093429690227 15.641893601045012 334.2512365700677\n","116: 311.3010280691087 15.30432109721005 326.6053491663188\n","117: 314.25962145254016 15.022467473056167 329.2820889255963\n","118: 325.4914412908256 15.015119404532015 340.5065606953576\n","119: 302.3695754427463 13.971512457355857 316.34108790010214\n","120: 305.27531484141946 14.028216030448675 319.30353087186813\n","121: 310.0454559456557 13.548242622055113 323.5936985677108\n","122: 317.22671053931117 14.28479115664959 331.51150169596076\n","123: 313.6217264905572 13.845865635201335 327.46759212575853\n","124: 304.17373191192746 13.391862629912794 317.56559454184026\n","125: 306.1017410978675 13.458295047283173 319.56003614515066\n","126: 319.94735705293715 13.551914568059146 333.4992716209963\n","127: 307.41401071846485 13.921706262975931 321.3357169814408\n","128: 313.313852135092 13.350747678428888 326.6645998135209\n","129: 306.0553210498765 12.987058381550014 319.0423794314265\n","130: 291.7277750717476 12.810772883705795 304.5385479554534\n","131: 317.6290625818074 13.47850804682821 331.1075706286356\n","132: 303.23517456650734 12.944510924629867 316.1796854911372\n","133: 301.93507994432 12.675990424584597 314.6110703689046\n","134: 310.72991021350026 12.760705520398915 323.4906157338992\n","135: 303.4116627536714 12.967169653158635 316.37883240683004\n","136: 305.6809606682509 13.201323818415403 318.8822844866663\n","137: 293.96325392555445 12.21029544621706 306.1735493717715\n","138: 291.35497967898846 12.591318083927035 303.9462977629155\n","139: 305.5377689441666 12.557131445035338 318.09490038920194\n","140: 296.295802699402 12.097111175302416 308.3929138747044\n","141: 306.07290201820433 12.390329942572862 318.4632319607772\n","142: 301.6357114110142 12.155068818014115 313.7907802290283\n","143: 298.93079533055425 12.159821514971554 311.0906168455258\n","144: 294.0786877796054 11.897126572206616 305.975814351812\n","145: 296.6343634016812 11.76342382421717 308.39778722589836\n","146: 313.7879255451262 12.04768145410344 325.83560699922964\n","147: 308.6694451607764 11.603283582255244 320.2727287430316\n","148: 300.1899401638657 11.412665599025786 311.60260576289147\n","149: 297.11225894652307 11.814069582149386 308.92632852867246\n","150: 307.8152134809643 12.388799033593386 320.2040125145577\n","151: 300.3632950242609 11.302260810509324 311.6655558347702\n","152: 308.0939026251435 11.701550043653697 319.7954526687972\n","153: 295.1621321020648 11.89420926105231 307.0563413631171\n","154: 300.3384497631341 11.573131242301315 311.91158100543544\n","155: 292.9842807967216 11.83280182396993 304.8170826206915\n","156: 300.703068934381 11.505869625136256 312.20893855951726\n","157: 303.4474717695266 11.754061289131641 315.20153305865824\n","158: 302.19108909368515 11.55820355983451 313.74929265351966\n","159: 304.3414403684437 11.040495768655092 315.3819361370988\n","160: 291.43222633190453 10.874012818094343 302.3062391499989\n","161: 291.76868601702154 10.875128995161504 302.64381501218304\n","162: 308.403307441622 11.02686742413789 319.4301748657599\n","163: 301.1726275216788 11.383393598720431 312.55602112039924\n","164: 292.06353647541255 10.802035730797797 302.86557220621035\n","165: 296.46995572559536 11.418843668885529 307.8887993944809\n","166: 304.80154272541404 11.523828381672502 316.32537110708654\n","167: 299.04844941198826 11.404576490167528 310.4530259021558\n","168: 296.2683044914156 11.319661677349359 307.587966168765\n","169: 296.3246645675972 11.519277437590063 307.8439420051873\n","170: 304.13362829759717 11.497690138872713 315.6313184364699\n","171: 295.4246330745518 11.201868251431733 306.62650132598355\n","172: 287.9227843657136 11.451087550260127 299.3738719159737\n","173: 282.13657363690436 10.987623010296375 293.12419664720073\n","174: 301.4452095385641 10.957799853291363 312.40300939185545\n","175: 295.9187631588429 11.002460247371346 306.92122340621427\n","176: 291.913188688457 11.066720669157803 302.9799093576148\n","177: 296.4519831407815 10.677766447886825 307.12974958866835\n","178: 306.6344361649826 10.918320447206497 317.5527566121891\n","179: 298.27735679922625 11.062011059839278 309.33936785906553\n","180: 295.9786225967109 10.762799778953195 306.7414223756641\n","181: 304.28584324009717 11.330472094006836 315.616315334104\n","182: 300.77010252326727 11.160966985393316 311.9310695086606\n","183: 300.44233608525246 10.74056648556143 311.1829025708139\n","184: 284.4277722351253 10.341972508933395 294.7697447440587\n","185: 302.00647342950106 11.104079723358154 313.1105531528592\n","186: 296.1170869320631 10.973463435191661 307.09055036725476\n","187: 294.788440387696 11.516247033141553 306.3046874208376\n","188: 284.0931826978922 11.399695447646081 295.49287814553827\n","189: 295.8719419017434 11.180665890686214 307.0526077924296\n","190: 300.4162505976856 10.432408260181546 310.8486588578671\n","191: 291.18229591473937 10.399346480611712 301.5816423953511\n","192: 290.222162055783 10.167432502377778 300.38959455816075\n","193: 293.3979807533324 10.860735174734145 304.2587159280665\n","194: 283.9877465106547 10.655129646882415 294.6428761575371\n","195: 297.4077985920012 10.47429356398061 307.8820921559818\n","196: 302.85186074860394 11.018027774058282 313.8698885226622\n","197: 308.9675889797509 10.452043366152793 319.41963234590366\n","198: 297.5460836319253 11.188243079930544 308.73432671185583\n","199: 296.7862219726667 10.91195041174069 307.69817238440737\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"doR5kS81SW5F"},"source":["# Save model"],"id":"doR5kS81SW5F"},{"cell_type":"code","metadata":{"id":"frozen-damage","executionInfo":{"status":"ok","timestamp":1625003221403,"user_tz":-60,"elapsed":13,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_cifar10')\n","# model = models.__dict__['ResNet18']()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_cifar10'))"],"id":"frozen-damage","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLlLnmpRSZAl"},"source":["# Test on Test Data"],"id":"BLlLnmpRSZAl"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aboriginal-lafayette","executionInfo":{"status":"ok","timestamp":1625003224709,"user_tz":-60,"elapsed":3314,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"5f53fce7-1533-4f75-b9a2-903a0f350ba1"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"aboriginal-lafayette","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9546\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xHj0XxoMScbg"},"source":["# Test on Train Data"],"id":"xHj0XxoMScbg"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"therapeutic-orlando","executionInfo":{"status":"ok","timestamp":1625003245781,"user_tz":-60,"elapsed":21077,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"d68621aa-50a1-47a4-d02e-447ba3af8fb9"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"therapeutic-orlando","execution_count":12,"outputs":[{"output_type":"stream","text":["0.99926\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XCvQUWwMSetI","executionInfo":{"status":"ok","timestamp":1625003245781,"user_tz":-60,"elapsed":17,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"XCvQUWwMSetI","execution_count":12,"outputs":[]}]}
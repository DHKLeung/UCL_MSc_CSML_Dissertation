{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup cifar10 NN sc aug.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_b2Nvqg3Oifz"},"source":["# Import Libraries"],"id":"_b2Nvqg3Oifz"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"romantic-purchase","executionInfo":{"status":"ok","timestamp":1625153034451,"user_tz":-60,"elapsed":561,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"807d92a4-61c8-4fb0-bd85-39c2f2cb84ae"},"source":["import torch\n","from torchvision import transforms, datasets\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"romantic-purchase","execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qiPvZSTKO0Pk"},"source":["# Import outside code"],"id":"qiPvZSTKO0Pk"},{"cell_type":"code","metadata":{"id":"kw0NscKvO2bI","executionInfo":{"status":"ok","timestamp":1625153035031,"user_tz":-60,"elapsed":582,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"kw0NscKvO2bI","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMYYosNOO2V3","executionInfo":{"status":"ok","timestamp":1625153035322,"user_tz":-60,"elapsed":293,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"sMYYosNOO2V3","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pg95DAmvO8un"},"source":["# Configuration"],"id":"pg95DAmvO8un"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silver-clear","executionInfo":{"status":"ok","timestamp":1625153035323,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"3743e34c-f5b8-4648-c1ca-cccf9bcbb5c3"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),  # can omit\n","    transforms.RandomHorizontalFlip(),  # can omit\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","batch_size = 128\n","step_size = 0.1\n","random_seed = 0\n","epochs = 200\n","L2_decay = 1e-4\n","alpha = 1.\n","\n","torch.manual_seed(random_seed)"],"id":"silver-clear","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f2a1fc11930>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"uDMInb7zPT3v"},"source":["# Data"],"id":"uDMInb7zPT3v"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"relative-mobility","executionInfo":{"status":"ok","timestamp":1625153037842,"user_tz":-60,"elapsed":2521,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"c931e71d-0b49-44cb-d84e-50ae23ab78a0"},"source":["\"\"\"\n","Data\n","\"\"\"\n","train_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=True, download=True, transform=transform_train)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=False, download=True, transform=transform_test)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"relative-mobility","execution_count":5,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0-R2uhkAPjkv"},"source":["# Models, Loss, Optimiser"],"id":"0-R2uhkAPjkv"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"starting-chancellor","executionInfo":{"status":"ok","timestamp":1625153040475,"user_tz":-60,"elapsed":2634,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"730cc794-2e0d-43da-ec20-30295855694f"},"source":["model = ResNet18()\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"starting-chancellor","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"1qFqkwNmQgeO"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"1qFqkwNmQgeO"},{"cell_type":"code","metadata":{"id":"EUOeD7cFhZ4E","executionInfo":{"status":"ok","timestamp":1625153040476,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_cifar10_sc(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_uc_list = list()\n","    labels_uc_list = list()\n","    unique_classes = torch.unique(labels)\n","    for uc in unique_classes:\n","        mask_uc = (labels == uc).flatten()  # flatten to avoid the labels are in column vector\n","        inputs_uc = inputs[mask_uc]\n","        labels_uc = labels[mask_uc]\n","        batch_size_uc = labels_uc.size(0)\n","        idx = torch.randperm(batch_size_uc).to('cuda')\n","        mixup_inputs_uc = lmbda * inputs_uc + (1 - lmbda) * inputs_uc[idx]\n","        mixup_inputs_uc_list.append(mixup_inputs_uc)\n","        labels_uc_list.append(labels_uc)\n","    mixup_inputs_sc = torch.vstack(mixup_inputs_uc_list)\n","    mixup_labels_sc = torch.cat(labels_uc_list, dim=0)  # use cat not hstack to avoid labels are in column vector\n","    return mixup_inputs_sc, mixup_labels_sc, lmbda"],"id":"EUOeD7cFhZ4E","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TlGDo8vIQoZk"},"source":["# Training"],"id":"TlGDo8vIQoZk"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"written-bookmark","executionInfo":{"status":"ok","timestamp":1625163007165,"user_tz":-60,"elapsed":9966691,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"08cbe735-fa90-4b67-da1a-086e3b25c2c1"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","\n","        # Mixup perturbation with same class #\n","        mixup_inputs_sc, mixup_labels_sc, lmbda = mixup_cifar10_sc(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_sc))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_sc = augment_outputs[original_num:]\n","        mixup_loss_sc = criterion(mixup_outputs_sc, mixup_labels_sc)  # no need to mix the loss up since it is now mixup with same class, just use ordinary cross entropy\n","        loss = criterion(outputs, labels)\n","        augment_loss = mixup_loss_sc + loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_sc.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += augment_loss.item()\n","\n","        # Gradient Calculation & Optimisation #\n","        augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"written-bookmark","execution_count":8,"outputs":[{"output_type":"stream","text":["0: 1041.0329570770264 1444.1557912826538 2485.188751220703\n","1: 902.3405611515045 910.993129491806 1813.333688735962\n","2: 901.6059527397156 903.4585783481598 1805.0645260810852\n","3: 901.3761959075928 901.4344499111176 1802.8106455802917\n","4: 901.0407228469849 900.4030969142914 1801.4438209533691\n","5: 900.998407125473 899.6862189769745 1800.6846299171448\n","6: 895.9107813835144 895.163613319397 1791.0743899345398\n","7: 775.6640796661377 798.6579623222351 1574.3220450878143\n","8: 731.3021438121796 756.7919517755508 1488.094093322754\n","9: 695.1517637968063 723.5105078220367 1418.6622717380524\n","10: 632.0229227542877 669.488343000412 1301.5112655162811\n","11: 576.7549295425415 613.3088490962982 1190.063777923584\n","12: 520.0652421712875 556.9688929319382 1077.03413438797\n","13: 463.2658144235611 494.33447605371475 957.6002913713455\n","14: 413.7175602912903 438.9323523044586 852.6499135494232\n","15: 373.2966442704201 398.5971465706825 771.8937923908234\n","16: 331.6632031202316 354.38046592473984 686.043668627739\n","17: 300.65648126602173 317.37991696596146 618.0364001989365\n","18: 273.8103725910187 286.7296004295349 560.5399737358093\n","19: 253.24089142680168 262.3835891485214 515.6244792938232\n","20: 236.66395157575607 241.79641431570053 478.46036648750305\n","21: 224.21223136782646 225.89313676953316 450.105368077755\n","22: 210.2801392376423 212.13165286183357 422.41179168224335\n","23: 200.02294901013374 199.73084884881973 399.75379824638367\n","24: 193.7860997915268 191.34905302524567 385.13515269756317\n","25: 186.94638708233833 183.64147672057152 370.5878650546074\n","26: 177.42421409487724 172.75091262161732 350.17512679100037\n","27: 172.02983030676842 166.46060985326767 338.4904400110245\n","28: 167.95348905026913 159.8400795161724 327.7935688495636\n","29: 160.15178169310093 153.22715681791306 313.37893891334534\n","30: 159.5545238852501 148.71075655519962 308.2652811706066\n","31: 152.23298735916615 141.04330910742283 293.2762965261936\n","32: 148.4173554033041 138.1114692837 286.52882438898087\n","33: 146.81478798389435 133.6762986034155 280.4910858273506\n","34: 142.42610450088978 127.55506187677383 269.9811657965183\n","35: 137.89039747416973 123.62653182446957 261.5169288814068\n","36: 135.02725739777088 120.61188593506813 255.63914349675179\n","37: 131.66168339550495 118.87331223487854 250.53499525785446\n","38: 131.0764665156603 115.01308066397905 246.08954739570618\n","39: 129.1422691270709 112.66517162322998 241.80744022130966\n","40: 123.0229447633028 106.140934959054 229.16387993097305\n","41: 119.85950765013695 103.56290809810162 223.42241656780243\n","42: 119.73972171545029 101.53564465045929 221.27536630630493\n","43: 116.99406236410141 99.74429964274168 216.73836240172386\n","44: 115.55754739046097 97.22533175349236 212.78287935256958\n","45: 112.36807132512331 94.14953043311834 206.51760230958462\n","46: 109.25243078172207 90.61004589498043 199.862477093935\n","47: 109.39492656290531 90.33700888603926 199.73193535208702\n","48: 109.31889416277409 88.33656866103411 197.65546292066574\n","49: 106.35675382614136 84.07475318014622 190.43150717020035\n","50: 101.46084725856781 82.30407799780369 183.7649250626564\n","51: 103.85709520429373 82.63077461719513 186.48786981403828\n","52: 101.35247129201889 81.19765265285969 182.5501240491867\n","53: 97.61558052897453 78.5448406636715 176.16042105853558\n","54: 97.87274654209614 76.44950042665005 174.32224701344967\n","55: 98.7223522812128 74.75444549322128 173.47679805755615\n","56: 95.57187517732382 73.55144486576319 169.1233197003603\n","57: 94.6398680433631 73.34051328152418 167.98038102686405\n","58: 93.78350424021482 72.84511371701956 166.6286179870367\n","59: 93.67588405311108 70.87052771449089 164.5464111417532\n","60: 90.79558826982975 68.58867901936173 159.38426713645458\n","61: 91.7812959253788 69.8473413027823 161.62863701581955\n","62: 91.57033321261406 67.83707138895988 159.4074043482542\n","63: 87.73038860410452 67.15763092041016 154.8880193978548\n","64: 86.50721608102322 64.84628006070852 151.3534957319498\n","65: 88.32207498699427 65.0669835805893 153.38905876874924\n","66: 87.60995692759752 65.69871181249619 153.30866867303848\n","67: 91.74159754440188 68.4359663464129 160.17756365984678\n","68: 85.87794004380703 61.988952711224556 147.8668930903077\n","69: 87.43321332335472 63.3279242515564 150.76113772392273\n","70: 82.98874523490667 60.28813871741295 143.2768841162324\n","71: 81.83606434613466 58.7777761220932 140.61384077370167\n","72: 81.4613017141819 57.27078205347061 138.73208366334438\n","73: 80.61720566824079 57.07555524818599 137.69276071339846\n","74: 79.45799227058887 56.477100130170584 135.9350922703743\n","75: 78.75125186517835 54.950929526239634 133.70218178629875\n","76: 82.2653744854033 54.952013328671455 137.2173878401518\n","77: 75.58654414117336 53.41956778243184 129.00611201673746\n","78: 79.49653007462621 54.137152552604675 133.63368305563927\n","79: 78.00803229212761 54.161605924367905 132.16963824629784\n","80: 76.84102086722851 52.5476074591279 129.38862822949886\n","81: 76.02756135165691 51.91697431355715 127.94453603029251\n","82: 75.99437056854367 52.56173834577203 128.5561089143157\n","83: 76.80770974606276 52.198061775416136 129.0057713985443\n","84: 77.39403646811843 54.26239554584026 131.6564318239689\n","85: 72.47436191886663 49.53479264304042 122.00915456563234\n","86: 75.21493211761117 51.69054101407528 126.9054730758071\n","87: 71.24676232039928 49.4031142257154 120.64987658709288\n","88: 74.88710065186024 48.57855673506856 123.46565721929073\n","89: 72.6279068775475 50.27142629772425 122.89933344721794\n","90: 72.48572231829166 48.6466422714293 121.13236437737942\n","91: 70.81623204611242 48.6442984957248 119.46053037792444\n","92: 72.41054389998317 49.62797932699323 122.0385231077671\n","93: 71.59007942676544 47.34557932987809 118.93565882742405\n","94: 71.78406361863017 47.54080732166767 119.32487084716558\n","95: 75.11613957211375 50.390448646619916 125.50658844411373\n","96: 69.36939631775022 46.18115214910358 115.55054904520512\n","97: 74.00169536843896 46.54156542941928 120.54326096922159\n","98: 68.86920223757625 46.343667421489954 115.21286939829588\n","99: 67.17370145395398 45.71869529038668 112.89239660650492\n","100: 44.76475703716278 22.81944321282208 67.58420033380389\n","101: 33.65568624716252 14.875491976737976 48.531178168952465\n","102: 31.84516923641786 12.419303545262665 44.26447269693017\n","103: 29.12593494169414 10.15017354581505 39.27610845025629\n","104: 28.707730321679264 9.125553122255951 37.833283421583474\n","105: 26.972832712810487 8.431639740942046 35.40447235573083\n","106: 25.486271641217172 7.604300774401054 33.09057256951928\n","107: 24.3817126089707 6.8754573105834424 31.25716996472329\n","108: 24.197910441551358 6.126123033114709 30.32403338421136\n","109: 25.28008073940873 6.017021920066327 31.29710274329409\n","110: 24.519799071829766 5.802049396093935 30.321848462335765\n","111: 22.08228059625253 5.1528032689820975 27.23508393159136\n","112: 22.30323516507633 5.126561189536005 27.429796438198537\n","113: 24.825710104894824 4.912360968184657 29.73807118437253\n","114: 22.099323297385126 4.542575697763823 26.641898915171623\n","115: 23.481553979218006 4.409703986602835 27.891257991082966\n","116: 22.156428873422556 4.018975849612616 26.17540474794805\n","117: 20.90040885657072 3.981492987368256 24.8819019170478\n","118: 20.33920434024185 3.825364501448348 24.16456886800006\n","119: 20.775908675044775 3.8435825433698483 24.619491157121956\n","120: 19.562942052958533 3.440047381562181 23.002989416942\n","121: 19.792478587827645 3.474948279268574 23.26742684841156\n","122: 21.116812837542966 3.3636145985219628 24.480427384376526\n","123: 19.698351086350158 3.372981331194751 23.07133248820901\n","124: 18.286698890733533 3.15575577999698 21.442454716656357\n","125: 20.08132013550494 3.0778887987835333 23.15920890402049\n","126: 19.376179062586743 2.853735179582145 22.22991422296036\n","127: 17.733684949576855 2.9162275578128174 20.64991254801862\n","128: 19.677364317933097 2.663246351701673 22.340610610088333\n","129: 19.427907525445335 2.7053886982030235 22.133296236163005\n","130: 18.668371816864237 2.5172707681776956 21.18564258888364\n","131: 20.337153074797243 2.5762348738498986 22.913387920241803\n","132: 17.9092005379498 2.470694656483829 20.379895209800452\n","133: 16.907156399684027 2.5839909338392317 19.49114731187001\n","134: 17.648684677260462 2.4141301282215863 20.06281483906787\n","135: 17.69322434347123 2.3623461075185332 20.05557049124036\n","136: 18.03634889004752 2.363528611836955 20.399877446936443\n","137: 19.155660858144984 2.3710698953364044 21.5267307756003\n","138: 17.44903041573707 2.3797463537193835 19.828776728594676\n","139: 15.939562100218609 2.2963588190323208 18.235920964274555\n","140: 16.63342656288296 2.015380763274152 18.648807333549485\n","141: 16.561645342677366 2.072536825435236 18.634182167472318\n","142: 17.36282199091511 2.0559351414558478 19.41875709989108\n","143: 16.7578624880407 2.0451703604194336 18.803032820345834\n","144: 17.569770274567418 1.9851902234076988 19.55496047018096\n","145: 16.330837439163588 1.9376487176632509 18.268486179877073\n","146: 16.48973971500527 2.0448706494353246 18.534610343398526\n","147: 16.691039152792655 1.8212167918100022 18.51225590915419\n","148: 14.62798180419486 1.8025502933305688 16.430532083613798\n","149: 15.487102179671638 1.8248850846430287 17.311987273860723\n","150: 16.38948925328441 1.861096897686366 18.250586153706536\n","151: 17.35938741604332 1.8803031009738334 19.2396905538626\n","152: 17.53155374666676 2.0602882900857367 19.59184205508791\n","153: 18.383424052153714 1.926864740846213 20.310288756154478\n","154: 16.89325301925419 1.8081658093142323 18.701418781653047\n","155: 17.52421202347614 1.8256421586556826 19.349854186410084\n","156: 16.68327451380901 1.8637608574645128 18.547035372816026\n","157: 15.972972149495035 1.6976581157650799 17.670630247099325\n","158: 14.853335639752913 1.7091363962972537 16.562472075689584\n","159: 16.498228275158908 1.8234616728732362 18.32168999291025\n","160: 16.593234339903574 1.6280125277407933 18.22124679677654\n","161: 14.617944825033192 1.688532689702697 16.306477453908883\n","162: 16.532638859411236 1.6088352981314529 18.141474177478813\n","163: 14.280426654964685 1.644988726999145 15.925415414967574\n","164: 15.934218626643997 1.6222976871358696 17.556516316486523\n","165: 14.608369996771216 1.545213158649858 16.153583101229742\n","166: 15.814732319675386 1.6275682603591122 17.442300541792065\n","167: 16.02366577729117 1.5524686649732757 17.576134447357617\n","168: 15.275431790039875 1.4884201285603922 16.763851855997927\n","169: 16.273398815188557 1.6215326485398691 17.894931453396566\n","170: 14.06696609442588 1.3820203132054303 15.448986410861835\n","171: 15.406079762091395 1.464202575822128 16.870282317511737\n","172: 16.13607873703586 1.6988390067708679 17.8349177778\n","173: 16.119477021158673 1.580092697768123 17.699569704011083\n","174: 16.179224851715844 1.5066292889823671 17.685854131705128\n","175: 15.378187447902746 1.411070236528758 16.789257687865756\n","176: 14.830611674347892 1.4378938197041862 16.268505470943637\n","177: 15.658084316528402 1.6206475162762217 17.278731819591485\n","178: 14.931394162995275 1.5311393963929731 16.462533580954187\n","179: 13.407600438600639 1.4353450303897262 14.842945443815552\n","180: 13.767112273839302 1.2453133830131264 15.012425650842488\n","181: 14.149664702359587 1.4379203147254884 15.587585094152018\n","182: 16.19707787875086 1.3385132401745068 17.535591142135672\n","183: 14.772759016021155 1.4047385110752657 16.177497536409646\n","184: 16.532628573535476 1.6037860975775402 18.136414644075558\n","185: 14.27524976420682 1.3999829712556675 15.675232807989232\n","186: 15.354162133065984 1.3131951132018003 16.667357256170362\n","187: 16.163744284713175 1.4787331837578677 17.642477436340414\n","188: 15.423287176643498 1.5533140017651021 16.9766012361506\n","189: 14.667687760957051 1.3503682778391521 16.018056096392684\n","190: 14.991590832010843 1.5509680400718935 16.54255884862505\n","191: 14.980333651939873 1.4529777568677673 16.433311388245784\n","192: 16.055432476336136 1.5198702436464373 17.575302729499526\n","193: 15.66176124336198 1.3670418845722452 17.028803117456846\n","194: 15.025420958234463 1.3326039809471695 16.358024974004366\n","195: 13.791351391177159 1.4329357929964317 15.224287194898352\n","196: 14.08658781426493 1.366315022620256 15.45290286326781\n","197: 14.424323351122439 1.366645104091731 15.790968421963044\n","198: 14.703285673225764 1.170641252581845 15.873926953761838\n","199: 14.34394997858908 1.3027926697977819 15.646742667071521\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"doR5kS81SW5F"},"source":["# Save model"],"id":"doR5kS81SW5F"},{"cell_type":"code","metadata":{"id":"frozen-damage","executionInfo":{"status":"ok","timestamp":1625163007168,"user_tz":-60,"elapsed":11,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_cifar10')\n","# model = models.__dict__['ResNet18']()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_cifar10'))"],"id":"frozen-damage","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLlLnmpRSZAl"},"source":["# Test on Test Data"],"id":"BLlLnmpRSZAl"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aboriginal-lafayette","executionInfo":{"status":"ok","timestamp":1625163010408,"user_tz":-60,"elapsed":3247,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"3e0aca37-c66e-442c-acc4-a3680884dd98"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"aboriginal-lafayette","execution_count":10,"outputs":[{"output_type":"stream","text":["0.9408\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xHj0XxoMScbg"},"source":["# Test on Train Data"],"id":"xHj0XxoMScbg"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"therapeutic-orlando","executionInfo":{"status":"ok","timestamp":1625163031782,"user_tz":-60,"elapsed":21377,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"4b62fcd2-6e44-4a3e-80e8-ae12c94fde07"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"therapeutic-orlando","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9998\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XCvQUWwMSetI","executionInfo":{"status":"ok","timestamp":1625163031786,"user_tz":-60,"elapsed":30,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"XCvQUWwMSetI","execution_count":11,"outputs":[]}]}
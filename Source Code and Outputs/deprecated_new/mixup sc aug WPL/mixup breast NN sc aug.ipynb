{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN sc aug.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625148438959,"user_tz":-60,"elapsed":29093,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"3d7ab08e-1dcb-41bf-eafd-a658ab65bd91"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625148439774,"user_tz":-60,"elapsed":822,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625148440136,"user_tz":-60,"elapsed":363,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625148440136,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"ee36b51c-c132-4af3-ed9a-e834af702330"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f310c570a50>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625148440137,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625148446862,"user_tz":-60,"elapsed":6729,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"b539e875-60bf-43e9-ff86-3a8505b4d32b"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"mxcQ0s-iYgel","executionInfo":{"status":"ok","timestamp":1625148446862,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_sc(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_uc_list = list()\n","    labels_uc_list = list()\n","    unique_classes = torch.unique(labels)\n","    for uc in unique_classes:\n","        mask_uc = (labels == uc).flatten()  # flatten to avoid the labels are in column vector\n","        inputs_uc = inputs[mask_uc]\n","        labels_uc = labels[mask_uc]\n","        batch_size_uc = labels_uc.size(0)\n","        idx = torch.randperm(batch_size_uc).to('cuda')\n","        mixup_inputs_uc = lmbda * inputs_uc + (1 - lmbda) * inputs_uc[idx]\n","        mixup_inputs_uc_list.append(mixup_inputs_uc)\n","        labels_uc_list.append(labels_uc)\n","    mixup_inputs_sc = torch.vstack(mixup_inputs_uc_list)\n","    mixup_labels_sc = torch.cat(labels_uc_list, dim=0)  # use cat not hstack to avoid labels are in column vector\n","    return mixup_inputs_sc, mixup_labels_sc, lmbda"],"id":"mxcQ0s-iYgel","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625148449987,"user_tz":-60,"elapsed":3127,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"878f04d8-b069-4a66-cea8-cf0fc72a3171"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup perturbation with same class #\n","        mixup_inputs_sc, mixup_labels_sc, lmbda = mixup_breast_sc(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_sc))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_sc = augment_outputs[original_num:]\n","        mixup_loss_sc = criterion(mixup_outputs_sc, mixup_labels_sc)  # no need to mix the loss up since it is now mixup with same class, just use ordinary cross entropy\n","        loss = criterion(outputs, labels)\n","        augment_loss = mixup_loss_sc + loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_sc.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += augment_loss.item()\n","\n","        # Gradient Calculation & Optimisation #\n","        augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":8,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.1238932013511658 2.126030206680298 4.249923467636108\n","1: 2.0530518293380737 2.059046447277069 4.112098336219788\n","2: 1.9367963671684265 1.9440348148345947 3.8808311223983765\n","3: 1.7782453298568726 1.798176109790802 3.5764214992523193\n","4: 1.599032700061798 1.6278765201568604 3.2269091606140137\n","5: 1.4120173156261444 1.442177802324295 2.8541951179504395\n","6: 1.192524492740631 1.2384960651397705 2.4310206174850464\n","7: 0.9470098614692688 1.043838232755661 1.9908481240272522\n","8: 0.8199081718921661 0.8714916408061981 1.691399872303009\n","9: 0.6453150510787964 0.713613748550415 1.3589287996292114\n","10: 0.4202774092555046 0.5845224261283875 1.0047998130321503\n","11: 0.3810076266527176 0.5189291536808014 0.8999367952346802\n","12: 0.30955203622579575 0.44222812354564667 0.7517801523208618\n","13: 0.2851589173078537 0.3946606367826462 0.6798195540904999\n","14: 0.2590476796030998 0.3494996652007103 0.6085473448038101\n","15: 0.19526977837085724 0.33350515365600586 0.5287749320268631\n","16: 0.13631433993577957 0.295011468231678 0.431325800716877\n","17: 0.15121223777532578 0.2723611369729042 0.4235733598470688\n","18: 0.20374421402812004 0.26790355145931244 0.4716477543115616\n","19: 0.17028217390179634 0.24619989842176437 0.416482076048851\n","20: 0.1142708994448185 0.26148734241724014 0.37575824558734894\n","21: 0.15331871062517166 0.2271517962217331 0.38047050684690475\n","22: 0.051345134153962135 0.21912772580981255 0.27047285437583923\n","23: 0.08213496208190918 0.21215185895562172 0.2942868173122406\n","24: 0.07284596096724272 0.21726858615875244 0.29011454433202744\n","25: 0.07871062867343426 0.21790317445993423 0.29661380499601364\n","26: 0.11708969436585903 0.2106195203959942 0.32770922034978867\n","27: 0.06462252978235483 0.18714907113462687 0.25177159532904625\n","28: 0.12816207110881805 0.201106209307909 0.32926828414201736\n","29: 0.132655818015337 0.19941433519124985 0.33207015693187714\n","30: 0.042725526727735996 0.19224661588668823 0.23497214540839195\n","31: 0.11441556364297867 0.20064953714609146 0.31506510078907013\n","32: 0.04958834871649742 0.18936480954289436 0.23895315825939178\n","33: 0.162653929553926 0.2181042693555355 0.3807581998407841\n","34: 0.0909433625638485 0.19262664392590523 0.2835699990391731\n","35: 0.06591587141156197 0.17714721709489822 0.2430630885064602\n","36: 0.13898460566997528 0.20331700146198273 0.342301607131958\n","37: 0.11683983448892832 0.17588100209832191 0.29272083565592766\n","38: 0.04594064876437187 0.17017829231917858 0.2161189466714859\n","39: 0.06903219874948263 0.16328440699726343 0.23231660388410091\n","40: 0.08726807869970798 0.17642397060990334 0.2636920437216759\n","41: 0.05019827652722597 0.17419323697686195 0.22439150884747505\n","42: 0.07556967716664076 0.17604372650384903 0.25161340087652206\n","43: 0.050575729459524155 0.1793728806078434 0.22994860634207726\n","44: 0.07009115349501371 0.18494334258139133 0.2550344876945019\n","45: 0.10634460113942623 0.16350162029266357 0.26984621956944466\n","46: 0.0982295498251915 0.171097744256258 0.2693272978067398\n","47: 0.03220694651827216 0.15726679377257824 0.1894737370312214\n","48: 0.06437011994421482 0.15785509534180164 0.22222522273659706\n","49: 0.06367928441613913 0.17013852298259735 0.23381780833005905\n","50: 0.035534477792680264 0.15717396885156631 0.19270844385027885\n","51: 0.04802884720265865 0.1892740447074175 0.23730289191007614\n","52: 0.06364516727626324 0.16999283991754055 0.23363801091909409\n","53: 0.07532023638486862 0.1515428964048624 0.22686313465237617\n","54: 0.06233809748664498 0.1814270205795765 0.2437651213258505\n","55: 0.08383189141750336 0.1525472179055214 0.23637910559773445\n","56: 0.10191290453076363 0.16815497167408466 0.27006787061691284\n","57: 0.05148180667310953 0.15629202872514725 0.2077738381922245\n","58: 0.02880726009607315 0.14521494880318642 0.17402220889925957\n","59: 0.07122849300503731 0.1552322506904602 0.2264607474207878\n","60: 0.03343544062227011 0.15577274188399315 0.18920818343758583\n","61: 0.05832186434417963 0.16519933566451073 0.22352120652794838\n","62: 0.036906955763697624 0.16163041070103645 0.19853736832737923\n","63: 0.10523056797683239 0.17163100466132164 0.2768615633249283\n","64: 0.0798549996688962 0.15066828951239586 0.23052329197525978\n","65: 0.03713114466518164 0.15138302836567163 0.18851417861878872\n","66: 0.06901358347386122 0.13643141090869904 0.20544499158859253\n","67: 0.07373659545555711 0.1406918354332447 0.21442843228578568\n","68: 0.057460238225758076 0.14316543191671371 0.20062566921114922\n","69: 0.0320119964890182 0.1442924216389656 0.1763044223189354\n","70: 0.06802916433662176 0.13298935256898403 0.20101852342486382\n","71: 0.03804757120087743 0.1591509161517024 0.19719848223030567\n","72: 0.06167338462546468 0.1352853886783123 0.1969587691128254\n","73: 0.036609955597668886 0.12799871433526278 0.1646086648106575\n","74: 0.025845044292509556 0.14023859426379204 0.16608364135026932\n","75: 0.05479882122017443 0.1288757286965847 0.18367455527186394\n","76: 0.04538151575252414 0.12679545022547245 0.17217696458101273\n","77: 0.043660759925842285 0.13872121274471283 0.18238196894526482\n","78: 0.085030822083354 0.1317831538617611 0.21681397408246994\n","79: 0.017405332531780005 0.13682056590914726 0.1542258970439434\n","80: 0.08129305206239223 0.12660785019397736 0.20790090411901474\n","81: 0.01682695047929883 0.13556106388568878 0.15238801389932632\n","82: 0.04338787496089935 0.13290955871343613 0.17629743367433548\n","83: 0.03970314236357808 0.11816476751118898 0.15786790661513805\n","84: 0.031992340460419655 0.13754623383283615 0.16953857243061066\n","85: 0.035877321381121874 0.13414020277559757 0.1700175255537033\n","86: 0.051469264551997185 0.12981346622109413 0.18128272891044617\n","87: 0.06282955221831799 0.12339259684085846 0.1862221509218216\n","88: 0.053299553226679564 0.12192375212907791 0.17522330582141876\n","89: 0.04182248329743743 0.12186000496149063 0.1636824943125248\n","90: 0.030244898982346058 0.1155329467728734 0.14577784575521946\n","91: 0.04639226431027055 0.12390603870153427 0.17029830440878868\n","92: 0.047445963602513075 0.11488542333245277 0.16233138926327229\n","93: 0.03202940942719579 0.13417264446616173 0.16620204970240593\n","94: 0.06227260269224644 0.11797606199979782 0.1802486628293991\n","95: 0.04443024564534426 0.12109622731804848 0.16552647575736046\n","96: 0.04587780265137553 0.12264675088226795 0.16852455399930477\n","97: 0.031486326828598976 0.11739202961325645 0.14887835457921028\n","98: 0.015073609538376331 0.12108216434717178 0.13615577295422554\n","99: 0.05056278686970472 0.11984269693493843 0.17040548846125603\n","100: 0.02493241522461176 0.11764593794941902 0.1425783522427082\n","101: 0.017808570992201567 0.10751443542540073 0.12532300874590874\n","102: 0.028558662626892328 0.11361502297222614 0.14217368140816689\n","103: 0.038230377715080976 0.12279655598104 0.16102693602442741\n","104: 0.05447736196219921 0.10473467223346233 0.15921203419566154\n","105: 0.014038710854947567 0.10254278592765331 0.11658150143921375\n","106: 0.02652331837452948 0.11150385066866875 0.13802716508507729\n","107: 0.015809975564479828 0.10118209850043058 0.11699207685887814\n","108: 0.05293133296072483 0.11301550641655922 0.1659468375146389\n","109: 0.028619933174923062 0.10088426247239113 0.12950419634580612\n","110: 0.04888410586863756 0.1044752188026905 0.1533593237400055\n","111: 0.024945778772234917 0.10111207887530327 0.12605785951018333\n","112: 0.02784256055019796 0.10596761479973793 0.13381017372012138\n","113: 0.04519151337444782 0.11794186756014824 0.1631333827972412\n","114: 0.036729563027620316 0.09636390954256058 0.1330934762954712\n","115: 0.031004371121525764 0.09701023437082767 0.12801460549235344\n","116: 0.01474635920021683 0.10406443849205971 0.11881079897284508\n","117: 0.06418271572329104 0.09392461832612753 0.15810733754187822\n","118: 0.015435939654707909 0.09814470820128918 0.11358064971864223\n","119: 0.041135014267638326 0.10045108944177628 0.14158610254526138\n","120: 0.058091649785637856 0.10176676698029041 0.15985841862857342\n","121: 0.05952345347031951 0.09321507811546326 0.1527385301887989\n","122: 0.029678340069949627 0.1135898120701313 0.1432681530714035\n","123: 0.015623199753463268 0.10015813633799553 0.11578133329749107\n","124: 0.033314000349491835 0.1034981980919838 0.13681219890713692\n","125: 0.0583368856459856 0.08967474102973938 0.14801162853837013\n","126: 0.05476049520075321 0.09418654628098011 0.14894704148173332\n","127: 0.04082480724900961 0.09516958706080914 0.13599439337849617\n","128: 0.022900310577824712 0.10067084804177284 0.12357116118073463\n","129: 0.031286862096749246 0.08518509566783905 0.11647195555269718\n","130: 0.021131898742169142 0.08294545952230692 0.10407735593616962\n","131: 0.0413665696978569 0.08897841908037663 0.13034499064087868\n","132: 0.030456931330263615 0.09121854417026043 0.12167547643184662\n","133: 0.025139850564301014 0.08352152351289988 0.10866137780249119\n","134: 0.007033123169094324 0.09244368597865105 0.09947680868208408\n","135: 0.031051897909492254 0.08785467967391014 0.11890657991170883\n","136: 0.029094074852764606 0.08366877026855946 0.11276284232735634\n","137: 0.03258392587304115 0.0838596411049366 0.11644356697797775\n","138: 0.024237697012722492 0.0836668536067009 0.10790454968810081\n","139: 0.015464204363524914 0.08272761479020119 0.09819181822240353\n","140: 0.02757437899708748 0.08576806075870991 0.11334243789315224\n","141: 0.02023970242589712 0.08150298334658146 0.1017426885664463\n","142: 0.035617140238173306 0.08822155417874455 0.12383869802579284\n","143: 0.016794504830613732 0.07690693158656359 0.0937014352530241\n","144: 0.01218185038305819 0.0753682591021061 0.08755011018365622\n","145: 0.04127396026160568 0.08532308042049408 0.12659704312682152\n","146: 0.03409530548378825 0.08318264316767454 0.11727795004844666\n","147: 0.03393944073468447 0.08408416248857975 0.11802360229194164\n","148: 0.009625249425880611 0.08307242766022682 0.09269767999649048\n","149: 0.024008827982470393 0.08155632857233286 0.10556515771895647\n","150: 0.05509615200571716 0.07082573184743524 0.12592187989503145\n","151: 0.017429518047720194 0.07420952059328556 0.09163904003798962\n","152: 0.03623981727287173 0.07372772134840488 0.10996753722429276\n","153: 0.05291940597817302 0.07290571834892035 0.12582512386143208\n","154: 0.01296461233869195 0.07632152177393436 0.08928613364696503\n","155: 0.02064510527998209 0.0706297941505909 0.09127489849925041\n","156: 0.028421591967344284 0.07602535933256149 0.10444694757461548\n","157: 0.03385834186337888 0.0789925642311573 0.11285090446472168\n","158: 0.05112390499562025 0.07673921808600426 0.12786312028765678\n","159: 0.014301048126071692 0.07597831077873707 0.09027935937047005\n","160: 0.009711339138448238 0.07996055297553539 0.08967189490795135\n","161: 0.026460349094122648 0.07536193542182446 0.10182228125631809\n","162: 0.015861106337979436 0.07778748124837875 0.09364858828485012\n","163: 0.0261668898165226 0.07409417256712914 0.10026106052100658\n","164: 0.03314049215987325 0.0826389268040657 0.11577941663563251\n","165: 0.03079708735458553 0.08177161682397127 0.11256870068609715\n","166: 0.02634482504799962 0.07838310394436121 0.10472792759537697\n","167: 0.021938419435173273 0.07766066677868366 0.09959908574819565\n","168: 0.013695658883079886 0.0749615766108036 0.08865723386406898\n","169: 0.0168529839720577 0.07177485059946775 0.08862783759832382\n","170: 0.024942176416516304 0.07221819646656513 0.09716037288308144\n","171: 0.02288155583664775 0.07621783018112183 0.09909938648343086\n","172: 0.046588501427322626 0.07310428004711866 0.11969277635216713\n","173: 0.028100936906412244 0.07903004437685013 0.1071309857070446\n","174: 0.024153073551133275 0.0812531802803278 0.105406254529953\n","175: 0.00973094068467617 0.07829919829964638 0.0880301408469677\n","176: 0.010290002916008234 0.07530269306153059 0.08559269644320011\n","177: 0.01278848061338067 0.07129724137485027 0.08408572059124708\n","178: 0.012138631427660584 0.08156353421509266 0.09370216727256775\n","179: 0.012763761915266514 0.07615493796765804 0.08891869895160198\n","180: 0.018336064647883177 0.07221267838031054 0.090548743493855\n","181: 0.01226068614050746 0.07475130259990692 0.08701198920607567\n","182: 0.03531070705503225 0.08326310478150845 0.11857381090521812\n","183: 0.02846684237010777 0.08036180585622787 0.10882864892482758\n","184: 0.029725931584835052 0.07441051676869392 0.10413644835352898\n","185: 0.016152087482623756 0.07534492015838623 0.09149700775742531\n","186: 0.018799967132508755 0.07471933029592037 0.0935192983597517\n","187: 0.013275107252411544 0.08300808072090149 0.0962831862270832\n","188: 0.01087608328089118 0.07826711516827345 0.08914319984614849\n","189: 0.048523067496716976 0.07196700572967529 0.12049007043242455\n","190: 0.02135237120091915 0.06986089330166578 0.09121326450258493\n","191: 0.022189503302797675 0.07093688286840916 0.0931263854727149\n","192: 0.01739388331770897 0.07480867765843868 0.0922025591135025\n","193: 0.030050531029701233 0.07324263826012611 0.10329316928982735\n","194: 0.026260641869157553 0.08159451372921467 0.10785515792667866\n","195: 0.029127367539331317 0.07479900307953358 0.10392637178301811\n","196: 0.024046608712524176 0.07804113812744617 0.10208774730563164\n","197: 0.02819808735512197 0.06999813858419657 0.09819622803479433\n","198: 0.018242311663925648 0.07053848635405302 0.08878079988062382\n","199: 0.021143185906112194 0.07012885808944702 0.09127204678952694\n","200: 0.030368416104465723 0.06870091147720814 0.09906932711601257\n","201: 0.023642418673262 0.08092667814344168 0.10456909844651818\n","202: 0.012198940850794315 0.07408068887889385 0.08627962879836559\n","203: 0.013761090114712715 0.07720683142542839 0.09096792340278625\n","204: 0.01104361703619361 0.071602588519454 0.08264620788395405\n","205: 0.015194513369351625 0.06989556644111872 0.08509008027613163\n","206: 0.04907290660776198 0.07117575034499168 0.12024866044521332\n","207: 0.02106833178550005 0.07362599018961191 0.09469432197511196\n","208: 0.026047077029943466 0.06888496410101652 0.09493204113095999\n","209: 0.028958235401660204 0.07660381495952606 0.10556205175817013\n","210: 0.01532699353992939 0.07754489779472351 0.09287189319729805\n","211: 0.009761853492818773 0.07021623756736517 0.0799780897796154\n","212: 0.01724358624778688 0.07187910936772823 0.08912269584834576\n","213: 0.01052174624055624 0.07549837231636047 0.08602011762559414\n","214: 0.01601525256410241 0.07203353755176067 0.08804878778755665\n","215: 0.03147873352281749 0.07692833431065083 0.10840706899762154\n","216: 0.007470820797607303 0.07230447232723236 0.07977529428899288\n","217: 0.02066122880205512 0.08159736962988973 0.10225859843194485\n","218: 0.020599066512659192 0.07360370829701424 0.09420277364552021\n","219: 0.01556728733703494 0.06904950644820929 0.08461679518222809\n","220: 0.030584911815822124 0.0798378698527813 0.11042277701199055\n","221: 0.010473382310010493 0.06659132009372115 0.07706470228731632\n","222: 0.010425431188195944 0.07061465829610825 0.08104008808732033\n","223: 0.01850239047780633 0.06982123665511608 0.08832362852990627\n","224: 0.028343798592686653 0.07420028746128082 0.10254408791661263\n","225: 0.03589429217390716 0.07082848437130451 0.10672277770936489\n","226: 0.013405327452346683 0.06951777171343565 0.08292309939861298\n","227: 0.011090075829997659 0.07041075639426708 0.08150083385407925\n","228: 0.014537536539137363 0.07227245904505253 0.08680999837815762\n","229: 0.016041143448092043 0.06757096759974957 0.083612110465765\n","230: 0.02566447446588427 0.07116109319031239 0.09682556800544262\n","231: 0.013585010543465614 0.07250703684985638 0.08609204925596714\n","232: 0.02115278597921133 0.07015395723283291 0.09130674414336681\n","233: 0.006704109255224466 0.07019289117306471 0.07689699996262789\n","234: 0.011190027929842472 0.06829693261533976 0.07948696240782738\n","235: 0.029125493252649903 0.06625657109543681 0.09538206458091736\n","236: 0.015259905252605677 0.06797938887029886 0.08323929458856583\n","237: 0.025368841597810388 0.07473595067858696 0.10010479390621185\n","238: 0.014041322283446789 0.07637358829379082 0.09041490964591503\n","239: 0.02745095081627369 0.07012195698916912 0.09757290780544281\n","240: 0.01739709242247045 0.06904703751206398 0.08644413016736507\n","241: 0.03448890894651413 0.07493896409869194 0.10942787304520607\n","242: 0.02553460607305169 0.07090716063976288 0.09644176438450813\n","243: 0.025399284902960062 0.07449156511574984 0.09989085048437119\n","244: 0.037138765677809715 0.08370375353842974 0.12084251921623945\n","245: 0.025101081002503633 0.06985359452664852 0.09495467692613602\n","246: 0.020293771754950285 0.07571031525731087 0.09600408747792244\n","247: 0.024362802505493164 0.08121464680880308 0.1055774474516511\n","248: 0.013822084409184754 0.0704819168895483 0.08430400118231773\n","249: 0.011654723202809691 0.06883491761982441 0.0804896429181099\n","250: 0.013225510017946362 0.07036788575351238 0.08359339646995068\n","251: 0.01814704725984484 0.07727614603936672 0.09542319178581238\n","252: 0.008039646432735026 0.066854452714324 0.07489410042762756\n","253: 0.03049872536212206 0.06731917429715395 0.09781790152192116\n","254: 0.04496078286319971 0.07347730547189713 0.11843808740377426\n","255: 0.02249612403102219 0.07115652784705162 0.09365265443921089\n","256: 0.006317478371784091 0.07334716059267521 0.07966464012861252\n","257: 0.02009734371677041 0.07468512654304504 0.09478246979415417\n","258: 0.04148895130492747 0.07679056376218796 0.11827951297163963\n","259: 0.022911213105544448 0.06476483214646578 0.08767604269087315\n","260: 0.03247210755944252 0.06805071048438549 0.10052281618118286\n","261: 0.026256218901835382 0.06669742800295353 0.0929536484181881\n","262: 0.011226321803405881 0.0661710174754262 0.07739733904600143\n","263: 0.015476524364203215 0.07316838391125202 0.08864490874111652\n","264: 0.03959982842206955 0.078533248975873 0.11813307926058769\n","265: 0.034676242619752884 0.0757228173315525 0.11039905995130539\n","266: 0.014128684531897306 0.06827597878873348 0.08240466378629208\n","267: 0.008339574211277068 0.0710399504750967 0.07937952410429716\n","268: 0.034302913350984454 0.06787778064608574 0.10218069329857826\n","269: 0.022999203531071544 0.06365707027725875 0.08665627241134644\n","270: 0.012264305260032415 0.06628788635134697 0.07855219300836325\n","271: 0.014193034963682294 0.0732700414955616 0.08746307715773582\n","272: 0.01137877837754786 0.06611472275108099 0.0774935008957982\n","273: 0.01848536031320691 0.06856662128120661 0.0870519820600748\n","274: 0.022020111326128244 0.072916679084301 0.0949367918074131\n","275: 0.021312066121026874 0.07246357202529907 0.09377563931047916\n","276: 0.008368625538423657 0.06643262598663568 0.0748012512922287\n","277: 0.05492502171546221 0.0668634744361043 0.12178849242627621\n","278: 0.01949190255254507 0.06698227487504482 0.08647417649626732\n","279: 0.019494460662826896 0.0731187704950571 0.09261323139071465\n","280: 0.02606927347369492 0.06952419318258762 0.09559346549212933\n","281: 0.017026093788444996 0.06759811751544476 0.08462421223521233\n","282: 0.016246351646259427 0.06707915104925632 0.08332550153136253\n","283: 0.01663617743179202 0.0682534258812666 0.0848896037787199\n","284: 0.028563253581523895 0.0696181058883667 0.0981813594698906\n","285: 0.00606329704169184 0.07504085637629032 0.08110415004193783\n","286: 0.0223139391746372 0.06325504556298256 0.0855689849704504\n","287: 0.012247406644746661 0.06918355263769627 0.081430958583951\n","288: 0.02704769605770707 0.06806270591914654 0.09511040151119232\n","289: 0.018970845034345984 0.06475254893302917 0.0837233942002058\n","290: 0.006596027466002852 0.07255592197179794 0.07915194891393185\n","291: 0.008613592945039272 0.06816852651536465 0.0767821203917265\n","292: 0.022558209719136357 0.06814840994775295 0.09070662036538124\n","293: 0.008998337900266051 0.06962060183286667 0.07861893996596336\n","294: 0.016780842328444123 0.06706633046269417 0.0838471706956625\n","295: 0.023383648600429296 0.06821833364665508 0.09160198085010052\n","296: 0.007949986262246966 0.06289150658994913 0.07084149215370417\n","297: 0.025557198561728 0.07395127136260271 0.09950847178697586\n","298: 0.022490484174340963 0.06599375978112221 0.08848424442112446\n","299: 0.01624073530547321 0.0691110547631979 0.08535178937017918\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625148449988,"user_tz":-60,"elapsed":14,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625148449988,"user_tz":-60,"elapsed":13,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"aa3c010f-9984-4fb1-b0e1-17caf1d52413"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":10,"outputs":[{"output_type":"stream","text":["0.9736842105263158\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625148449989,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"ba0838ca-a95f-4f99-b9c1-4914c19d10cf"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9912023460410557\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625148449989,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":11,"outputs":[]}]}
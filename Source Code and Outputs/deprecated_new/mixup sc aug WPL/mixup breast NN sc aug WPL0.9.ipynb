{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN sc aug WPL0.9.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625148817368,"user_tz":-60,"elapsed":19215,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"54a9669f-fc85-4a92-f17d-e452907f48ef"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625148818314,"user_tz":-60,"elapsed":947,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625148818686,"user_tz":-60,"elapsed":376,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625148818686,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"afc285f4-b293-427e-b308-ece76880c657"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","perturb_loss_weight = 0.9\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fcb8b5c4a90>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625148818892,"user_tz":-60,"elapsed":208,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625148825395,"user_tz":-60,"elapsed":6504,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"b1da6891-2074-410c-a35b-d328b25ab665"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"fkMC5_yoZnWd","executionInfo":{"status":"ok","timestamp":1625148825395,"user_tz":-60,"elapsed":2,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_sc(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_uc_list = list()\n","    labels_uc_list = list()\n","    unique_classes = torch.unique(labels)\n","    for uc in unique_classes:\n","        mask_uc = (labels == uc).flatten()  # flatten to avoid the labels are in column vector\n","        inputs_uc = inputs[mask_uc]\n","        labels_uc = labels[mask_uc]\n","        batch_size_uc = labels_uc.size(0)\n","        idx = torch.randperm(batch_size_uc).to('cuda')\n","        mixup_inputs_uc = lmbda * inputs_uc + (1 - lmbda) * inputs_uc[idx]\n","        mixup_inputs_uc_list.append(mixup_inputs_uc)\n","        labels_uc_list.append(labels_uc)\n","    mixup_inputs_sc = torch.vstack(mixup_inputs_uc_list)\n","    mixup_labels_sc = torch.cat(labels_uc_list, dim=0)  # use cat not hstack to avoid labels are in column vector\n","    return mixup_inputs_sc, mixup_labels_sc, lmbda"],"id":"fkMC5_yoZnWd","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625148829740,"user_tz":-60,"elapsed":4347,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"d38bccec-503e-4dce-fffd-d72d3387ed82"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup perturbation with same class #\n","        mixup_inputs_sc, mixup_labels_sc, lmbda = mixup_breast_sc(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_sc))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_sc = augment_outputs[original_num:]\n","        mixup_loss_sc = criterion(mixup_outputs_sc, mixup_labels_sc)  # no need to mix the loss up since it is now mixup with same class, just use ordinary cross entropy\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_sc + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_sc.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_sc.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":8,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.1309109330177307 2.132653534412384 4.263564467430115\n","1: 2.0938700437545776 2.0972601175308228 4.1911301612854\n","2: 2.025959014892578 2.0302228927612305 4.056181907653809\n","3: 1.942561149597168 1.953150451183319 3.895711600780487\n","4: 1.8498092293739319 1.8644000887870789 3.7142093181610107\n","5: 1.7482298612594604 1.7631969451904297 3.51142680644989\n","6: 1.6360921263694763 1.6588968634605408 3.294988989830017\n","7: 1.5046106278896332 1.544646441936493 3.049257069826126\n","8: 1.4028516709804535 1.4257733821868896 2.828625053167343\n","9: 1.2612046301364899 1.29188472032547 2.55308935046196\n","10: 1.0808428823947906 1.1627400517463684 2.243582934141159\n","11: 0.9720733165740967 1.0411165952682495 2.013189911842346\n","12: 0.8589505255222321 0.924095094203949 1.783045619726181\n","13: 0.7484569847583771 0.8240180015563965 1.5724749863147736\n","14: 0.6513710469007492 0.7155369967222214 1.3669080436229706\n","15: 0.5483465641736984 0.6557773947715759 1.2041239589452744\n","16: 0.45945411920547485 0.5816868096590042 1.041140928864479\n","17: 0.4150330349802971 0.5273985266685486 0.9424315616488457\n","18: 0.4232727810740471 0.48032842576503754 0.9036012068390846\n","19: 0.3742273226380348 0.4368835687637329 0.8111108914017677\n","20: 0.3019016459584236 0.4254687875509262 0.7273704335093498\n","21: 0.3125622943043709 0.37364280968904495 0.6862051039934158\n","22: 0.19147473573684692 0.33975163474678993 0.5312263704836369\n","23: 0.20649277418851852 0.3416360691189766 0.5481288433074951\n","24: 0.17112423479557037 0.3253597319126129 0.4964839667081833\n","25: 0.18423137441277504 0.30192528665065765 0.4861566610634327\n","26: 0.24394313246011734 0.3095189779996872 0.5534621104598045\n","27: 0.11858535557985306 0.2819889336824417 0.40057428926229477\n","28: 0.19046013429760933 0.2725898548960686 0.4630499891936779\n","29: 0.16325867176055908 0.26595648378133774 0.4292151555418968\n","30: 0.10642247460782528 0.270463727414608 0.3768862020224333\n","31: 0.17927182093262672 0.2503506727516651 0.42962249368429184\n","32: 0.08535663038492203 0.2398635894060135 0.3252202197909355\n","33: 0.17654145881533623 0.24633024632930756 0.4228717051446438\n","34: 0.12386586889624596 0.22072509303689003 0.344590961933136\n","35: 0.10107554122805595 0.23233462125062943 0.3334101624786854\n","36: 0.1436472125351429 0.2239958494901657 0.3676430620253086\n","37: 0.14092568680644035 0.20926491916179657 0.3501906059682369\n","38: 0.12318205088376999 0.21280119568109512 0.3359832465648651\n","39: 0.11607170663774014 0.21692632883787155 0.3329980354756117\n","40: 0.11055418848991394 0.19856296852231026 0.3091171570122242\n","41: 0.11052636802196503 0.19873512908816338 0.3092614971101284\n","42: 0.0775369256734848 0.20055720955133438 0.2780941352248192\n","43: 0.07605367060750723 0.1984669603407383 0.2745206309482455\n","44: 0.07165001425892115 0.18930203840136528 0.2609520526602864\n","45: 0.12476702034473419 0.2018621452152729 0.3266291655600071\n","46: 0.06572584621608257 0.18089386075735092 0.2466197069734335\n","47: 0.06618237681686878 0.17601721361279488 0.24219959042966366\n","48: 0.09279636852443218 0.18635156750679016 0.27914793603122234\n","49: 0.07462464272975922 0.17037111707031727 0.24499575980007648\n","50: 0.07894960418343544 0.1845962032675743 0.26354580745100975\n","51: 0.09078717418015003 0.17338508367538452 0.26417225785553455\n","52: 0.09586224425584078 0.17260858789086342 0.2684708321467042\n","53: 0.07582790497690439 0.1720968335866928 0.2479247385635972\n","54: 0.06259133480489254 0.16573139280080795 0.2283227276057005\n","55: 0.10382669419050217 0.16616899520158768 0.26999568939208984\n","56: 0.09416824020445347 0.17103715240955353 0.265205392614007\n","57: 0.0792403444647789 0.17425672709941864 0.25349707156419754\n","58: 0.06437211856245995 0.16980748996138573 0.23417960852384567\n","59: 0.06325606442987919 0.1573085691779852 0.22056463360786438\n","60: 0.05259046331048012 0.1768554113805294 0.22944587469100952\n","61: 0.0600900761783123 0.15553938783705235 0.21562946401536465\n","62: 0.03641484584659338 0.16207280382514 0.19848764967173338\n","63: 0.07943286839872599 0.15681184455752373 0.23624471295624971\n","64: 0.08854520693421364 0.14981677569448948 0.23836198262870312\n","65: 0.05895412899553776 0.16497152298688889 0.22392565198242664\n","66: 0.09637520834803581 0.16846996918320656 0.26484517753124237\n","67: 0.09038055501878262 0.1582840345799923 0.2486645895987749\n","68: 0.08396145794540644 0.15080079808831215 0.23476225603371859\n","69: 0.04430318716913462 0.14864488318562508 0.1929480703547597\n","70: 0.05278166010975838 0.14707839488983154 0.19986005499958992\n","71: 0.04254690697416663 0.16647958755493164 0.20902649452909827\n","72: 0.07513654883950949 0.16841872408986092 0.2435552729293704\n","73: 0.030811975710093975 0.1544194407761097 0.18523141648620367\n","74: 0.059683529660105705 0.15873916074633598 0.2184226904064417\n","75: 0.0449902443215251 0.1532873660326004 0.1982776103541255\n","76: 0.056768523529171944 0.14891070872545242 0.20567923225462437\n","77: 0.07511371793225408 0.1588185951113701 0.23393231304362416\n","78: 0.10753676667809486 0.15030525997281075 0.2578420266509056\n","79: 0.038703286089003086 0.14514770731329918 0.18385099340230227\n","80: 0.09485196322202682 0.13944760896265507 0.2342995721846819\n","81: 0.03031007992103696 0.14602631330490112 0.17633639322593808\n","82: 0.05513191036880016 0.1479763239622116 0.20310823433101177\n","83: 0.05407750001177192 0.14300046488642693 0.19707796489819884\n","84: 0.07534520979970694 0.13264294154942036 0.2079881513491273\n","85: 0.05900726839900017 0.1429731696844101 0.20198043808341026\n","86: 0.03980137873440981 0.13269896991550922 0.17250034864991903\n","87: 0.0845007635653019 0.1483525112271309 0.23285327479243279\n","88: 0.05912120221182704 0.13343962654471397 0.192560828756541\n","89: 0.07120203133672476 0.13236095383763313 0.2035629851743579\n","90: 0.05589188635349274 0.1412147656083107 0.19710665196180344\n","91: 0.04196985624730587 0.1420810930430889 0.18405094929039478\n","92: 0.055781313218176365 0.13167355209589005 0.1874548653140664\n","93: 0.057781441137194633 0.13466106355190277 0.1924425046890974\n","94: 0.04857208579778671 0.13114473596215248 0.1797168217599392\n","95: 0.05299075320363045 0.13734959438443184 0.1903403475880623\n","96: 0.06726186815649271 0.1407189704477787 0.2079808386042714\n","97: 0.04275396838784218 0.12258372828364372 0.1653376966714859\n","98: 0.060517746955156326 0.1380787082016468 0.19859645515680313\n","99: 0.07652697246521711 0.14065338671207428 0.2171803591772914\n","100: 0.03587568877264857 0.12741564586758614 0.1632913346402347\n","101: 0.019972022622823715 0.12254336848855019 0.1425153911113739\n","102: 0.03079981543123722 0.13213151320815086 0.16293132863938808\n","103: 0.028075316920876503 0.13552914187312126 0.16360445879399776\n","104: 0.058517178520560265 0.12756634876132011 0.18608352728188038\n","105: 0.037339881993830204 0.1204316783696413 0.1577715603634715\n","106: 0.023620459716767073 0.1363576091825962 0.15997806889936328\n","107: 0.0356063493527472 0.13575800880789757 0.17136435816064477\n","108: 0.03313648467883468 0.11983766406774521 0.15297414874657989\n","109: 0.022352627012878656 0.12239401787519455 0.1447466448880732\n","110: 0.05709076765924692 0.1239698976278305 0.18106066528707743\n","111: 0.039899105206131935 0.13502152636647224 0.17492063157260418\n","112: 0.029480041936039925 0.11562414932996035 0.14510419126600027\n","113: 0.04922271892428398 0.11748522706329823 0.1667079459875822\n","114: 0.0776427686214447 0.12082501128315926 0.19846777990460396\n","115: 0.0770157272927463 0.13018766045570374 0.20720338774845004\n","116: 0.023690269328653812 0.12348802573978901 0.14717829506844282\n","117: 0.0736381565220654 0.11739252880215645 0.19103068532422185\n","118: 0.01883775368332863 0.11734267696738243 0.13618043065071106\n","119: 0.08824745938181877 0.13127228990197182 0.2195197492837906\n","120: 0.07101508602499962 0.12922438606619835 0.20023947209119797\n","121: 0.07821377040818334 0.1255740188062191 0.20378778921440244\n","122: 0.06308014132082462 0.11623449251055717 0.1793146338313818\n","123: 0.020856547635048628 0.12023642845451832 0.14109297608956695\n","124: 0.058077358175069094 0.1290096677839756 0.1870870259590447\n","125: 0.05680443998426199 0.12098425254225731 0.1777886925265193\n","126: 0.04458488058298826 0.11061956081539392 0.1552044413983822\n","127: 0.0688878356013447 0.10858360677957535 0.17747144238092005\n","128: 0.05505868885666132 0.1373881045728922 0.1924467934295535\n","129: 0.04998229583725333 0.11368408240377903 0.16366637824103236\n","130: 0.05009276419878006 0.11859656497836113 0.1686893291771412\n","131: 0.050127269234508276 0.1103122178465128 0.16043948708102107\n","132: 0.08158693183213472 0.12806567549705505 0.20965260732918978\n","133: 0.07320473343133926 0.12735985964536667 0.20056459307670593\n","134: 0.029135020449757576 0.11260320991277695 0.14173823036253452\n","135: 0.040072510950267315 0.11431686207652092 0.15438937302678823\n","136: 0.02957143960520625 0.11534741520881653 0.14491885481402278\n","137: 0.04128565080463886 0.10882212035357952 0.15010777115821838\n","138: 0.020521180238574743 0.10650904104113579 0.12703022127971053\n","139: 0.036503275856375694 0.11446306854486465 0.15096634440124035\n","140: 0.0226850975304842 0.11054909229278564 0.13323418982326984\n","141: 0.03207333479076624 0.11039187386631966 0.1424652086570859\n","142: 0.049714392982423306 0.11623761802911758 0.1659520110115409\n","143: 0.02006502542644739 0.12147849053144455 0.14154351595789194\n","144: 0.014533322071656585 0.10464254394173622 0.1191758660133928\n","145: 0.03781785583123565 0.10529393889009953 0.14311179472133517\n","146: 0.04117531329393387 0.11200108751654625 0.15317640081048012\n","147: 0.042736275121569633 0.11707021109759808 0.1598064862191677\n","148: 0.02191512379795313 0.11971292272210121 0.14162804652005434\n","149: 0.011786127230152488 0.1212207693606615 0.133006896590814\n","150: 0.07349931169301271 0.1057640053331852 0.1792633170261979\n","151: 0.023885477567091584 0.10093249380588531 0.1248179713729769\n","152: 0.05120393820106983 0.10237523354589939 0.15357917174696922\n","153: 0.05225268192589283 0.10545053333044052 0.15770321525633335\n","154: 0.02809453383088112 0.10380405187606812 0.13189858570694923\n","155: 0.02907740045338869 0.09989871457219124 0.12897611502557993\n","156: 0.02964227180927992 0.10356973297894001 0.13321200478821993\n","157: 0.055512093706056476 0.10979586094617844 0.1653079546522349\n","158: 0.04160633776336908 0.10499841347336769 0.14660475123673677\n","159: 0.0246366742067039 0.10620502009987831 0.1308416943065822\n","160: 0.020608251448720694 0.10681341402232647 0.12742166547104716\n","161: 0.04451869614422321 0.10617508552968502 0.15069378167390823\n","162: 0.0346373796928674 0.10281561687588692 0.13745299656875432\n","163: 0.037016093730926514 0.10748879984021187 0.14450489357113838\n","164: 0.03605590062215924 0.10252727009356022 0.13858317071571946\n","165: 0.02436506631784141 0.1061670146882534 0.1305320810060948\n","166: 0.04651963151991367 0.10237862914800644 0.1488982606679201\n","167: 0.018337692134082317 0.1004044571891427 0.11874214932322502\n","168: 0.027664994122460485 0.09935174696147442 0.1270167410839349\n","169: 0.02916614105924964 0.11378007382154465 0.1429462148807943\n","170: 0.036791592836380005 0.10470828786492348 0.14149988070130348\n","171: 0.04362891335040331 0.10635526664555073 0.14998417999595404\n","172: 0.05891989264637232 0.11614063382148743 0.17506052646785975\n","173: 0.07323421793989837 0.11906471289694309 0.19229893083684146\n","174: 0.0378330962266773 0.1122492142021656 0.1500823104288429\n","175: 0.04357487475499511 0.11969429068267345 0.16326916543766856\n","176: 0.04522063257172704 0.1255083568394184 0.17072898941114545\n","177: 0.03327532741241157 0.10685164108872414 0.1401269685011357\n","178: 0.023814614629372954 0.09927978552877903 0.12309440015815198\n","179: 0.014688797295093536 0.12144016474485397 0.1361289620399475\n","180: 0.013951272936537862 0.10288131795823574 0.1168325908947736\n","181: 0.019581242464482784 0.11609671078622341 0.1356779532507062\n","182: 0.08550046477466822 0.12226808443665504 0.20776854921132326\n","183: 0.031184536404907703 0.10704759322106838 0.13823212962597609\n","184: 0.036917007993906736 0.11704638227820396 0.1539633902721107\n","185: 0.03667977685108781 0.10651109740138054 0.14319087425246835\n","186: 0.024570412002503872 0.10222668945789337 0.12679710146039724\n","187: 0.028499568812549114 0.11081992089748383 0.13931948971003294\n","188: 0.01597222313284874 0.11224859952926636 0.1282208226621151\n","189: 0.06248295493423939 0.10436076484620571 0.1668437197804451\n","190: 0.02595780766569078 0.10668346844613552 0.1326412761118263\n","191: 0.04951216513291001 0.1017348300665617 0.1512469951994717\n","192: 0.019133024150505662 0.09892112575471401 0.11805414990521967\n","193: 0.016205964609980583 0.10510622709989548 0.12131219170987606\n","194: 0.04360397928394377 0.09792549582198262 0.1415294751059264\n","195: 0.03173685120418668 0.1235931795090437 0.15533003071323037\n","196: 0.05372830666601658 0.12142023257911205 0.17514853924512863\n","197: 0.06033935770392418 0.10533510148525238 0.16567445918917656\n","198: 0.03916496830061078 0.10415471158921719 0.14331967988982797\n","199: 0.03850291855633259 0.11546562239527702 0.1539685409516096\n","200: 0.03469902672804892 0.0981232225894928 0.13282224931754172\n","201: 0.042307764291763306 0.10242848470807076 0.14473624899983406\n","202: 0.04914158722385764 0.10425183549523354 0.15339342271909118\n","203: 0.04644476110115647 0.09975241962820292 0.1461971807293594\n","204: 0.024883736157789826 0.10365480929613113 0.12853854545392096\n","205: 0.04425075696781278 0.10029096063226461 0.1445417176000774\n","206: 0.042999190744012594 0.10614746436476707 0.14914665510877967\n","207: 0.01915867906063795 0.11354801803827286 0.1327066970989108\n","208: 0.055124519392848015 0.10663526318967342 0.16175978258252144\n","209: 0.04756616149097681 0.10188810713589191 0.14945426862686872\n","210: 0.03198828233871609 0.09945151396095753 0.13143979629967362\n","211: 0.014056575950235128 0.1090479027479887 0.12310447869822383\n","212: 0.06449653021991253 0.11751104891300201 0.18200757913291454\n","213: 0.0246749147772789 0.10747711546719074 0.13215203024446964\n","214: 0.027155187912285328 0.09937587101012468 0.12653105892241\n","215: 0.04870484955608845 0.11638212949037552 0.16508697904646397\n","216: 0.014159705257043242 0.10310930386185646 0.1172690091188997\n","217: 0.015534936566837132 0.09857365302741528 0.11410858959425241\n","218: 0.03867365885525942 0.09975075162947178 0.1384244104847312\n","219: 0.025790960527956486 0.11717084608972073 0.1429618066176772\n","220: 0.022935257758945227 0.10843625105917454 0.13137150881811976\n","221: 0.01972390804439783 0.10578154772520065 0.12550545576959848\n","222: 0.020283044083043933 0.11048499494791031 0.13076803903095424\n","223: 0.028196365106850863 0.09716067928820848 0.12535704439505935\n","224: 0.03503149561583996 0.10765773802995682 0.14268923364579678\n","225: 0.0739449355751276 0.11789252236485481 0.19183745793998241\n","226: 0.015300344675779343 0.10067767091095448 0.11597801558673382\n","227: 0.01768822828307748 0.10129712335765362 0.1189853516407311\n","228: 0.043695126194506884 0.11801533540710807 0.16171046160161495\n","229: 0.02962093986570835 0.10666119679808617 0.13628213666379452\n","230: 0.06646762555465102 0.11400626040995121 0.18047388596460223\n","231: 0.021897583501413465 0.0993418712168932 0.12123945471830666\n","232: 0.01726109767332673 0.10508587211370468 0.12234696978703141\n","233: 0.011488052085042 0.10102289542555809 0.11251094751060009\n","234: 0.02537520555779338 0.10163270682096481 0.1270079123787582\n","235: 0.03767310455441475 0.10631698369979858 0.14399008825421333\n","236: 0.022347749210894108 0.11647439934313297 0.13882214855402708\n","237: 0.024144110502675176 0.10136999189853668 0.12551410240121186\n","238: 0.033854588167741895 0.10306954011321068 0.13692412828095257\n","239: 0.08072672225534916 0.1093951128423214 0.19012183509767056\n","240: 0.013441517949104309 0.10167721100151539 0.1151187289506197\n","241: 0.03527387697249651 0.10206713527441025 0.13734101224690676\n","242: 0.04758994560688734 0.10231700353324413 0.14990694914013147\n","243: 0.0415194786619395 0.10107463411986828 0.14259411278180778\n","244: 0.016803731210529804 0.09801242779940367 0.11481615900993347\n","245: 0.027321827597916126 0.09884322248399258 0.1261650500819087\n","246: 0.04092268133535981 0.11081772111356258 0.1517404024489224\n","247: 0.034418840892612934 0.11065631732344627 0.1450751582160592\n","248: 0.014145903289318085 0.10775879770517349 0.12190470099449158\n","249: 0.024555001175031066 0.10372662357985973 0.1282816247548908\n","250: 0.022976063890382648 0.0968974120914936 0.11987347598187625\n","251: 0.03811908373609185 0.11367925256490707 0.15179833630099893\n","252: 0.017849255120381713 0.12058715894818306 0.13843641406856477\n","253: 0.06255301181226969 0.10376211069524288 0.16631512250751257\n","254: 0.030068607069551945 0.10919754579663277 0.1392661528661847\n","255: 0.026088024489581585 0.1136536281555891 0.1397416526451707\n","256: 0.014507229905575514 0.10965633951127529 0.1241635694168508\n","257: 0.048248219303786755 0.11394789908081293 0.16219611838459969\n","258: 0.03757121437229216 0.09671664610505104 0.1342878604773432\n","259: 0.04198634787462652 0.11718674562871456 0.15917309350334108\n","260: 0.042653665877878666 0.11728265043348074 0.1599363163113594\n","261: 0.035198905505239964 0.119026904925704 0.15422581043094397\n","262: 0.03518357127904892 0.10301196947693825 0.13819554075598717\n","263: 0.032897827215492725 0.10671533644199371 0.13961316365748644\n","264: 0.0744832893833518 0.10787169449031353 0.18235498387366533\n","265: 0.06613825960084796 0.10305278562009335 0.1691910452209413\n","266: 0.03600447345525026 0.09706445410847664 0.1330689275637269\n","267: 0.01389729930087924 0.10170079953968525 0.11559809884056449\n","268: 0.0546238919487223 0.10502312891185284 0.15964702086057514\n","269: 0.04572467738762498 0.11137846112251282 0.1571031385101378\n","270: 0.028398657916113734 0.09625714272260666 0.1246558006387204\n","271: 0.029591920785605907 0.1040931660681963 0.1336850868538022\n","272: 0.02056159172207117 0.10073721222579479 0.12129880394786596\n","273: 0.01871515577659011 0.09899279847741127 0.11770795425400138\n","274: 0.018931732047349215 0.10716474428772926 0.12609647633507848\n","275: 0.032584937987849116 0.09600722324103117 0.1285921612288803\n","276: 0.06314228195697069 0.11711807735264301 0.1802603593096137\n","277: 0.08000175468623638 0.09919944033026695 0.17920119501650333\n","278: 0.014618128770962358 0.1046232059597969 0.11924133473075926\n","279: 0.029985918663442135 0.10381269082427025 0.13379860948771238\n","280: 0.07395826047286391 0.09630061779171228 0.1702588782645762\n","281: 0.03078486886806786 0.11425031907856464 0.1450351879466325\n","282: 0.01845076074823737 0.1020528394728899 0.12050360022112727\n","283: 0.03193795052357018 0.09747229889035225 0.12941024941392243\n","284: 0.05807344987988472 0.11270907521247864 0.17078252509236336\n","285: 0.016895713284611702 0.09601862914860249 0.11291434243321419\n","286: 0.023460404947400093 0.11178149469196796 0.13524189963936806\n","287: 0.018299255520105362 0.09858974814414978 0.11688900366425514\n","288: 0.07010624185204506 0.10885672830045223 0.1789629701524973\n","289: 0.03343689674511552 0.09733010455965996 0.13076700130477548\n","290: 0.024079078808426857 0.09823739714920521 0.12231647595763206\n","291: 0.011592044960707426 0.11590776406228542 0.12749980902299285\n","292: 0.014991916483268142 0.10125000402331352 0.11624192050658166\n","293: 0.013189584482461214 0.0973843839019537 0.11057396838441491\n","294: 0.035175792407244444 0.10524662584066391 0.14042241824790835\n","295: 0.03860639710910618 0.09593717195093632 0.1345435690600425\n","296: 0.02629476715810597 0.11118873581290245 0.13748350297100842\n","297: 0.04578382754698396 0.10764820128679276 0.1534320288337767\n","298: 0.04067639447748661 0.10771413333714008 0.1483905278146267\n","299: 0.03938949014991522 0.09794314578175545 0.13733263593167067\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625148829741,"user_tz":-60,"elapsed":13,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625148829741,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"8fb6d96c-e850-476f-cad9-0e543f071598"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":10,"outputs":[{"output_type":"stream","text":["0.9649122807017544\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625148829741,"user_tz":-60,"elapsed":7,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"5bc35798-32f0-4d97-d9a7-8c78ab465aeb"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9912023460410557\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625148829742,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":11,"outputs":[]}]}
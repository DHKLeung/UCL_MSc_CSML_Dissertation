{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN sc aug WPL0.25.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625148642196,"user_tz":-60,"elapsed":17557,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"1d101527-90ed-42ce-fec4-b3af30a2a61d"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5"},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W"},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625148643256,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"b48f08cf-84c0-4d67-b5d1-af7da467a40c"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","perturb_loss_weight = 0.25\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f5e38b0fa90>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule"},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625148649654,"user_tz":-60,"elapsed":6102,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"641bfa65-534d-4ef5-9c17-ad8e8da47829"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"fkMC5_yoZnWd"},"source":["def mixup_breast_sc(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_uc_list = list()\n","    labels_uc_list = list()\n","    unique_classes = torch.unique(labels)\n","    for uc in unique_classes:\n","        mask_uc = (labels == uc).flatten()  # flatten to avoid the labels are in column vector\n","        inputs_uc = inputs[mask_uc]\n","        labels_uc = labels[mask_uc]\n","        batch_size_uc = labels_uc.size(0)\n","        idx = torch.randperm(batch_size_uc).to('cuda')\n","        mixup_inputs_uc = lmbda * inputs_uc + (1 - lmbda) * inputs_uc[idx]\n","        mixup_inputs_uc_list.append(mixup_inputs_uc)\n","        labels_uc_list.append(labels_uc)\n","    mixup_inputs_sc = torch.vstack(mixup_inputs_uc_list)\n","    mixup_labels_sc = torch.cat(labels_uc_list, dim=0)  # use cat not hstack to avoid labels are in column vector\n","    return mixup_inputs_sc, mixup_labels_sc, lmbda"],"id":"fkMC5_yoZnWd","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625148652526,"user_tz":-60,"elapsed":2873,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"11541997-c374-4d3b-abe2-85c6c68d5e91"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup perturbation with same class #\n","        mixup_inputs_sc, mixup_labels_sc, lmbda = mixup_breast_sc(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_sc))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_sc = augment_outputs[original_num:]\n","        mixup_loss_sc = criterion(mixup_outputs_sc, mixup_labels_sc)  # no need to mix the loss up since it is now mixup with same class, just use ordinary cross entropy\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_sc + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_sc.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_sc.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.14479660987854 2.1461178064346313 4.290914416313171\n","1: 2.107052743434906 2.11075222492218 4.217804968357086\n","2: 2.0414501428604126 2.0454984307289124 4.086948573589325\n","3: 1.951378345489502 1.9616674780845642 3.913045823574066\n","4: 1.8565687537193298 1.8696897625923157 3.7262585163116455\n","5: 1.7609657645225525 1.77393239736557 3.5348981618881226\n","6: 1.648213505744934 1.6688289642333984 3.3170424699783325\n","7: 1.5137364268302917 1.5511459708213806 3.0648823976516724\n","8: 1.4122351706027985 1.434130072593689 2.8463652431964874\n","9: 1.2755157947540283 1.3113662600517273 2.5868820548057556\n","10: 1.1135289371013641 1.1864245533943176 2.2999534904956818\n","11: 0.9997867941856384 1.0702883005142212 2.0700750946998596\n","12: 0.8879449367523193 0.9501141905784607 1.83805912733078\n","13: 0.7834292203187943 0.8525446951389313 1.6359739154577255\n","14: 0.6938850730657578 0.7576487809419632 1.451533854007721\n","15: 0.5875856280326843 0.6845381259918213 1.2721237540245056\n","16: 0.5261412858963013 0.627591609954834 1.1537328958511353\n","17: 0.4584575295448303 0.561437338590622 1.0198948681354523\n","18: 0.4503393769264221 0.5057532489299774 0.9560926258563995\n","19: 0.41408005356788635 0.4671960175037384 0.8812760710716248\n","20: 0.3404768034815788 0.44899600744247437 0.7894728109240532\n","21: 0.34615664929151535 0.40528854727745056 0.7514451965689659\n","22: 0.2033257558941841 0.3820227012038231 0.5853484570980072\n","23: 0.24209126830101013 0.36826401948928833 0.6103552877902985\n","24: 0.21518152952194214 0.3367224559187889 0.551903985440731\n","25: 0.2314830757677555 0.346223920583725 0.5777069963514805\n","26: 0.25048626214265823 0.3274795562028885 0.5779658183455467\n","27: 0.18370290845632553 0.30475910007953644 0.48846200853586197\n","28: 0.2186986245214939 0.282688707113266 0.5013873316347599\n","29: 0.18627839162945747 0.29136446863412857 0.47764286026358604\n","30: 0.13263117522001266 0.27855636924505234 0.411187544465065\n","31: 0.1707625687122345 0.25804540514945984 0.42880797386169434\n","32: 0.09803877025842667 0.24858340993523598 0.34662218019366264\n","33: 0.1739487424492836 0.2432200163602829 0.4171687588095665\n","34: 0.1488896030932665 0.2469598948955536 0.3958494979888201\n","35: 0.09839785099029541 0.22973324730992317 0.3281310983002186\n","36: 0.16250505298376083 0.242350772023201 0.4048558250069618\n","37: 0.1555497646331787 0.2266654521226883 0.382215216755867\n","38: 0.12148852087557316 0.22212106734514236 0.3436095882207155\n","39: 0.1088084988296032 0.20787619799375534 0.31668469682335854\n","40: 0.11834786459803581 0.21588066220283508 0.3342285268008709\n","41: 0.10365200601518154 0.20987632125616074 0.3135283272713423\n","42: 0.0828164853155613 0.20357323437929153 0.28638971969485283\n","43: 0.09192439913749695 0.20507585257291794 0.2970002517104149\n","44: 0.0941135510802269 0.20618962869048119 0.3003031797707081\n","45: 0.1376800686120987 0.19018938764929771 0.3278694562613964\n","46: 0.10666101146489382 0.20001813769340515 0.30667914915829897\n","47: 0.073540098965168 0.1806640289723873 0.2542041279375553\n","48: 0.11909926868975163 0.19226646423339844 0.31136573292315006\n","49: 0.07692713849246502 0.1809183768928051 0.2578455153852701\n","50: 0.09705671668052673 0.18915992230176926 0.286216638982296\n","51: 0.07820050790905952 0.17181072756648064 0.25001123547554016\n","52: 0.10572741366922855 0.17717308178544044 0.282900495454669\n","53: 0.10224826820194721 0.17206111922860146 0.27430938743054867\n","54: 0.0683628860861063 0.16707156226038933 0.23543444834649563\n","55: 0.0855092965066433 0.16353771463036537 0.24904701113700867\n","56: 0.08921186067163944 0.16538121178746223 0.2545930724591017\n","57: 0.08208907954394817 0.156577929854393 0.23866700939834118\n","58: 0.08350288681685925 0.1800798960030079 0.26358278281986713\n","59: 0.09435742348432541 0.16665634512901306 0.26101376861333847\n","60: 0.04895712248980999 0.1578192301094532 0.2067763525992632\n","61: 0.053725543431937695 0.16250737011432648 0.21623291354626417\n","62: 0.060154437087476254 0.14931794442236423 0.2094723815098405\n","63: 0.08696445263922215 0.15936196967959404 0.24632642231881618\n","64: 0.07377249002456665 0.16380097344517708 0.23757346346974373\n","65: 0.07414681743830442 0.1550198383629322 0.22916665580123663\n","66: 0.07151880860328674 0.15168848261237144 0.2232072912156582\n","67: 0.0837351344525814 0.14835547655820847 0.23209061101078987\n","68: 0.07629648596048355 0.1481599286198616 0.22445641458034515\n","69: 0.05135361943393946 0.1508900336921215 0.20224365312606096\n","70: 0.05594716593623161 0.14370516315102577 0.19965232908725739\n","71: 0.04180290084332228 0.1496829353272915 0.19148583617061377\n","72: 0.05707015283405781 0.15378864854574203 0.21085880137979984\n","73: 0.05073302332311869 0.15707948803901672 0.2078125113621354\n","74: 0.04924314934760332 0.1537107490003109 0.20295389834791422\n","75: 0.05998754035681486 0.14029081165790558 0.20027835201472044\n","76: 0.05766873899847269 0.14100599102675915 0.19867473002523184\n","77: 0.0760277034714818 0.1529463417828083 0.2289740452542901\n","78: 0.0867178849875927 0.13837353512644768 0.22509142011404037\n","79: 0.07123449631035328 0.14709962159395218 0.21833411790430546\n","80: 0.08879536669701338 0.1316747758537531 0.22047014255076647\n","81: 0.029675304889678955 0.14153514429926872 0.17121044918894768\n","82: 0.05551525391638279 0.14374488219618797 0.19926013611257076\n","83: 0.03234567027539015 0.1263013333082199 0.15864700358361006\n","84: 0.07815340626984835 0.12968068942427635 0.2078340956941247\n","85: 0.06279800273478031 0.12750187143683434 0.19029987417161465\n","86: 0.03654412552714348 0.12978846114128828 0.16633258666843176\n","87: 0.07556851021945477 0.13224514573812485 0.2078136559575796\n","88: 0.04621800780296326 0.1490229070186615 0.19524091482162476\n","89: 0.05435732286423445 0.1485459916293621 0.20290331449359655\n","90: 0.05182349681854248 0.12919696047902107 0.18102045729756355\n","91: 0.03326718369498849 0.13124076277017593 0.16450794646516442\n","92: 0.05698821693658829 0.11940626800060272 0.176394484937191\n","93: 0.05980871059000492 0.13015225902199745 0.18996096961200237\n","94: 0.08456109324470162 0.14277790486812592 0.22733899811282754\n","95: 0.07548006996512413 0.12747174873948097 0.2029518187046051\n","96: 0.059659052174538374 0.13098994456231594 0.19064899673685431\n","97: 0.02696042787283659 0.13725505024194717 0.16421547811478376\n","98: 0.033712287433445454 0.12012070044875145 0.1538329878821969\n","99: 0.05792704503983259 0.12202131003141403 0.17994835507124662\n","100: 0.03543353220447898 0.1319404225796461 0.1673739547841251\n","101: 0.027366635855287313 0.12773291021585464 0.15509954607114196\n","102: 0.017053385265171528 0.1255524791777134 0.14260586444288492\n","103: 0.02256954088807106 0.11770186387002468 0.14027140475809574\n","104: 0.07728139963001013 0.12926622107625008 0.2065476207062602\n","105: 0.03975878935307264 0.11555858515202999 0.15531737450510263\n","106: 0.046832185704261065 0.11521563865244389 0.16204782435670495\n","107: 0.021908952854573727 0.13518920168280602 0.15709815453737974\n","108: 0.03937143366783857 0.11681038327515125 0.15618181694298983\n","109: 0.030477925203740597 0.12565108947455883 0.15612901467829943\n","110: 0.06502228230237961 0.11991756781935692 0.18493985012173653\n","111: 0.04487927444279194 0.10853652283549309 0.15341579727828503\n","112: 0.04072017828002572 0.11003739759325981 0.15075757587328553\n","113: 0.04011212382465601 0.11207472532987595 0.15218684915453196\n","114: 0.07945926487445831 0.11576609499752522 0.19522535987198353\n","115: 0.08105421997606754 0.1240249965339899 0.20507921651005745\n","116: 0.02321851160377264 0.10612554475665092 0.12934405636042356\n","117: 0.06812197435647249 0.11357089877128601 0.1816928731277585\n","118: 0.016693556448444724 0.10803941823542118 0.1247329746838659\n","119: 0.09682552004233003 0.12892480939626694 0.22575032943859696\n","120: 0.056447346694767475 0.11421998217701912 0.1706673288717866\n","121: 0.04523368179798126 0.10860143601894379 0.15383511781692505\n","122: 0.06097923777997494 0.10862521454691887 0.1696044523268938\n","123: 0.028399227652698755 0.10923182405531406 0.13763105170801282\n","124: 0.01840172102674842 0.11948220990598202 0.13788393093273044\n","125: 0.061365794856101274 0.11325700487941504 0.1746227997355163\n","126: 0.06185902841389179 0.10579218715429306 0.16765121556818485\n","127: 0.06039873883128166 0.10545367561280727 0.16585241444408894\n","128: 0.03636736725457013 0.10239281505346298 0.1387601823080331\n","129: 0.05853924015536904 0.10398777760565281 0.16252701776102185\n","130: 0.048432206735014915 0.1018748339265585 0.1503070406615734\n","131: 0.04108467698097229 0.10878190957009792 0.1498665865510702\n","132: 0.066425452940166 0.1001546960324049 0.1665801489725709\n","133: 0.029149889945983887 0.09844736196100712 0.127597251906991\n","134: 0.01473642559722066 0.09757794439792633 0.11231436999514699\n","135: 0.028693945379927754 0.09518211614340544 0.12387606152333319\n","136: 0.03221716359257698 0.09805772081017494 0.13027488440275192\n","137: 0.053913545329123735 0.0995456613600254 0.15345920668914914\n","138: 0.02433762839064002 0.10651279147714376 0.13085041986778378\n","139: 0.06478420738130808 0.11501259170472622 0.1797967990860343\n","140: 0.02011000318452716 0.09630420245230198 0.11641420563682914\n","141: 0.0181038579903543 0.0958323385566473 0.1139361965470016\n","142: 0.03451008349657059 0.09944242052733898 0.13395250402390957\n","143: 0.022348969476297498 0.09700628370046616 0.11935525317676365\n","144: 0.022084435680881143 0.1123853474855423 0.13446978316642344\n","145: 0.03355365991592407 0.09642946161329746 0.12998312152922153\n","146: 0.036482919938862324 0.10065812803804874 0.13714104797691107\n","147: 0.032302997540682554 0.10475079342722893 0.13705379096791148\n","148: 0.0285442266613245 0.09485934115946293 0.12340356782078743\n","149: 0.02302328171208501 0.09999880939722061 0.12302209110930562\n","150: 0.0718245143070817 0.09429298155009747 0.16611749585717916\n","151: 0.04792507225647569 0.10330264177173376 0.15122771402820945\n","152: 0.032198112457990646 0.0916262585669756 0.12382437102496624\n","153: 0.05661075375974178 0.10438661649823189 0.16099737025797367\n","154: 0.023144172504544258 0.10011226823553443 0.12325644074007869\n","155: 0.03463313449174166 0.0940508209168911 0.12868395540863276\n","156: 0.04192004119977355 0.1063186302781105 0.14823867147788405\n","157: 0.030566406436264515 0.09685866720974445 0.12742507364600897\n","158: 0.08661695173941553 0.11135828960686922 0.19797524134628475\n","159: 0.014679784188047051 0.1010617446154356 0.11574152880348265\n","160: 0.03014548495411873 0.09643544442951679 0.12658092938363552\n","161: 0.05225584236904979 0.09736079443246126 0.14961663680151105\n","162: 0.03729039477184415 0.09338639490306377 0.13067678967490792\n","163: 0.0629851024132222 0.1138237714767456 0.1768088738899678\n","164: 0.024617287330329418 0.09334790892899036 0.11796519625931978\n","165: 0.020817956887185574 0.09351667761802673 0.11433463450521231\n","166: 0.02165450481697917 0.09486245922744274 0.11651696404442191\n","167: 0.054001799784600735 0.10847511328756809 0.16247691307216883\n","168: 0.014187245164066553 0.0907555390149355 0.10494278417900205\n","169: 0.023798720445483923 0.09646408818662167 0.12026280863210559\n","170: 0.03993532829917967 0.08933380711823702 0.1292691354174167\n","171: 0.033181996550410986 0.09015127923339605 0.12333327578380704\n","172: 0.05830980185419321 0.11396262235939503 0.17227242421358824\n","173: 0.06294648442417383 0.09287812933325768 0.1558246137574315\n","174: 0.0471617104485631 0.09921257197856903 0.14637428242713213\n","175: 0.043206572998315096 0.1056299451738596 0.1488365181721747\n","176: 0.015317837009206414 0.09162449277937412 0.10694232978858054\n","177: 0.024963464587926865 0.10107606276869774 0.1260395273566246\n","178: 0.018967800308018923 0.09230112470686436 0.11126892501488328\n","179: 0.0348843801766634 0.09568781033158302 0.13057219050824642\n","180: 0.027350751217454672 0.10221247561275959 0.12956322683021426\n","181: 0.024908453226089478 0.10184961557388306 0.12675806879997253\n","182: 0.03837102372199297 0.10382571630179882 0.1421967400237918\n","183: 0.028373383451253176 0.09795777313411236 0.12633115658536553\n","184: 0.04455043841153383 0.09214907884597778 0.13669951725751162\n","185: 0.026716007851064205 0.10865334793925285 0.13536935579031706\n","186: 0.03270853590220213 0.09091303497552872 0.12362157087773085\n","187: 0.03060000401455909 0.08886950556188822 0.11946950957644731\n","188: 0.03917191829532385 0.08826214913278818 0.12743406742811203\n","189: 0.08234907779842615 0.1021419819444418 0.18449105974286795\n","190: 0.02309646923094988 0.08869736548513174 0.11179383471608162\n","191: 0.02938078623265028 0.09593237191438675 0.12531315814703703\n","192: 0.021786809898912907 0.09424631297588348 0.11603312287479639\n","193: 0.03355174092575908 0.0925673320889473 0.12611907301470637\n","194: 0.024499224964529276 0.08993513602763414 0.11443436099216342\n","195: 0.041503461077809334 0.09589529596269131 0.13739875704050064\n","196: 0.055403415113687515 0.1071779653429985 0.16258138045668602\n","197: 0.027611584402620792 0.09820176288485527 0.12581334728747606\n","198: 0.025131866801530123 0.09944381471723318 0.1245756815187633\n","199: 0.060177534352988005 0.10020091570913792 0.16037845006212592\n","200: 0.05641489662230015 0.1053738221526146 0.16178871877491474\n","201: 0.033314564265310764 0.09952593594789505 0.13284050021320581\n","202: 0.013997258385643363 0.10973920673131943 0.12373646511696279\n","203: 0.02490812074393034 0.08868436049669981 0.11359248124063015\n","204: 0.0228908050339669 0.09210275299847126 0.11499355803243816\n","205: 0.016216760501265526 0.08879162464290857 0.1050083851441741\n","206: 0.030507846269756556 0.09980747289955616 0.13031531916931272\n","207: 0.03234832640737295 0.0923125296831131 0.12466085609048605\n","208: 0.032964701764285564 0.09844296053051949 0.13140766229480505\n","209: 0.03187423897907138 0.08895378187298775 0.12082802085205913\n","210: 0.042851679027080536 0.0935825016349554 0.13643418066203594\n","211: 0.018679595086723566 0.0886177122592926 0.10729730734601617\n","212: 0.022473490796983242 0.09015517681837082 0.11262866761535406\n","213: 0.035960376262664795 0.10250071808695793 0.13846109434962273\n","214: 0.052658271975815296 0.10997052490711212 0.16262879688292742\n","215: 0.04930634773336351 0.10669048689305782 0.15599683462642133\n","216: 0.014271523803472519 0.09624931402504444 0.11052083782851696\n","217: 0.012632763013243675 0.1007538978010416 0.11338666081428528\n","218: 0.024880656972527504 0.09236691519618034 0.11724757216870785\n","219: 0.023429958149790764 0.09093322604894638 0.11436318419873714\n","220: 0.01835726946592331 0.0944422073662281 0.11279947683215141\n","221: 0.044228360056877136 0.09241608530282974 0.13664444535970688\n","222: 0.013914277777075768 0.0975915901362896 0.11150586791336536\n","223: 0.021821365226060152 0.08725642785429955 0.1090777930803597\n","224: 0.03823670279234648 0.08901522494852543 0.1272519277408719\n","225: 0.054007040336728096 0.09609870798885822 0.15010574832558632\n","226: 0.029573504347354174 0.09182089567184448 0.12139440001919866\n","227: 0.025094777578487992 0.1117254737764597 0.1368202513549477\n","228: 0.05614004610106349 0.1090357257053256 0.1651757718063891\n","229: 0.028596247546374798 0.09297245927155018 0.12156870681792498\n","230: 0.03267389815300703 0.08841956779360771 0.12109346594661474\n","231: 0.022550147492438555 0.09734565392136574 0.11989580141380429\n","232: 0.019980260636657476 0.10281329602003098 0.12279355665668845\n","233: 0.01814087573438883 0.09294822439551353 0.11108910012990236\n","234: 0.030399251263588667 0.08991188369691372 0.12031113496050239\n","235: 0.03579746722243726 0.09327091090381145 0.12906837812624872\n","236: 0.03060146886855364 0.09711394645273685 0.1277154153212905\n","237: 0.03459251346066594 0.0935498122125864 0.12814232567325234\n","238: 0.013883128063753247 0.10363053902983665 0.1175136670935899\n","239: 0.05345108546316624 0.08888391777873039 0.14233500324189663\n","240: 0.021029307506978512 0.0872568991035223 0.10828620661050081\n","241: 0.04743796866387129 0.09144415892660618 0.13888212759047747\n","242: 0.03873915318399668 0.09814237058162689 0.13688152376562357\n","243: 0.014402836794033647 0.10836450289934874 0.12276733969338238\n","244: 0.024506870191544294 0.09239176101982594 0.11689863121137023\n","245: 0.034271215088665485 0.0913669653236866 0.12563818041235209\n","246: 0.020416106563061476 0.09402498882263899 0.11444109538570046\n","247: 0.03343891678377986 0.09359515644609928 0.12703407322987914\n","248: 0.014337795553728938 0.08669604454189539 0.10103384009562433\n","249: 0.01158648356795311 0.09606622159481049 0.1076527051627636\n","250: 0.025683421175926924 0.08719340618699789 0.11287682736292481\n","251: 0.025071291252970695 0.08931100554764271 0.1143822968006134\n","252: 0.038451789412647486 0.10231109522283077 0.14076288463547826\n","253: 0.06272426899522543 0.10149216279387474 0.16421643178910017\n","254: 0.04655427625402808 0.0923595279455185 0.13891380419954658\n","255: 0.016403389861807227 0.0892257783561945 0.10562916821800172\n","256: 0.050923445262014866 0.09850103594362736 0.14942448120564222\n","257: 0.04382753907702863 0.10282509308308363 0.14665263216011226\n","258: 0.03681838698685169 0.08739406708627939 0.12421245407313108\n","259: 0.05287869763560593 0.08894844725728035 0.14182714489288628\n","260: 0.04239130159839988 0.08987183682620525 0.13226313842460513\n","261: 0.026705342577770352 0.08981091156601906 0.11651625414378941\n","262: 0.017892405856400728 0.0862494083121419 0.10414181416854262\n","263: 0.02407173067331314 0.09062136709690094 0.11469309777021408\n","264: 0.05197062995284796 0.09836885705590248 0.15033948700875044\n","265: 0.024234408978372812 0.08549045538529754 0.10972486436367035\n","266: 0.027748622931540012 0.09494737535715103 0.12269599828869104\n","267: 0.02290431526489556 0.10464990697801113 0.1275542222429067\n","268: 0.04983564466238022 0.0884163100272417 0.13825195468962193\n","269: 0.027186948573216796 0.09284920990467072 0.12003615847788751\n","270: 0.012860454502515495 0.08487635990604758 0.09773681440856308\n","271: 0.03059596475213766 0.08830617740750313 0.11890214215964079\n","272: 0.012649879790842533 0.08991729095578194 0.10256717074662447\n","273: 0.028427843004465103 0.09121761471033096 0.11964545771479607\n","274: 0.03405764698982239 0.0981062799692154 0.13216392695903778\n","275: 0.0512069771066308 0.11269148625433445 0.16389846336096525\n","276: 0.05540594644844532 0.10534513369202614 0.16075108014047146\n","277: 0.07841958105564117 0.0940336138010025 0.17245319485664368\n","278: 0.018079186556860805 0.09131843410432339 0.10939762066118419\n","279: 0.019337631296366453 0.09793227165937424 0.11726990295574069\n","280: 0.06802093796432018 0.0968393012881279 0.16486023925244808\n","281: 0.018299132818356156 0.08997025713324547 0.10826938995160162\n","282: 0.026620740303769708 0.08968042768537998 0.11630116798914969\n","283: 0.01818594103679061 0.10118933767080307 0.11937527870759368\n","284: 0.06264724861830473 0.0977853499352932 0.16043259855359793\n","285: 0.019791933242231607 0.087533307261765 0.10732524050399661\n","286: 0.027237628819420934 0.09358295984566212 0.12082058866508305\n","287: 0.019329732283949852 0.10835913568735123 0.12768886797130108\n","288: 0.05247841961681843 0.10338754765689373 0.15586596727371216\n","289: 0.012260096380487084 0.0938326008617878 0.10609269724227488\n","290: 0.015898854937404394 0.09386470541357994 0.10976356035098433\n","291: 0.02880363306030631 0.09143795445561409 0.1202415875159204\n","292: 0.04804155929014087 0.09095162712037563 0.1389931864105165\n","293: 0.013825605157762766 0.09201212227344513 0.1058377274312079\n","294: 0.03340532258152962 0.08841374516487122 0.12181906774640083\n","295: 0.05039049871265888 0.08768186345696449 0.13807236216962337\n","296: 0.011906456900760531 0.08985399827361107 0.1017604551743716\n","297: 0.030243269400671124 0.08953660354018211 0.11977987294085324\n","298: 0.030868201050907373 0.0965944305062294 0.12746263155713677\n","299: 0.03385212388820946 0.08954501152038574 0.1233971354085952\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details"},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625148652818,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"c4ab026b-98d4-4685-a503-e44bb0192870"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":null,"outputs":[{"output_type":"stream","text":["0.9824561403508771\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625148652819,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"33cea274-02fe-4f4a-ae37-4bb4099928d2"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":null,"outputs":[{"output_type":"stream","text":["0.9882697947214076\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy"},"source":[""],"id":"preceding-galaxy","execution_count":null,"outputs":[]}]}
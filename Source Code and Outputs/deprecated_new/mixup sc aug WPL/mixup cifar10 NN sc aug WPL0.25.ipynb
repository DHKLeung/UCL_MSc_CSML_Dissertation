{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup cifar10 NN sc aug WPL0.25.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_b2Nvqg3Oifz"},"source":["# Import Libraries"],"id":"_b2Nvqg3Oifz"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"romantic-purchase","executionInfo":{"status":"ok","timestamp":1625153033699,"user_tz":-60,"elapsed":711,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"1e3d7f02-530d-4313-a09d-c6095ff1a961"},"source":["import torch\n","from torchvision import transforms, datasets\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"romantic-purchase","execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qiPvZSTKO0Pk"},"source":["# Import outside code"],"id":"qiPvZSTKO0Pk"},{"cell_type":"code","metadata":{"id":"kw0NscKvO2bI","executionInfo":{"status":"ok","timestamp":1625153034223,"user_tz":-60,"elapsed":527,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"kw0NscKvO2bI","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMYYosNOO2V3","executionInfo":{"status":"ok","timestamp":1625153034506,"user_tz":-60,"elapsed":284,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"sMYYosNOO2V3","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pg95DAmvO8un"},"source":["# Configuration"],"id":"pg95DAmvO8un"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silver-clear","executionInfo":{"status":"ok","timestamp":1625153034506,"user_tz":-60,"elapsed":2,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"18dc3987-242d-448f-ed4c-f090f15aa7a7"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),  # can omit\n","    transforms.RandomHorizontalFlip(),  # can omit\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","batch_size = 128\n","step_size = 0.1\n","random_seed = 0\n","epochs = 200\n","L2_decay = 1e-4\n","alpha = 1.\n","perturb_loss_weight = 0.25\n","\n","torch.manual_seed(random_seed)"],"id":"silver-clear","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f5ec2486bb0>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"uDMInb7zPT3v"},"source":["# Data"],"id":"uDMInb7zPT3v"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"relative-mobility","executionInfo":{"status":"ok","timestamp":1625153037028,"user_tz":-60,"elapsed":2523,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"a0181462-112c-4d66-c46c-145292a605ed"},"source":["\"\"\"\n","Data\n","\"\"\"\n","train_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=True, download=True, transform=transform_train)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=False, download=True, transform=transform_test)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"relative-mobility","execution_count":5,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0-R2uhkAPjkv"},"source":["# Models, Loss, Optimiser"],"id":"0-R2uhkAPjkv"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"starting-chancellor","executionInfo":{"status":"ok","timestamp":1625153039667,"user_tz":-60,"elapsed":2641,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"8c6253a1-e6aa-4605-96e8-db74b9bfcb47"},"source":["model = ResNet18()\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"starting-chancellor","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"1qFqkwNmQgeO"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"1qFqkwNmQgeO"},{"cell_type":"code","metadata":{"id":"5O_m4HZanmam","executionInfo":{"status":"ok","timestamp":1625153039667,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_cifar10_sc(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_uc_list = list()\n","    labels_uc_list = list()\n","    unique_classes = torch.unique(labels)\n","    for uc in unique_classes:\n","        mask_uc = (labels == uc).flatten()  # flatten to avoid the labels are in column vector\n","        inputs_uc = inputs[mask_uc]\n","        labels_uc = labels[mask_uc]\n","        batch_size_uc = labels_uc.size(0)\n","        idx = torch.randperm(batch_size_uc).to('cuda')\n","        mixup_inputs_uc = lmbda * inputs_uc + (1 - lmbda) * inputs_uc[idx]\n","        mixup_inputs_uc_list.append(mixup_inputs_uc)\n","        labels_uc_list.append(labels_uc)\n","    mixup_inputs_sc = torch.vstack(mixup_inputs_uc_list)\n","    mixup_labels_sc = torch.cat(labels_uc_list, dim=0)  # use cat not hstack to avoid labels are in column vector\n","    return mixup_inputs_sc, mixup_labels_sc, lmbda"],"id":"5O_m4HZanmam","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TlGDo8vIQoZk"},"source":["# Training"],"id":"TlGDo8vIQoZk"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"written-bookmark","executionInfo":{"status":"ok","timestamp":1625163028327,"user_tz":-60,"elapsed":9988662,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"f7f20618-3885-4b9c-ea8e-da5d7811f8dc"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","\n","        # Mixup perturbation with same class #\n","        mixup_inputs_sc, mixup_labels_sc, lmbda = mixup_cifar10_sc(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_sc))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_sc = augment_outputs[original_num:]\n","        mixup_loss_sc = criterion(mixup_outputs_sc, mixup_labels_sc)  # no need to mix the loss up since it is now mixup with same class, just use ordinary cross entropy\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_sc + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_sc.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_sc.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"written-bookmark","execution_count":8,"outputs":[{"output_type":"stream","text":["0: 583.9043872952461 605.5224543809891 1189.4268416762352\n","1: 379.8167961239815 393.8569923043251 773.6737884283066\n","2: 292.4608817398548 298.246133685112 590.7070154249668\n","3: 243.7745668888092 245.5752509534359 489.3498178422451\n","4: 210.03060239553452 207.07419514656067 417.1047975420952\n","5: 190.54255264997482 181.94472429156303 372.48727694153786\n","6: 173.1189778894186 163.02057924866676 336.13955713808537\n","7: 161.21847061812878 147.55229234695435 308.7707629650831\n","8: 150.07248905301094 134.61395333707333 284.68644239008427\n","9: 139.0847808420658 123.33929350972176 262.42407435178757\n","10: 133.39371313154697 113.32687539607286 246.72058852761984\n","11: 127.74066832661629 106.492585465312 234.2332537919283\n","12: 120.44066938757896 97.84306801110506 218.28373739868402\n","13: 114.44376012682915 91.77848480641842 206.22224493324757\n","14: 109.77207233011723 87.44857415556908 197.2206464856863\n","15: 105.36001851409674 80.09693545848131 185.45695397257805\n","16: 102.97243992984295 77.18182865530252 180.15426858514547\n","17: 99.41308341175318 75.42054470255971 174.8336281143129\n","18: 96.98528914898634 70.85227045416832 167.83755960315466\n","19: 94.16318498551846 68.55045409500599 162.71363908052444\n","20: 93.3267085403204 64.3812908232212 157.7079993635416\n","21: 93.18787835538387 63.21155137568712 156.399429731071\n","22: 88.47652285173535 58.236693773418665 146.71321662515402\n","23: 88.1529258415103 56.89470386505127 145.04762970656157\n","24: 85.4469894990325 56.70023612305522 142.14722562208772\n","25: 86.11005821824074 55.66496492177248 141.77502314001322\n","26: 81.7242129072547 51.95026965998113 133.67448256723583\n","27: 80.62190277501941 52.692226618528366 133.31412939354777\n","28: 80.9435169659555 50.81463452428579 131.7581514902413\n","29: 77.52507617510855 48.519222812727094 126.04429898783565\n","30: 80.26349837612361 48.76694942545146 129.03044780157506\n","31: 78.10754270479083 47.70479458197951 125.81233728677034\n","32: 77.0378671810031 45.737919226288795 122.77578640729189\n","33: 77.63058416545391 44.659146048128605 122.28973021358252\n","34: 74.77691094949841 42.420392479747534 117.19730342924595\n","35: 73.84905371069908 42.94538996182382 116.7944436725229\n","36: 71.96444557607174 42.72500720061362 114.68945277668536\n","37: 69.80234409868717 42.789872497320175 112.59221659600735\n","38: 73.74199391901493 41.45326578244567 115.1952597014606\n","39: 72.48461396619678 38.71150217950344 111.19611614570022\n","40: 68.9774385150522 38.205403693020344 107.18284220807254\n","41: 69.10453376919031 38.612941555678844 107.71747532486916\n","42: 68.52577986940742 39.3126926086843 107.83847247809172\n","43: 68.28260957822204 36.97041724063456 105.2530268188566\n","44: 71.15058978274465 39.15976994857192 110.31035973131657\n","45: 66.94679766893387 37.42172020301223 104.3685178719461\n","46: 69.11053732037544 38.124366296455264 107.2349036168307\n","47: 70.7834535650909 39.20532430149615 109.98877786658704\n","48: 67.3160599116236 36.84932953864336 104.16538945026696\n","49: 68.45366415381432 35.74874932318926 104.20241347700357\n","50: 64.41947215795517 33.830987989902496 98.25046014785767\n","51: 66.73825790733099 35.674898222088814 102.4131561294198\n","52: 67.36536077782512 36.58628823235631 103.95164901018143\n","53: 62.16372438147664 33.04570220876485 95.20942659024149\n","54: 66.86873178184032 35.561247915029526 102.42997969686985\n","55: 64.97221985086799 32.01913674175739 96.99135659262538\n","56: 65.16722714714706 33.4242504555732 98.59147760272026\n","57: 65.00523054227233 33.281227607280016 98.28645814955235\n","58: 65.10033422894776 33.31253228522837 98.41286651417613\n","59: 65.67002092860639 33.99914629757404 99.66916722618043\n","60: 62.92777724005282 32.47321678139269 95.40099402144551\n","61: 64.60596081800759 32.87182869017124 97.47778950817883\n","62: 64.31677871011198 32.967189732939005 97.28396844305098\n","63: 61.73185611702502 31.715588599443436 93.44744471646845\n","64: 63.89083384163678 33.47649473790079 97.36732857953757\n","65: 63.33182413876057 32.11212167143822 95.44394581019878\n","66: 61.455514036118984 31.43579457513988 92.89130861125886\n","67: 63.15630039945245 31.531816671602428 94.68811707105488\n","68: 64.34604818373919 31.88312837202102 96.2291765557602\n","69: 61.95863876119256 30.19529722351581 92.15393598470837\n","70: 63.59478251822293 31.81113857589662 95.40592109411955\n","71: 64.88498390838504 33.6840081140399 98.56899202242494\n","72: 59.67268883064389 29.961110210046172 89.63379904069006\n","73: 63.97176984138787 32.842177933547646 96.81394777493551\n","74: 59.84444775059819 30.074459227733314 89.9189069783315\n","75: 62.272643776610494 30.015394885092974 92.28803866170347\n","76: 63.05747283156961 30.21942417230457 93.27689700387418\n","77: 61.351513139903545 30.273427934385836 91.62494107428938\n","78: 59.370366379618645 28.350896450690925 87.72126283030957\n","79: 61.27331786416471 30.041837884113193 91.3151557482779\n","80: 62.92917013913393 33.34188977256417 96.2710599116981\n","81: 60.08249293267727 29.867229252122343 89.94972218479961\n","82: 61.84447227232158 31.30025010649115 93.14472237881273\n","83: 61.93741003610194 29.5271033346653 91.46451337076724\n","84: 62.270093251019716 31.227504733018577 93.4975979840383\n","85: 59.422069476917386 29.447937378659844 88.87000685557723\n","86: 60.574573788791895 30.755894038826227 91.33046782761812\n","87: 57.66554203350097 28.45634050387889 86.12188253737986\n","88: 59.340209102258086 28.251180574297905 87.59138967655599\n","89: 58.39675439149141 28.13809892348945 86.53485331498086\n","90: 57.47321696486324 28.672340494580567 86.14555745944381\n","91: 57.21364890038967 28.15663549490273 85.3702843952924\n","92: 61.025682497769594 28.862154659815133 89.88783715758473\n","93: 60.874915862455964 30.10564185399562 90.98055771645159\n","94: 58.30104641802609 28.51147244311869 86.81251886114478\n","95: 61.831081656739116 30.14887316059321 91.97995481733233\n","96: 59.70208046771586 28.686391449533403 88.38847191724926\n","97: 62.23203638754785 28.38565685879439 90.61769324634224\n","98: 60.56400281749666 30.96243280917406 91.52643562667072\n","99: 59.08997468277812 30.03137655928731 89.12135124206543\n","100: 37.24667814746499 11.728633095975965 48.975311243440956\n","101: 26.907254111021757 6.137051498168148 33.044305609189905\n","102: 25.83121271408163 4.549094611895271 30.3803073259769\n","103: 24.122303247684613 3.647018963121809 27.769322210806422\n","104: 23.486578338895924 3.1121321653481573 26.59871050424408\n","105: 22.177563533419743 2.6574868807801977 24.83505041419994\n","106: 20.187527193978895 2.3640597211779095 22.551586915156804\n","107: 20.508188734180294 2.170497708109906 22.6786864422902\n","108: 19.4333861664054 1.78214455183479 21.21553071824019\n","109: 20.02294807432918 1.6652956319157965 21.688243706244975\n","110: 19.854098982876167 1.5102966590202413 21.364395641896408\n","111: 17.564595385512803 1.4872408087539952 19.051836194266798\n","112: 17.75326719126315 1.4015694599947892 19.154836651257938\n","113: 20.250333577685524 1.30291212569864 21.553245703384164\n","114: 18.510634106758516 1.1649896768067265 19.675623783565243\n","115: 19.325223265448585 1.3221507593843853 20.64737402483297\n","116: 18.361998305423185 1.0864807975594886 19.448479102982674\n","117: 17.01332633016864 1.0930789690319216 18.10640529920056\n","118: 16.75357516965596 0.9576012086472474 17.711176378303207\n","119: 16.347694314375985 1.048218279669527 17.395912594045512\n","120: 15.892559973930474 0.9560139251116198 16.848573899042094\n","121: 16.048246179736452 0.8825014674366685 16.93074764717312\n","122: 16.117976719891885 0.7837729890525225 16.901749708944408\n","123: 15.350939280411694 0.7545246418449096 16.105463922256604\n","124: 14.466285101982066 0.7701119475095766 15.236397049491643\n","125: 16.721975921536796 0.7423866667450056 17.4643625882818\n","126: 15.524140663532307 0.7149841046702932 16.2391247682026\n","127: 14.762622970243683 0.705894056292891 15.468517026536574\n","128: 17.198436583334114 0.7435573216716875 17.941993905005802\n","129: 14.965295677626273 0.6965856444221572 15.66188132204843\n","130: 14.554292819200782 0.6731573232682422 15.227450142469024\n","131: 16.844370648585027 0.6953104522835929 17.53968110086862\n","132: 14.669378924038028 0.6552609932477935 15.324639917285822\n","133: 12.792851028672885 0.5619753522187239 13.354826380891609\n","134: 14.653580425714608 0.5547413178355782 15.208321743550187\n","135: 14.981752523803152 0.5597849656187464 15.541537489421898\n","136: 14.397114944586065 0.589920640544733 14.987035585130798\n","137: 14.450399095570901 0.554037825888372 15.004436921459273\n","138: 14.026694772590417 0.5945919723963016 14.621286744986719\n","139: 12.739089300317573 0.5401514267869061 13.27924072710448\n","140: 13.058451503457036 0.6024003174388781 13.660851820895914\n","141: 14.239564936753595 0.5645396277977852 14.80410456455138\n","142: 14.013206052855821 0.5014376901453943 14.514643743001216\n","143: 13.754231332131894 0.5101001084149175 14.264331440546812\n","144: 14.659488308680011 0.5252619666134706 15.184750275293482\n","145: 12.672392806329299 0.5186357147613307 13.19102852109063\n","146: 12.714149561172235 0.4840357123175636 13.198185273489798\n","147: 13.596547186258249 0.5047966036072467 14.101343789865496\n","148: 11.566960756550543 0.4774383622716414 12.044399118822184\n","149: 13.164899348965264 0.49778905184939504 13.662688400814659\n","150: 13.522470551077276 0.4752926017245045 13.997763152801781\n","151: 14.009553986630635 0.44769780014758 14.457251786778215\n","152: 13.917058204184286 0.5170948331506224 14.434153037334909\n","153: 14.282575031596934 0.48937419922003755 14.771949230816972\n","154: 14.253328829450766 0.4529293210725882 14.706258150523354\n","155: 14.054399690357968 0.46422130280552665 14.518620993163495\n","156: 13.386636903102044 0.4495012886764016 13.836138191778446\n","157: 13.199955658521503 0.5055194114829646 13.705475070004468\n","158: 12.805142826837255 0.42656892864761176 13.231711755484866\n","159: 13.782338161981897 0.3847830506856553 14.167121212667553\n","160: 13.076668347406667 0.4026389878927148 13.479307335299382\n","161: 11.809453211651999 0.41038080078578787 12.219834012437786\n","162: 13.90840771223884 0.377966820349684 14.286374532588525\n","163: 12.070422714823508 0.42492717603454366 12.495349890858051\n","164: 12.043323949736077 0.41269391037349124 12.456017860109569\n","165: 12.266481077269418 0.39285626296623377 12.659337340235652\n","166: 11.959022969269427 0.38252616726094857 12.341549136530375\n","167: 12.295298940007342 0.4387929297372466 12.734091869744589\n","168: 11.94239216738788 0.4037081202477566 12.346100287635636\n","169: 12.342407420510426 0.3892081848171074 12.731615605327534\n","170: 10.866446988322423 0.42287299577583326 11.289319984098256\n","171: 12.684256507403916 0.34734065970405936 13.031597167107975\n","172: 12.471034440386575 0.4275587680094759 12.89859320839605\n","173: 13.096364407043438 0.37698654824635014 13.473350955289789\n","174: 12.39878184182453 0.3396635196331772 12.738445361457707\n","175: 12.008826782228425 0.32446691501536407 12.33329369724379\n","176: 12.140150660387008 0.35250125019956613 12.492651910586574\n","177: 12.314193287005764 0.4208444601390511 12.735037747144816\n","178: 12.013623770981212 0.3475514742458472 12.36117524522706\n","179: 10.835614720286685 0.3599187837826321 11.195533504069317\n","180: 10.557465860067168 0.3489852047423483 10.906451064809517\n","181: 11.251807082706364 0.33884896845484036 11.590656051161204\n","182: 13.3120663924783 0.3428173714855802 13.65488376396388\n","183: 10.93264264530444 0.3286741009906109 11.261316746295051\n","184: 12.30015299867955 0.3134357226699649 12.613588721349515\n","185: 10.859901717019966 0.3272469896255643 11.18714870664553\n","186: 11.678758079797262 0.3296611897421826 12.008419269539445\n","187: 12.907571736112004 0.3703632830911374 13.277935019203142\n","188: 12.454727572461707 0.3785474924625305 12.833275064924237\n","189: 11.879443427722435 0.29533186465414474 12.17477529237658\n","190: 10.87129586932133 0.3061295914594666 11.177425460780796\n","191: 10.918419889683719 0.30231027710760827 11.220730166791327\n","192: 11.631404107189155 0.31150156974763377 11.942905676936789\n","193: 12.666434443497565 0.3496465873395209 13.016081030837086\n","194: 11.089869016810553 0.3313083185821597 11.421177335392713\n","195: 11.375071409973316 0.3291460613763775 11.704217471349693\n","196: 10.593801336624892 0.3506759736337699 10.944477310258662\n","197: 10.386217800994928 0.3098288952714938 10.696046696266421\n","198: 10.884439194465813 0.2956371521067922 11.180076346572605\n","199: 11.251330424056505 0.370332246660837 11.621662670717342\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"doR5kS81SW5F"},"source":["# Save model"],"id":"doR5kS81SW5F"},{"cell_type":"code","metadata":{"id":"frozen-damage","executionInfo":{"status":"ok","timestamp":1625163028329,"user_tz":-60,"elapsed":11,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_cifar10')\n","# model = models.__dict__['ResNet18']()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_cifar10'))"],"id":"frozen-damage","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLlLnmpRSZAl"},"source":["# Test on Test Data"],"id":"BLlLnmpRSZAl"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aboriginal-lafayette","executionInfo":{"status":"ok","timestamp":1625163031679,"user_tz":-60,"elapsed":3357,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"2d25b66d-ba15-4750-96b1-7e0b22572ba9"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"aboriginal-lafayette","execution_count":10,"outputs":[{"output_type":"stream","text":["0.9511\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xHj0XxoMScbg"},"source":["# Test on Train Data"],"id":"xHj0XxoMScbg"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"therapeutic-orlando","executionInfo":{"status":"ok","timestamp":1625163052944,"user_tz":-60,"elapsed":21267,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"2f808998-83f1-47c0-847a-b8058f7573e4"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"therapeutic-orlando","execution_count":11,"outputs":[{"output_type":"stream","text":["0.99998\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XCvQUWwMSetI","executionInfo":{"status":"ok","timestamp":1625163052945,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"XCvQUWwMSetI","execution_count":11,"outputs":[]}]}
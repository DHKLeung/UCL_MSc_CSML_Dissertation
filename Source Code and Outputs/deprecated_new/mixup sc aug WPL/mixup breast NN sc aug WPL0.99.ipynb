{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN sc aug WPL0.99.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625148858973,"user_tz":-60,"elapsed":18961,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"7d9b4682-f6a9-4e6a-8f8a-92f129a9308b"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625148859846,"user_tz":-60,"elapsed":876,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625148860141,"user_tz":-60,"elapsed":297,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625148860141,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"309fea63-f2c4-43bb-9b6c-c64b69f6eb70"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","perturb_loss_weight = 0.99\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f9eccaa6a50>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625148860348,"user_tz":-60,"elapsed":209,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625148866801,"user_tz":-60,"elapsed":6454,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"2c59681c-51bc-4793-f06f-ed4386809955"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"fkMC5_yoZnWd","executionInfo":{"status":"ok","timestamp":1625148866801,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_sc(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_uc_list = list()\n","    labels_uc_list = list()\n","    unique_classes = torch.unique(labels)\n","    for uc in unique_classes:\n","        mask_uc = (labels == uc).flatten()  # flatten to avoid the labels are in column vector\n","        inputs_uc = inputs[mask_uc]\n","        labels_uc = labels[mask_uc]\n","        batch_size_uc = labels_uc.size(0)\n","        idx = torch.randperm(batch_size_uc).to('cuda')\n","        mixup_inputs_uc = lmbda * inputs_uc + (1 - lmbda) * inputs_uc[idx]\n","        mixup_inputs_uc_list.append(mixup_inputs_uc)\n","        labels_uc_list.append(labels_uc)\n","    mixup_inputs_sc = torch.vstack(mixup_inputs_uc_list)\n","    mixup_labels_sc = torch.cat(labels_uc_list, dim=0)  # use cat not hstack to avoid labels are in column vector\n","    return mixup_inputs_sc, mixup_labels_sc, lmbda"],"id":"fkMC5_yoZnWd","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625148869935,"user_tz":-60,"elapsed":3141,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"ef2729ad-08fd-4a7f-c1f6-51cc28ce0d74"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup perturbation with same class #\n","        mixup_inputs_sc, mixup_labels_sc, lmbda = mixup_breast_sc(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_sc))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_sc = augment_outputs[original_num:]\n","        mixup_loss_sc = criterion(mixup_outputs_sc, mixup_labels_sc)  # no need to mix the loss up since it is now mixup with same class, just use ordinary cross entropy\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_sc + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_sc.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_sc.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":8,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.1379395723342896 2.139131486415863 4.277071058750153\n","1: 2.0958878993988037 2.09984689950943 4.195734798908234\n","2: 2.0365174412727356 2.0405598282814026 4.077077269554138\n","3: 1.9459598660469055 1.9588127136230469 3.9047725796699524\n","4: 1.856951355934143 1.8732237815856934 3.7301751375198364\n","5: 1.7558882236480713 1.772137463092804 3.5280256867408752\n","6: 1.6451290845870972 1.670138657093048 3.3152677416801453\n","7: 1.5164687931537628 1.5602818131446838 3.0767506062984467\n","8: 1.4227156341075897 1.442846417427063 2.8655620515346527\n","9: 1.2947109043598175 1.3321790397167206 2.626889944076538\n","10: 1.116700679063797 1.2067475616931915 2.3234482407569885\n","11: 1.0017044842243195 1.0800190269947052 2.0817235112190247\n","12: 0.9012015461921692 0.979604184627533 1.8808057308197021\n","13: 0.7789978384971619 0.8658889830112457 1.6448868215084076\n","14: 0.6926134675741196 0.7722586393356323 1.464872106909752\n","15: 0.581896185874939 0.6981811821460724 1.2800773680210114\n","16: 0.4927331656217575 0.6450517773628235 1.137784942984581\n","17: 0.4804154634475708 0.5874390751123428 1.0678545385599136\n","18: 0.47616858780384064 0.5402033030986786 1.0163718909025192\n","19: 0.4403064697980881 0.5272491425275803 0.9675556123256683\n","20: 0.3636222407221794 0.49945372343063354 0.863075964152813\n","21: 0.41088129580020905 0.47199420630931854 0.8828755021095276\n","22: 0.22260069102048874 0.4532063454389572 0.675807036459446\n","23: 0.29013095051050186 0.43150073289871216 0.721631683409214\n","24: 0.25643574818968773 0.3925788551568985 0.6490146033465862\n","25: 0.1783641204237938 0.38668717443943024 0.565051294863224\n","26: 0.2799186110496521 0.38552622497081757 0.6654448360204697\n","27: 0.1819915510714054 0.3643292635679245 0.5463208146393299\n","28: 0.2834898307919502 0.37843140959739685 0.6619212403893471\n","29: 0.2510671988129616 0.37534190714359283 0.6264091059565544\n","30: 0.19783997908234596 0.3672202229499817 0.5650602020323277\n","31: 0.22725260630249977 0.3686158359050751 0.5958684422075748\n","32: 0.12389897741377354 0.33294055610895157 0.4568395335227251\n","33: 0.24576251953840256 0.3365003690123558 0.5822628885507584\n","34: 0.16049247793853283 0.32507771998643875 0.4855701979249716\n","35: 0.14144077897071838 0.3092886731028557 0.45072945207357407\n","36: 0.24525602534413338 0.3340044170618057 0.5792604424059391\n","37: 0.23621520027518272 0.3131749629974365 0.5493901632726192\n","38: 0.10784183442592621 0.28806013241410255 0.39590196684002876\n","39: 0.12031104043126106 0.30685294046998024 0.4271639809012413\n","40: 0.1678028553724289 0.2978895604610443 0.4656924158334732\n","41: 0.14380473271012306 0.3030516132712364 0.4468563459813595\n","42: 0.13218049705028534 0.2839348688721657 0.416115365922451\n","43: 0.15186031721532345 0.27576886862516403 0.4276291858404875\n","44: 0.10124620608985424 0.2764089107513428 0.377655116841197\n","45: 0.17308411374688148 0.27134979888796806 0.44443391263484955\n","46: 0.12147760763764381 0.2895957715809345 0.41107337921857834\n","47: 0.09882196225225925 0.27065766230225563 0.3694796245545149\n","48: 0.16975748538970947 0.2879812642931938 0.4577387496829033\n","49: 0.16283772140741348 0.2665965296328068 0.42943425104022026\n","50: 0.11712236888706684 0.26090021058917046 0.3780225794762373\n","51: 0.13695410639047623 0.2611008323729038 0.39805493876338005\n","52: 0.1434295680373907 0.27646689862012863 0.41989646665751934\n","53: 0.15998990461230278 0.26946941018104553 0.4294593147933483\n","54: 0.12152892351150513 0.25629278272390366 0.3778217062354088\n","55: 0.1742195151746273 0.2653689906001091 0.4395885057747364\n","56: 0.1843979600816965 0.27861300855875015 0.46301096864044666\n","57: 0.13475307822227478 0.24913248047232628 0.38388555869460106\n","58: 0.09392303694039583 0.2695796862244606 0.36350272316485643\n","59: 0.1351455021649599 0.26294858008623123 0.39809408225119114\n","60: 0.1396186649799347 0.2846967428922653 0.4243154078722\n","61: 0.0831978777423501 0.24754513427615166 0.33074301201850176\n","62: 0.07471831515431404 0.25789403915405273 0.3326123543083668\n","63: 0.17774656042456627 0.2508687376976013 0.4286152981221676\n","64: 0.14328566938638687 0.24628181010484695 0.3895674794912338\n","65: 0.15218694508075714 0.27696534991264343 0.4291522949934006\n","66: 0.13508997671306133 0.25546182692050934 0.39055180363357067\n","67: 0.10832819994539022 0.23877602443099022 0.34710422437638044\n","68: 0.125674229580909 0.22812817618250847 0.3538024057634175\n","69: 0.063809665851295 0.23084326833486557 0.29465293418616056\n","70: 0.07468769699335098 0.23645661771297455 0.31114431470632553\n","71: 0.0847582221031189 0.24308841675519943 0.32784663885831833\n","72: 0.13809771090745926 0.23492717370390892 0.3730248846113682\n","73: 0.07262514345347881 0.24572616815567017 0.318351311609149\n","74: 0.07584221102297306 0.25290264561772346 0.3287448566406965\n","75: 0.09065316244959831 0.2596125565469265 0.3502657189965248\n","76: 0.10062424466013908 0.22684063017368317 0.32746487483382225\n","77: 0.06994679756462574 0.22532661259174347 0.2952734101563692\n","78: 0.14920343458652496 0.23313265666365623 0.3823360912501812\n","79: 0.09040084574371576 0.23017872869968414 0.3205795744433999\n","80: 0.1478721983730793 0.24583978950977325 0.39371198788285255\n","81: 0.05582105368375778 0.24125701934099197 0.29707807302474976\n","82: 0.09474712703377008 0.24571425467729568 0.34046138171106577\n","83: 0.08017418906092644 0.22825107350945473 0.30842526257038116\n","84: 0.11218835972249508 0.2367553636431694 0.3489437233656645\n","85: 0.12654328625649214 0.2223366517573595 0.34887993801385164\n","86: 0.08645705133676529 0.2275327704846859 0.3139898218214512\n","87: 0.13565977849066257 0.23931963741779327 0.37497941590845585\n","88: 0.1074129631742835 0.23834645748138428 0.3457594206556678\n","89: 0.10191471548750997 0.21853509172797203 0.320449807215482\n","90: 0.056737031787633896 0.26475984044373035 0.32149687223136425\n","91: 0.061966960318386555 0.2296048179268837 0.29157177824527025\n","92: 0.09661232866346836 0.2524755522608757 0.34908788092434406\n","93: 0.11464029364287853 0.24895954877138138 0.3635998424142599\n","94: 0.10820511542260647 0.24536247551441193 0.3535675909370184\n","95: 0.1121448464691639 0.23197201639413834 0.34411686286330223\n","96: 0.13804198056459427 0.22453827410936356 0.3625802546739578\n","97: 0.059063101187348366 0.2533242627978325 0.31238736398518085\n","98: 0.08825706550851464 0.2390281856060028 0.32728525111451745\n","99: 0.09647641330957413 0.2387820929288864 0.33525850623846054\n","100: 0.07069115806370974 0.23535338044166565 0.3060445385053754\n","101: 0.05681528802961111 0.2345966398715973 0.2914119279012084\n","102: 0.05747793335467577 0.2405333649367094 0.2980112982913852\n","103: 0.06457158643752337 0.2232292704284191 0.2878008568659425\n","104: 0.11084526870399714 0.2159555498510599 0.32680081855505705\n","105: 0.04081213381141424 0.2267851121723652 0.26759724598377943\n","106: 0.04546353220939636 0.24225062131881714 0.2877141535282135\n","107: 0.08405855670571327 0.2330000214278698 0.31705857813358307\n","108: 0.1105309883132577 0.21429205499589443 0.3248230433091521\n","109: 0.07851000037044287 0.2169424407184124 0.29545244108885527\n","110: 0.09328865632414818 0.2394452914595604 0.3327339477837086\n","111: 0.10987718123942614 0.23261190205812454 0.3424890832975507\n","112: 0.0767906978726387 0.2246626727283001 0.3014533706009388\n","113: 0.11945435777306557 0.22629812359809875 0.3457524813711643\n","114: 0.1399482972919941 0.22653550282120705 0.36648380011320114\n","115: 0.07076805457472801 0.22292404621839523 0.29369210079312325\n","116: 0.038192689418792725 0.2265542559325695 0.26474694535136223\n","117: 0.1392485098913312 0.22656413167715073 0.3658126415684819\n","118: 0.044775962829589844 0.23685141652822495 0.2816273793578148\n","119: 0.11245391517877579 0.22674499824643135 0.33919891342520714\n","120: 0.16461600735783577 0.22214383631944656 0.38675984367728233\n","121: 0.12121670972555876 0.22314953804016113 0.3443662477657199\n","122: 0.12066877447068691 0.2340179793536663 0.3546867538243532\n","123: 0.0851256512105465 0.22103725001215935 0.30616290122270584\n","124: 0.11699917539954185 0.24579618498682976 0.3627953603863716\n","125: 0.16654788004234433 0.2231607586145401 0.38970863865688443\n","126: 0.10674379952251911 0.2216637097299099 0.328407509252429\n","127: 0.1001630611717701 0.23718351125717163 0.3373465724289417\n","128: 0.0690908171236515 0.21533966436982155 0.28443048149347305\n","129: 0.09077231120318174 0.22263642959296703 0.3134087407961488\n","130: 0.09847756288945675 0.22783058881759644 0.3263081517070532\n","131: 0.10209651477634907 0.23471598327159882 0.3368124980479479\n","132: 0.11623183079063892 0.2286885380744934 0.34492036886513233\n","133: 0.08987390575930476 0.2277185097336769 0.3175924154929817\n","134: 0.06972690764814615 0.2081824243068695 0.27790933195501566\n","135: 0.08163645304739475 0.24661586806178093 0.3282523211091757\n","136: 0.09408672712743282 0.23259034007787704 0.32667706720530987\n","137: 0.1780688613653183 0.25334132462739944 0.43141018599271774\n","138: 0.046916065737605095 0.22922416776418686 0.27614023350179195\n","139: 0.05648818239569664 0.23021984472870827 0.2867080271244049\n","140: 0.051167980302125216 0.22598400712013245 0.27715198742225766\n","141: 0.06767037138342857 0.22154371812939644 0.289214089512825\n","142: 0.03637448092922568 0.21779831126332283 0.2541727921925485\n","143: 0.051728443475440145 0.22518673539161682 0.27691517886705697\n","144: 0.03716555796563625 0.2019059844315052 0.23907154239714146\n","145: 0.10154074244201183 0.24595541507005692 0.34749615751206875\n","146: 0.06584689393639565 0.210943765938282 0.27679065987467766\n","147: 0.09142998233437538 0.22292685508728027 0.31435683742165565\n","148: 0.04941704263910651 0.21453066915273666 0.2639477117918432\n","149: 0.048204039223492146 0.22962770611047745 0.2778317453339696\n","150: 0.12147676618769765 0.2348075546324253 0.35628432082012296\n","151: 0.10880900733172894 0.24656394869089127 0.3553729560226202\n","152: 0.164992761798203 0.2352289818227291 0.4002217436209321\n","153: 0.1467250669375062 0.2263568975031376 0.3730819644406438\n","154: 0.06707901228219271 0.2388930767774582 0.3059720890596509\n","155: 0.137362249661237 0.2329993825405836 0.3703616322018206\n","156: 0.07290926668792963 0.20992668345570564 0.2828359501436353\n","157: 0.09396395832300186 0.21662215515971184 0.3105861134827137\n","158: 0.15688770823180676 0.21489128842949867 0.3717789966613054\n","159: 0.026532090734690428 0.2044937815517187 0.23102587228640914\n","160: 0.0701512023806572 0.22586560994386673 0.2960168123245239\n","161: 0.10050589963793755 0.2299625650048256 0.33046846464276314\n","162: 0.12219196278601885 0.21526404842734337 0.3374560112133622\n","163: 0.09235149063169956 0.22591852396726608 0.31827001459896564\n","164: 0.06972799636423588 0.23482265695929527 0.30455065332353115\n","165: 0.05202797334641218 0.22380787506699562 0.2758358484134078\n","166: 0.033119503408670425 0.21557239443063736 0.24869189783930779\n","167: 0.1192058827728033 0.21948738396167755 0.33869326673448086\n","168: 0.05364138539880514 0.2156648449599743 0.26930623035877943\n","169: 0.044307807460427284 0.21982012689113617 0.26412793435156345\n","170: 0.09327215142548084 0.23276669904589653 0.3260388504713774\n","171: 0.12802964076399803 0.22758666798472404 0.3556163087487221\n","172: 0.15079671517014503 0.2292562536895275 0.38005296885967255\n","173: 0.11048010177910328 0.20978979766368866 0.32026989944279194\n","174: 0.07793676760047674 0.21531678177416325 0.29325354937464\n","175: 0.025656134821474552 0.20152200758457184 0.2271781424060464\n","176: 0.08812483493238688 0.21021247655153275 0.2983373114839196\n","177: 0.08668695017695427 0.235089972615242 0.3217769227921963\n","178: 0.06376340007409453 0.20668582245707512 0.27044922253116965\n","179: 0.04182118643075228 0.2122361660003662 0.2540573524311185\n","180: 0.023299329448491335 0.22518689185380936 0.2484862213023007\n","181: 0.052449687384068966 0.22272012382745743 0.2751698112115264\n","182: 0.09713011421263218 0.21589476987719536 0.31302488408982754\n","183: 0.043316567316651344 0.20684722997248173 0.25016379728913307\n","184: 0.11217255471274257 0.22300289571285248 0.33517545042559505\n","185: 0.09647510340437293 0.2147885337471962 0.31126363715156913\n","186: 0.05111316964030266 0.2267652302980423 0.27787839993834496\n","187: 0.0710040619596839 0.21272072196006775 0.28372478391975164\n","188: 0.02840388659387827 0.2108376882970333 0.23924157489091158\n","189: 0.1522179152816534 0.21468034759163857 0.36689826287329197\n","190: 0.08011933043599129 0.2265569269657135 0.3066762574017048\n","191: 0.09313564747571945 0.21381942927837372 0.30695507675409317\n","192: 0.061078055296093225 0.22776996344327927 0.2888480187393725\n","193: 0.07333741709589958 0.21350647509098053 0.2868438921868801\n","194: 0.10664993990212679 0.24964557960629463 0.3562955195084214\n","195: 0.0940454788506031 0.20254422444850206 0.29658970329910517\n","196: 0.08528044633567333 0.21492642909288406 0.3002068754285574\n","197: 0.1279413141310215 0.2521075401455164 0.3800488542765379\n","198: 0.06463941186666489 0.2151680439710617 0.2798074558377266\n","199: 0.07859542779624462 0.2076947670429945 0.2862901948392391\n","200: 0.08611716702580452 0.21268146485090256 0.2987986318767071\n","201: 0.08600928168743849 0.2247430458664894 0.3107523275539279\n","202: 0.05221043014898896 0.20224119443446398 0.25445162458345294\n","203: 0.1098644807934761 0.23120930790901184 0.34107378870248795\n","204: 0.027620085049420595 0.22532590478658676 0.25294598983600736\n","205: 0.04917712416499853 0.2077356819063425 0.25691280607134104\n","206: 0.1151425358839333 0.23697656951844692 0.35211910540238023\n","207: 0.06616839952766895 0.22189131379127502 0.288059713318944\n","208: 0.09073545597493649 0.22677357494831085 0.31750903092324734\n","209: 0.11513443058356643 0.2141258344054222 0.32926026498898864\n","210: 0.13296858593821526 0.2335541732609272 0.36652275919914246\n","211: 0.03387473523616791 0.23017571680247784 0.26405045203864574\n","212: 0.11520643718540668 0.23076082020998 0.3459672573953867\n","213: 0.060967071913182735 0.22893063724040985 0.2898977091535926\n","214: 0.05536448955535889 0.21400240063667297 0.26936689019203186\n","215: 0.07821307610720396 0.22007286921143532 0.2982859453186393\n","216: 0.05635396530851722 0.22520773857831955 0.28156170388683677\n","217: 0.02841205382719636 0.2339138574898243 0.26232591131702065\n","218: 0.0711432583630085 0.2281215861439705 0.299264844506979\n","219: 0.09084905683994293 0.21381841227412224 0.30466746911406517\n","220: 0.0629368657246232 0.216359693557024 0.2792965592816472\n","221: 0.08078178018331528 0.22480984777212143 0.3055916279554367\n","222: 0.10250371973961592 0.2039670255035162 0.3064707452431321\n","223: 0.029762123711407185 0.24161919578909874 0.2713813195005059\n","224: 0.07189403660595417 0.22178937122225761 0.2936834078282118\n","225: 0.12097799405455589 0.20849163085222244 0.32946962490677834\n","226: 0.05552039248868823 0.2051727995276451 0.26069319201633334\n","227: 0.040945850778371096 0.21969465911388397 0.26064050989225507\n","228: 0.03168798657134175 0.21186934038996696 0.24355732696130872\n","229: 0.09054288640618324 0.22313977032899857 0.3136826567351818\n","230: 0.10053355526179075 0.22315868735313416 0.3236922426149249\n","231: 0.07490495312958956 0.20327718555927277 0.2781821386888623\n","232: 0.05955968424677849 0.24140553548932076 0.30096521973609924\n","233: 0.02566868392750621 0.21593227982521057 0.24160096375271678\n","234: 0.09573607612401247 0.25655287504196167 0.35228895116597414\n","235: 0.08436170499771833 0.21525101363658905 0.2996127186343074\n","236: 0.08112099394202232 0.23307159543037415 0.31419258937239647\n","237: 0.07584328670054674 0.2138608731329441 0.28970415983349085\n","238: 0.02576311444863677 0.2183149755001068 0.24407808994874358\n","239: 0.1829362791031599 0.22675274312496185 0.40968902222812176\n","240: 0.08741609519347548 0.21380751952528954 0.301223614718765\n","241: 0.10973931476473808 0.22648519277572632 0.3362245075404644\n","242: 0.1109783835709095 0.23644390515983105 0.34742228873074055\n","243: 0.07020503375679255 0.20757941342890263 0.27778444718569517\n","244: 0.09649682324379683 0.218971386551857 0.3154682097956538\n","245: 0.075366435572505 0.20702705346047878 0.2823934890329838\n","246: 0.07963727600872517 0.22318636626005173 0.3028236422687769\n","247: 0.0933933537453413 0.22668730467557907 0.32008065842092037\n","248: 0.07855123281478882 0.2349739857017994 0.3135252185165882\n","249: 0.08009791001677513 0.22030501812696457 0.3004029281437397\n","250: 0.07801257539540529 0.22666943818330765 0.30468201357871294\n","251: 0.02176817273721099 0.20520686730742455 0.22697504004463553\n","252: 0.03447570651769638 0.21992750093340874 0.2544032074511051\n","253: 0.17659994959831238 0.22648309916257858 0.40308304876089096\n","254: 0.09960169717669487 0.22611596435308456 0.32571766152977943\n","255: 0.04348990973085165 0.24627898260951042 0.28976889234036207\n","256: 0.04726459621451795 0.20385833457112312 0.2511229307856411\n","257: 0.09697948768734932 0.2357029765844345 0.33268246427178383\n","258: 0.11412161216139793 0.2064388208091259 0.32056043297052383\n","259: 0.061082377564162016 0.21241255104541779 0.2734949286095798\n","260: 0.10934741329401731 0.22095178067684174 0.33029919397085905\n","261: 0.07249687146395445 0.21422233432531357 0.286719205789268\n","262: 0.0644090324640274 0.2115449719130993 0.2759540043771267\n","263: 0.08435824885964394 0.21345281228423119 0.2978110611438751\n","264: 0.13488334976136684 0.21902135014533997 0.3539046999067068\n","265: 0.11701927660033107 0.23499434627592564 0.3520136228762567\n","266: 0.07145937392488122 0.20324398204684258 0.2747033559717238\n","267: 0.047451755963265896 0.22118227556347847 0.26863403152674437\n","268: 0.10813748091459274 0.23034458607435226 0.338482066988945\n","269: 0.09464588481932878 0.23308417201042175 0.32773005682975054\n","270: 0.04372487356886268 0.21509522199630737 0.25882009556517005\n","271: 0.06956164538860321 0.21792759001255035 0.28748923540115356\n","272: 0.055277826730161905 0.2385769784450531 0.293854805175215\n","273: 0.031261144671589136 0.21333854645490646 0.2445996911264956\n","274: 0.08887964906170964 0.21098115667700768 0.2998608057387173\n","275: 0.14556346740573645 0.20894429460167885 0.3545077620074153\n","276: 0.045896345749497414 0.21520661562681198 0.2611029613763094\n","277: 0.20052329543977976 0.22955011762678623 0.430073413066566\n","278: 0.04370543081313372 0.21002406626939774 0.25372949708253145\n","279: 0.07178898062556982 0.21210119128227234 0.28389017190784216\n","280: 0.1013153288513422 0.21278957650065422 0.3141049053519964\n","281: 0.03007717290893197 0.20029983390122652 0.2303770068101585\n","282: 0.08862913027405739 0.26007967069745064 0.348708800971508\n","283: 0.10607813857495785 0.20880985260009766 0.3148879911750555\n","284: 0.1188970748335123 0.2146163173019886 0.3335133921355009\n","285: 0.018116860650479794 0.20253715105354786 0.22065401170402765\n","286: 0.07869930751621723 0.21730338037014008 0.2960026878863573\n","287: 0.05027934443205595 0.22601980715990067 0.2762991515919566\n","288: 0.13252632319927216 0.22189047187566757 0.35441679507493973\n","289: 0.06341782817617059 0.214842539280653 0.2782603674568236\n","290: 0.038674669340252876 0.21107670478522778 0.24975137412548065\n","291: 0.04218729957938194 0.2579636350274086 0.30015093460679054\n","292: 0.06600783206522465 0.22200721874833107 0.2880150508135557\n","293: 0.06877113413065672 0.21304060146212578 0.2818117355927825\n","294: 0.06814511492848396 0.21597542613744736 0.2841205410659313\n","295: 0.07742177532054484 0.19964793045073748 0.2770697057712823\n","296: 0.04266243241727352 0.22651222348213196 0.2691746558994055\n","297: 0.07782710576429963 0.2102360501885414 0.28806315595284104\n","298: 0.07691819965839386 0.2432292103767395 0.32014741003513336\n","299: 0.07228411175310612 0.2251305803656578 0.2974146921187639\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625148869935,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625148869935,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"d8f161b1-afaf-47d3-a5b4-57b820d617a6"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":10,"outputs":[{"output_type":"stream","text":["0.9736842105263158\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625148869936,"user_tz":-60,"elapsed":7,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"5e512381-78f6-404e-95ba-404b800214db"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9765395894428153\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625148869936,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":11,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup cifar10 NN sc aug WPL0.75.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_b2Nvqg3Oifz"},"source":["# Import Libraries"],"id":"_b2Nvqg3Oifz"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"romantic-purchase","executionInfo":{"status":"ok","timestamp":1625164900090,"user_tz":-60,"elapsed":23540,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"b59a91bf-644d-4244-e2dd-7db43a48c717"},"source":["import torch\n","from torchvision import transforms, datasets\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"romantic-purchase","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qiPvZSTKO0Pk"},"source":["# Import outside code"],"id":"qiPvZSTKO0Pk"},{"cell_type":"code","metadata":{"id":"kw0NscKvO2bI","executionInfo":{"status":"ok","timestamp":1625164900925,"user_tz":-60,"elapsed":838,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"kw0NscKvO2bI","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMYYosNOO2V3","executionInfo":{"status":"ok","timestamp":1625164901212,"user_tz":-60,"elapsed":289,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"sMYYosNOO2V3","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pg95DAmvO8un"},"source":["# Configuration"],"id":"pg95DAmvO8un"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silver-clear","executionInfo":{"status":"ok","timestamp":1625164901212,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"7fa559ab-04e3-4b45-e050-aea580dc1eba"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),  # can omit\n","    transforms.RandomHorizontalFlip(),  # can omit\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(\n","        (0.4914, 0.4822, 0.4465),\n","        (0.2023, 0.1994, 0.2010)\n","    )\n","])\n","\n","batch_size = 128\n","step_size = 0.1\n","random_seed = 0\n","epochs = 200\n","L2_decay = 1e-4\n","alpha = 1.\n","perturb_loss_weight = 0.75\n","\n","torch.manual_seed(random_seed)"],"id":"silver-clear","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f7afa040ad0>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"uDMInb7zPT3v"},"source":["# Data"],"id":"uDMInb7zPT3v"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"relative-mobility","executionInfo":{"status":"ok","timestamp":1625164910756,"user_tz":-60,"elapsed":9546,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"381618a2-6e9a-4127-e290-d259d752471e"},"source":["\"\"\"\n","Data\n","\"\"\"\n","train_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=True, download=True, transform=transform_train)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_set = datasets.CIFAR10(root='/content/gdrive/My Drive/colab', train=False, download=True, transform=transform_test)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"relative-mobility","execution_count":5,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0-R2uhkAPjkv"},"source":["# Models, Loss, Optimiser"],"id":"0-R2uhkAPjkv"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"starting-chancellor","executionInfo":{"status":"ok","timestamp":1625164917132,"user_tz":-60,"elapsed":6378,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"67f93dc7-6854-482f-bb37-6f9bcda617b5"},"source":["model = ResNet18()\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"starting-chancellor","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): PreActBlock(\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","    )\n","    (1): PreActBlock(\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"1qFqkwNmQgeO"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"1qFqkwNmQgeO"},{"cell_type":"code","metadata":{"id":"5O_m4HZanmam","executionInfo":{"status":"ok","timestamp":1625164917132,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_cifar10_sc(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_uc_list = list()\n","    labels_uc_list = list()\n","    unique_classes = torch.unique(labels)\n","    for uc in unique_classes:\n","        mask_uc = (labels == uc).flatten()  # flatten to avoid the labels are in column vector\n","        inputs_uc = inputs[mask_uc]\n","        labels_uc = labels[mask_uc]\n","        batch_size_uc = labels_uc.size(0)\n","        idx = torch.randperm(batch_size_uc).to('cuda')\n","        mixup_inputs_uc = lmbda * inputs_uc + (1 - lmbda) * inputs_uc[idx]\n","        mixup_inputs_uc_list.append(mixup_inputs_uc)\n","        labels_uc_list.append(labels_uc)\n","    mixup_inputs_sc = torch.vstack(mixup_inputs_uc_list)\n","    mixup_labels_sc = torch.cat(labels_uc_list, dim=0)  # use cat not hstack to avoid labels are in column vector\n","    return mixup_inputs_sc, mixup_labels_sc, lmbda"],"id":"5O_m4HZanmam","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TlGDo8vIQoZk"},"source":["# Training"],"id":"TlGDo8vIQoZk"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"written-bookmark","executionInfo":{"status":"ok","timestamp":1625174875183,"user_tz":-60,"elapsed":9958053,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"6ef67aa8-2cbe-45fa-ab5b-1f71ad135548"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","\n","        # Mixup perturbation with same class #\n","        mixup_inputs_sc, mixup_labels_sc, lmbda = mixup_cifar10_sc(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_sc))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_sc = augment_outputs[original_num:]\n","        mixup_loss_sc = criterion(mixup_outputs_sc, mixup_labels_sc)  # no need to mix the loss up since it is now mixup with same class, just use ordinary cross entropy\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_sc + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_sc.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_sc.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"written-bookmark","execution_count":8,"outputs":[{"output_type":"stream","text":["0: 623.6630322933197 668.4005990028381 1292.0636312961578\n","1: 426.747980594635 468.0593323111534 894.8073129057884\n","2: 319.8399268388748 353.1589204072952 672.99884724617\n","3: 260.31746622920036 284.61310774087906 544.9305739700794\n","4: 223.74190163612366 242.728215187788 466.47011682391167\n","5: 199.85804760456085 214.0883413553238 413.94638895988464\n","6: 180.84003892540932 190.5444297194481 371.3844686448574\n","7: 166.16505432128906 174.71052667498589 340.87558099627495\n","8: 158.00823080539703 162.70057459175587 320.7088053971529\n","9: 149.3555693179369 151.0703727453947 300.4259420633316\n","10: 140.0794882774353 140.3891450613737 280.468633338809\n","11: 134.51814678311348 133.08037757873535 267.59852436184883\n","12: 126.75648185610771 124.5484517365694 251.30493359267712\n","13: 121.21184796094894 118.60228701680899 239.81413497775793\n","14: 117.29571998119354 113.88020472228527 231.1759247034788\n","15: 111.78323827683926 106.90085916221142 218.68409743905067\n","16: 108.26120417565107 103.33673898130655 211.59794315695763\n","17: 105.6621320694685 99.03660675138235 204.69873882085085\n","18: 103.2947254627943 95.9887450709939 199.2834705337882\n","19: 99.8605118766427 91.82582574337721 191.6863376200199\n","20: 94.45281591266394 85.74178868532181 180.19460459798574\n","21: 94.0918657630682 84.98374047875404 179.07560624182224\n","22: 91.94264221191406 82.30289002507925 174.2455322369933\n","23: 91.524669110775 80.37021807953715 171.89488719031215\n","24: 88.74125220254064 78.35527386888862 167.09652607142925\n","25: 88.27278879284859 77.73100323230028 166.00379202514887\n","26: 84.48041830956936 73.99748875945807 158.47790706902742\n","27: 84.10028010606766 72.87248050048947 156.97276060655713\n","28: 83.43494729697704 70.91162993013859 154.34657722711563\n","29: 80.06381069868803 68.7120193503797 148.77583004906774\n","30: 82.10079942643642 68.90284292399883 151.00364235043526\n","31: 78.49631659686565 65.13056571409106 143.62688231095672\n","32: 76.35466796159744 64.05102197825909 140.40568993985653\n","33: 77.33482516184449 63.68760519847274 141.02243036031723\n","34: 76.73023359104991 62.44866193830967 139.17889552935958\n","35: 74.02960956841707 60.66614044457674 134.6957500129938\n","36: 74.35414705425501 61.027080591768026 135.38122764602304\n","37: 72.34066079929471 59.73032693192363 132.07098773121834\n","38: 75.10223312303424 59.81598914414644 134.91822226718068\n","39: 75.18814908340573 60.745273342356086 135.93342242576182\n","40: 68.6842635422945 55.12341758981347 123.80768113210797\n","41: 69.72587370872498 56.44916233792901 126.17503604665399\n","42: 67.4693660326302 53.398960307240486 120.86832633987069\n","43: 67.98584708198905 53.89175347238779 121.87760055437684\n","44: 68.76860962808132 54.11092831939459 122.87953794747591\n","45: 68.49525255709887 53.17069619894028 121.66594875603914\n","46: 66.53474958240986 52.3704539835453 118.90520356595516\n","47: 66.18569902330637 51.706244215369225 117.8919432386756\n","48: 68.08908501267433 53.67192357033491 121.76100858300924\n","49: 65.6406722664833 49.79985183477402 115.44052410125732\n","50: 63.247577987611294 49.056172002106905 112.3037499897182\n","51: 63.778403067961335 48.52446889132261 112.30287195928395\n","52: 65.56720310822129 50.88867089897394 116.45587400719523\n","53: 64.15322919562459 49.57324459776282 113.72647379338741\n","54: 64.06093654409051 48.809178449213505 112.87011499330401\n","55: 63.13551400974393 48.24730919115245 111.38282320089638\n","56: 65.08206143975258 48.661811804398894 113.74387324415147\n","57: 64.4893430173397 48.94101168215275 113.43035469949245\n","58: 62.23504156246781 47.4186055958271 109.65364715829492\n","59: 64.65752110257745 48.25226526707411 112.90978636965156\n","60: 61.65471217408776 46.463450552895665 108.11816272698343\n","61: 61.138138838112354 46.243505131453276 107.38164396956563\n","62: 60.88562613353133 44.71933417394757 105.6049603074789\n","63: 60.32135862484574 45.42597532644868 105.74733395129442\n","64: 59.364862425252795 44.56287760660052 103.92774003185332\n","65: 60.66640902124345 46.25788414105773 106.92429316230118\n","66: 58.96648474410176 45.069460678845644 104.0359454229474\n","67: 61.60671811923385 45.85331275500357 107.46003087423742\n","68: 60.66777242347598 44.1190924346447 104.78686485812068\n","69: 59.22531930357218 43.72894475236535 102.95426405593753\n","70: 59.74378798343241 45.04614894464612 104.78993692807853\n","71: 58.285764411091805 43.08125362172723 101.36701803281903\n","72: 60.5209472887218 44.780720392242074 105.30166768096387\n","73: 58.415370747447014 42.94069163314998 101.356062380597\n","74: 57.229978285729885 41.167166367173195 98.39714465290308\n","75: 58.40576596185565 43.45191680267453 101.85768276453018\n","76: 59.64313961006701 42.13481559418142 101.77795520424843\n","77: 57.74461717531085 43.369899281300604 101.11451645661145\n","78: 56.731698490679264 41.63378382474184 98.3654823154211\n","79: 58.44123359769583 42.05433839932084 100.49557199701667\n","80: 56.17518223077059 41.82102927565575 97.99621150642633\n","81: 56.11438864096999 40.73478220961988 96.84917085058987\n","82: 56.34171473234892 42.09997670352459 98.44169143587351\n","83: 57.44863508082926 41.12123786844313 98.5698729492724\n","84: 56.78046950697899 41.47087261080742 98.25134211778641\n","85: 55.58529035001993 40.44985709898174 96.03514744900167\n","86: 56.55060204863548 41.924645237624645 98.47524728626013\n","87: 53.38117569684982 38.545694440603256 91.92687013745308\n","88: 56.32057247310877 40.50380530208349 96.82437777519226\n","89: 56.379667051136494 41.00078112632036 97.38044817745686\n","90: 54.649130292236805 39.766497667878866 94.41562796011567\n","91: 54.4852463118732 38.96778200380504 93.45302831567824\n","92: 56.21415916457772 39.90684954635799 96.12100871093571\n","93: 55.68752979114652 39.972425639629364 95.65995543077588\n","94: 54.067913081496954 39.172070724889636 93.23998380638659\n","95: 55.98238146863878 39.988459035754204 95.97084050439298\n","96: 52.7295780275017 37.79347073100507 90.52304875850677\n","97: 57.97315177693963 41.03625662624836 99.00940840318799\n","98: 54.40143724158406 39.506392339244485 93.90782958082855\n","99: 52.55549980700016 37.5138861592859 90.06938596628606\n","100: 33.84047889802605 19.42120360583067 53.26168250385672\n","101: 24.92292677704245 12.140229109674692 37.06315588671714\n","102: 22.414507030975074 9.937802407657728 32.3523094386328\n","103: 19.577026127604768 7.979135956382379 27.556162083987147\n","104: 20.478245417121798 7.380533884977922 27.85877930209972\n","105: 18.9716132839676 6.626359011977911 25.59797229594551\n","106: 17.062574818963185 5.889821937540546 22.95239675650373\n","107: 16.26038467680337 5.227952888351865 21.488337565155234\n","108: 16.438816554145887 5.000503116520122 21.43931967066601\n","109: 15.981957503128797 4.542028503026813 20.52398600615561\n","110: 16.50826626457274 4.411339733982459 20.9196059985552\n","111: 14.783452530973591 4.169819369330071 18.953271900303662\n","112: 14.960343143553473 4.028831225237809 18.989174368791282\n","113: 16.089797850116156 3.691673394816462 19.781471244932618\n","114: 14.62288556736894 3.4448574519483373 18.067743019317277\n","115: 16.22960807243362 3.6273051085299812 19.856913180963602\n","116: 14.967618019785732 3.514062780421227 18.48168080020696\n","117: 13.712508549797349 3.184299166779965 16.896807716577314\n","118: 14.294282453716733 2.8719884965685196 17.166270950285252\n","119: 12.62874242698308 2.925949040974956 15.554691467958037\n","120: 13.508213397464715 2.492929907690268 16.001143305154983\n","121: 13.25494694430381 2.8498778660432436 16.104824810347054\n","122: 12.950866721221246 2.6670121887582354 15.617878909979481\n","123: 13.184568832628429 2.417819928989047 15.602388761617476\n","124: 11.952475397498347 2.5427037102053873 14.495179107703734\n","125: 13.993503833189607 2.433320349518908 16.426824182708515\n","126: 12.537773057003506 2.4446839829324745 14.98245703993598\n","127: 12.080941932217684 2.3122148649417795 14.393156797159463\n","128: 12.325367572833784 2.17429190070834 14.499659473542124\n","129: 11.687395595363341 2.0330778382485732 13.720473433611915\n","130: 11.660086499003228 2.092561502533499 13.752648001536727\n","131: 12.75452840141952 2.162707007781137 14.917235409200657\n","132: 12.31416031398112 1.9208980902039912 14.23505840418511\n","133: 10.701507860270794 1.953867333359085 12.65537519362988\n","134: 11.209318458946655 1.9415888630901463 13.150907322036801\n","135: 12.572060395497829 1.8526400308182929 14.424700426316122\n","136: 11.790556625754107 1.8466253293445334 13.63718195509864\n","137: 12.01574831752805 1.9379829600802623 13.953731277608313\n","138: 11.91471546847606 1.8314066436723806 13.746122112148441\n","139: 11.228490542795043 1.850198638101574 13.078689180896617\n","140: 11.263338851509616 1.6191520168504212 12.882490868360037\n","141: 10.725414782646112 1.6474972154246643 12.372911998070776\n","142: 11.702924797078595 1.7856377708376385 13.488562567916233\n","143: 11.62270244123647 1.8096175976388622 13.432320038875332\n","144: 11.773659343540203 1.750351625494659 13.524010969034862\n","145: 11.264510845765471 1.675358972104732 12.939869817870203\n","146: 10.235146612976678 1.7094331913685892 11.944579804345267\n","147: 10.298980689723976 1.599647116556298 11.898627806280274\n","148: 9.766290055820718 1.539630383515032 11.30592043933575\n","149: 10.459538670314942 1.6184149821056053 12.077953652420547\n","150: 10.603076062456239 1.5653284620930208 12.16840452454926\n","151: 11.72062247339636 1.5617538483638782 13.282376321760239\n","152: 11.415019214793574 1.665239622205263 13.080258836998837\n","153: 11.704833364463411 1.4531689535651822 13.158002318028593\n","154: 11.48600482364418 1.530138691989123 13.016143515633303\n","155: 11.242085053148912 1.6054040672606789 12.847489120409591\n","156: 11.08543389555416 1.5111973449529614 12.596631240507122\n","157: 11.052526365616359 1.4156790933484444 12.468205458964803\n","158: 9.705299559835112 1.3351789663138334 11.040478526148945\n","159: 11.32756266236538 1.5609057311958168 12.888468393561197\n","160: 11.360646192566492 1.468475326342741 12.829121518909233\n","161: 9.366818996873917 1.427018320246134 10.793837317120051\n","162: 12.11670707014855 1.4565801948338049 13.573287264982355\n","163: 9.487896693579387 1.5244234968558885 11.012320190435275\n","164: 10.27729460116825 1.4940342068148311 11.771328807983082\n","165: 10.55386932293186 1.3868681627500337 11.940737485681893\n","166: 10.378020749194548 1.4806299399060663 11.858650689100614\n","167: 9.358566386450548 1.4585735662840307 10.817139952734578\n","168: 9.812760643457295 1.3562939334078692 11.169054576865165\n","169: 10.058747268136358 1.2479091842251364 11.306656452361494\n","170: 9.281660619395552 1.2712097671173979 10.55287038651295\n","171: 10.159742319316138 1.3576463489443995 11.517388668260537\n","172: 10.43307038169587 1.435393649560865 11.868464031256735\n","173: 11.044091250863858 1.5033435021177866 12.547434752981644\n","174: 10.119784074369818 1.2103259285795502 11.330110002949368\n","175: 9.501541052537505 1.172767886033398 10.674308938570903\n","176: 9.897149598487886 1.410232242255006 11.307381840742892\n","177: 10.218013399949996 1.4063938447652617 11.624407244715258\n","178: 10.205533356813248 1.478548472179682 11.68408182899293\n","179: 9.112021007604199 1.319967031275155 10.431988038879354\n","180: 8.855002384807449 1.1904722662293352 10.045474651036784\n","181: 9.670187885814812 1.0729889473877847 10.743176833202597\n","182: 11.532829713949468 1.4922134341322817 13.02504314808175\n","183: 9.16012351505924 1.2348317200521706 10.39495523511141\n","184: 11.16389666107716 1.4049249157105805 12.568821576787741\n","185: 9.663235114567215 1.5375855561578646 11.20082067072508\n","186: 10.028597302560229 1.3543626394384773 11.382959941998706\n","187: 9.77295526501257 1.1778319758013822 10.950787240813952\n","188: 9.918895842070924 1.2950791259208927 11.213974967991817\n","189: 10.078266977070598 1.2865879084711196 11.364854885541718\n","190: 9.973326661493047 1.3131711925962009 11.286497854089248\n","191: 9.334071649878751 1.4239139243145473 10.757985574193299\n","192: 11.223944668308832 1.4296283507283079 12.65357301903714\n","193: 10.314569814014249 1.3144410567329032 11.629010870747152\n","194: 9.607428371818969 1.4011798954743426 11.008608267293312\n","195: 9.751119213819038 1.278071007647668 11.029190221466706\n","196: 9.63451058184728 1.2360056647303281 10.870516246577608\n","197: 10.267485044081695 1.3838891432242235 11.651374187305919\n","198: 10.123315741162514 1.3041471823235042 11.427462923486019\n","199: 9.974912279867567 1.3529166660737246 11.327828945941292\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"doR5kS81SW5F"},"source":["# Save model"],"id":"doR5kS81SW5F"},{"cell_type":"code","metadata":{"id":"frozen-damage","executionInfo":{"status":"ok","timestamp":1625174875184,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_cifar10')\n","# model = models.__dict__['ResNet18']()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_cifar10'))"],"id":"frozen-damage","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLlLnmpRSZAl"},"source":["# Test on Test Data"],"id":"BLlLnmpRSZAl"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aboriginal-lafayette","executionInfo":{"status":"ok","timestamp":1625174878484,"user_tz":-60,"elapsed":3306,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"cd699901-f9e1-4357-fb31-a789962d8676"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"aboriginal-lafayette","execution_count":10,"outputs":[{"output_type":"stream","text":["0.9443\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xHj0XxoMScbg"},"source":["# Test on Train Data"],"id":"xHj0XxoMScbg"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"therapeutic-orlando","executionInfo":{"status":"ok","timestamp":1625174899635,"user_tz":-60,"elapsed":20972,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"daaeab10-e9e5-4865-eff5-94970a572210"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.to('cuda')\n","        outputs = model(inputs)\n","        _, predicts = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"therapeutic-orlando","execution_count":11,"outputs":[{"output_type":"stream","text":["0.99946\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XCvQUWwMSetI","executionInfo":{"status":"ok","timestamp":1625174899636,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"XCvQUWwMSetI","execution_count":11,"outputs":[]}]}
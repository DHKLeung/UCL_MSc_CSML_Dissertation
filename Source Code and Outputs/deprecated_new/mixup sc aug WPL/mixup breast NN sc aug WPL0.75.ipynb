{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"mixup breast NN sc aug WPL0.75.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NyrKck5mLTzj"},"source":["# Import Libraries"],"id":"NyrKck5mLTzj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"proprietary-livestock","executionInfo":{"status":"ok","timestamp":1625148808918,"user_tz":-60,"elapsed":23277,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"c21575e5-50f7-4773-925a-87a4afef9034"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"id":"proprietary-livestock","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GFBzaRxYLmIz"},"source":["# Import outside code"],"id":"GFBzaRxYLmIz"},{"cell_type":"code","metadata":{"id":"mSeLSa5KLn_5","executionInfo":{"status":"ok","timestamp":1625148809946,"user_tz":-60,"elapsed":1031,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["import numpy as np\n","from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_circles, make_classification, make_regression\n","\n","\n","def train_val_test_split(data, labels, split=(0.6, 0.2, 0.2)):\n","    # Split data #\n","    num_data = data.shape[0]\n","    num_train_data = int(num_data * split[0])\n","    num_val_data = int(num_data * split[1])\n","    train_data = data[:num_train_data]\n","    train_labels = labels[:num_train_data]\n","    val_data = data[num_train_data:num_train_data + num_val_data]\n","    val_labels = labels[num_train_data:num_train_data + num_val_data]\n","    test_data = data[num_train_data + num_val_data:]\n","    test_labels = labels[num_train_data + num_val_data:]\n","    train_val_test = (train_data, train_labels, val_data, val_labels, test_data, test_labels)\n","    return train_val_test\n","\n","\n","def load_skl_data(data_name, need_num=None, split=(0.6, 0.2, 0.2)):\n","    # Load and unpack data from sklearn & randomise #\n","    if data_name == 'iris':\n","        skl_data = load_iris()\n","    elif data_name == 'wine':\n","        skl_data = load_wine()\n","    elif data_name == 'breast_cancer':\n","        skl_data = load_breast_cancer()\n","    num_data = skl_data['data'].shape[0]\n","    random_idx = np.random.permutation(num_data)\n","    data = skl_data['data'][random_idx]\n","    labels = skl_data['target'][random_idx]\n","\n","    # Require number of data #\n","    if need_num is not None:\n","        data = data[:need_num]\n","        labels = data[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_circular_data(need_num, noise=0.1, factor=0.5, split=(0.6, 0.2, 0.2)):\n","    # Load circular data #\n","    data, labels = make_circles(n_samples=need_num, noise=noise, factor=factor)\n","    labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_two_spirals(need_num, noise=0.5, split=(0.6, 0.2, 0.2)):\n","    # Create two spirals data #\n","    n = np.sqrt(np.random.rand(need_num, 1)) * 780 * (2 * np.pi) / 360\n","    d1x = -np.cos(n) * n + np.random.rand(need_num, 1) * noise\n","    d1y = np.sin(n) * n + np.random.rand(need_num, 1) * noise\n","    data_extended = np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y))))\n","    labels_extended = np.hstack((np.ones(need_num) * -1, np.ones(need_num)))\n","    idx = np.random.permutation(need_num * 2)\n","    data_extended = data_extended[idx]\n","    labels_extended = labels_extended[idx]\n","    data = data_extended[:need_num]\n","    labels = labels_extended[:need_num]\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_classification_dataset(need_num, need_features, need_classes=2, need_flip=0.01, class_sep=1.0, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for classification #\n","    n_informative = need_classes\n","    n_redundant = 0\n","    n_repeated = 0\n","    n_cluster_per_class = 2\n","    data, labels = make_classification(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_redundant=n_redundant, n_repeated=n_repeated, n_classes=need_classes, n_clusters_per_class=n_cluster_per_class, flip_y=need_flip, class_sep=class_sep, random_state=random_state)\n","\n","    # Change labels to +1/-1 if it is binary classification #\n","    if need_classes == 2:\n","        labels[labels == 0] = -1\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test\n","\n","\n","def load_random_regression_dataset(need_num, need_features, bias, noise=1, random_state=None, split=(0.6, 0.2, 0.2)):\n","    # Create data for regression #\n","    n_informative = need_features\n","    n_targets = 1\n","    data, labels = make_regression(n_samples=need_num, n_features=need_features, n_informative=n_informative, n_targets=n_targets, bias=bias, noise=noise, random_state=random_state)\n","\n","    # Split data #\n","    train_val_test = train_val_test_split(data, labels, split=split)\n","    return train_val_test"],"id":"mSeLSa5KLn_5","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"KU2jvzwwLn5W","executionInfo":{"status":"ok","timestamp":1625148810240,"user_tz":-60,"elapsed":295,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["'''ResNet in PyTorch.\n","\n","BasicBlock and Bottleneck module is from the original ResNet paper:\n","[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n","\n","PreActBlock and PreActBottleneck module is from the later paper:\n","[2] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n","    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n","'''\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.autograd import Variable\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBlock(nn.Module):\n","    '''Pre-activation version of the BasicBlock.'''\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBlock, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = conv3x3(in_planes, planes, stride)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = conv3x3(planes, planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class PreActBottleneck(nn.Module):\n","    '''Pre-activation version of the original Bottleneck module.'''\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(PreActBottleneck, self).__init__()\n","        self.bn1 = nn.BatchNorm2d(in_planes)\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(x))\n","        shortcut = self.shortcut(out)\n","        out = self.conv1(out)\n","        out = self.conv2(F.relu(self.bn2(out)))\n","        out = self.conv3(F.relu(self.bn3(out)))\n","        out += shortcut\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = conv3x3(3,64)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, lin=0, lout=5):\n","        out = x\n","        if lin < 1 and lout > -1:\n","            out = self.conv1(out)\n","            out = self.bn1(out)\n","            out = F.relu(out)\n","        if lin < 2 and lout > 0:\n","            out = self.layer1(out)\n","        if lin < 3 and lout > 1:\n","            out = self.layer2(out)\n","        if lin < 4 and lout > 2:\n","            out = self.layer3(out)\n","        if lin < 5 and lout > 3:\n","            out = self.layer4(out)\n","        if lout > 4:\n","            out = F.avg_pool2d(out, 4)\n","            out = out.view(out.size(0), -1)\n","            out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(PreActBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(Variable(torch.randn(1,3,32,32)))\n","    print(y.size())\n","\n","# test()\n"],"id":"KU2jvzwwLn5W","execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o5PtGmQoLrl7"},"source":["# Configuration"],"id":"o5PtGmQoLrl7"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"silent-johns","executionInfo":{"status":"ok","timestamp":1625148810439,"user_tz":-60,"elapsed":200,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"a77db6e2-b8fd-4d9a-fcf2-2d95776d0542"},"source":["\"\"\"\n","Configuration and Hyperparameters\n","\"\"\"\n","#torch.set_default_tensor_type(torch.cuda.FloatTensor)  # default all in GPU, in pytorch 1.9 even need dataloader to be in GPU\n","\n","batch_size = 128\n","step_size = 0.005\n","random_seed = 0\n","epochs = 300\n","L2_decay = 1e-4\n","alpha = 1.\n","perturb_loss_weight = 0.75\n","\n","torch.manual_seed(random_seed)"],"id":"silent-johns","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7fd78f887890>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"NRldvRO1L28r"},"source":["# Data"],"id":"NRldvRO1L28r"},{"cell_type":"code","metadata":{"id":"compressed-schedule","executionInfo":{"status":"ok","timestamp":1625148810440,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["train_data, train_labels, val_data, val_labels, test_data, test_labels = load_skl_data('breast_cancer')\n","test_data = np.vstack((val_data, test_data))\n","test_labels = np.hstack((val_labels, test_labels))\n","train_data = torch.from_numpy(train_data).type(torch.FloatTensor)\n","train_labels = torch.from_numpy(train_labels)\n","test_data = torch.from_numpy(test_data).type(torch.FloatTensor)\n","test_labels = torch.from_numpy(test_labels)\n","train_mean = torch.mean(train_data, 0)\n","train_std = torch.std(train_data, 0)\n","train_data = (train_data - train_mean) / train_std\n","test_data = (test_data - train_mean) / train_std\n","train_set = torch.utils.data.TensorDataset(train_data, train_labels)\n","test_set = torch.utils.data.TensorDataset(test_data, test_labels)\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)"],"id":"compressed-schedule","execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6loof5Y_L6ry"},"source":["# Model, Loss, Optimiser"],"id":"6loof5Y_L6ry"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buried-science","executionInfo":{"status":"ok","timestamp":1625148817195,"user_tz":-60,"elapsed":6758,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"f26b02aa-cdd3-4667-9c3f-901a985804eb"},"source":["class fc_model(nn.Module):\n","    def __init__(self):\n","        super(fc_model, self).__init__()\n","        self.fc1 = nn.Linear(30, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 32)\n","        self.fc4 = nn.Linear(32, 1)\n","    def forward(self, inputs):\n","        fc1_out = F.tanh(self.fc1(inputs))\n","        fc2_out = F.tanh(self.fc2(fc1_out))\n","        fc3_out = F.tanh(self.fc3(fc2_out))\n","        fc4_out = self.fc4(fc3_out)\n","        return fc4_out\n","\n","model = fc_model()\n","criterion = torch.nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=step_size, momentum=0.9, weight_decay=L2_decay)\n","step_size_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[int(epochs / 2)], gamma=0.1)\n","model.cuda()"],"id":"buried-science","execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["fc_model(\n","  (fc1): Linear(in_features=30, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=32, bias=True)\n","  (fc4): Linear(in_features=32, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kaMBwZHaMIRn"},"source":["# Data Augmentation / Perturbation AND corresponding loss"],"id":"kaMBwZHaMIRn"},{"cell_type":"code","metadata":{"id":"fkMC5_yoZnWd","executionInfo":{"status":"ok","timestamp":1625148817196,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["def mixup_breast_sc(inputs, labels, alpha):\n","    lmbda = torch.distributions.beta.Beta(alpha, alpha).sample().to('cuda')\n","    mixup_inputs_uc_list = list()\n","    labels_uc_list = list()\n","    unique_classes = torch.unique(labels)\n","    for uc in unique_classes:\n","        mask_uc = (labels == uc).flatten()  # flatten to avoid the labels are in column vector\n","        inputs_uc = inputs[mask_uc]\n","        labels_uc = labels[mask_uc]\n","        batch_size_uc = labels_uc.size(0)\n","        idx = torch.randperm(batch_size_uc).to('cuda')\n","        mixup_inputs_uc = lmbda * inputs_uc + (1 - lmbda) * inputs_uc[idx]\n","        mixup_inputs_uc_list.append(mixup_inputs_uc)\n","        labels_uc_list.append(labels_uc)\n","    mixup_inputs_sc = torch.vstack(mixup_inputs_uc_list)\n","    mixup_labels_sc = torch.cat(labels_uc_list, dim=0)  # use cat not hstack to avoid labels are in column vector\n","    return mixup_inputs_sc, mixup_labels_sc, lmbda"],"id":"fkMC5_yoZnWd","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQOqDOmDMYOj"},"source":["# Training"],"id":"SQOqDOmDMYOj"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pediatric-namibia","executionInfo":{"status":"ok","timestamp":1625148821669,"user_tz":-60,"elapsed":4476,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"eb75da6d-974d-423b-a474-d4fb000cf098"},"source":["\"\"\"\n","Training\n","\"\"\"\n","model.train()\n","for epoch in range(epochs):\n","    epoch_mixup_loss = 0.\n","    epoch_loss = 0.\n","    epoch_augment_loss = 0.\n","    for i, data in enumerate(train_loader, 0):\n","        optimizer.zero_grad()\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","\n","        # Mixup perturbation with same class #\n","        mixup_inputs_sc, mixup_labels_sc, lmbda = mixup_breast_sc(inputs, labels, alpha)\n","\n","        # Concatenate perturbation and original data, to do augmentation and loss computation #\n","        original_num = inputs.size(0)\n","        augment_inputs = torch.vstack((inputs, mixup_inputs_sc))\n","        augment_outputs = model(augment_inputs)\n","        outputs = augment_outputs[:original_num]\n","        mixup_outputs_sc = augment_outputs[original_num:]\n","        mixup_loss_sc = criterion(mixup_outputs_sc, mixup_labels_sc)  # no need to mix the loss up since it is now mixup with same class, just use ordinary cross entropy\n","        loss = criterion(outputs, labels)\n","        weighted_augment_loss = perturb_loss_weight * mixup_loss_sc + (1 - perturb_loss_weight) * loss\n","\n","        # Record #\n","        epoch_mixup_loss += mixup_loss_sc.item()\n","        epoch_loss += loss.item()\n","        epoch_augment_loss += (mixup_loss_sc.item() + loss.item())\n","\n","        # Gradient Calculation & Optimisation #\n","        weighted_augment_loss.backward()\n","        optimizer.step()\n","    \n","    # Step size scheduler #\n","    step_size_scheduler.step()\n","    \n","    # Print loss #\n","    print('{}: {} {} {}'.format(epoch, epoch_mixup_loss, epoch_loss, epoch_augment_loss))"],"id":"pediatric-namibia","execution_count":8,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0: 2.142886996269226 2.145088791847229 4.287975788116455\n","1: 2.1039353609085083 2.1066582798957825 4.210593640804291\n","2: 2.0430280566215515 2.0473565459251404 4.090384602546692\n","3: 1.9571951031684875 1.9691308736801147 3.9263259768486023\n","4: 1.8621655702590942 1.877387523651123 3.7395530939102173\n","5: 1.7660771012306213 1.7811339497566223 3.5472110509872437\n","6: 1.6590298414230347 1.6825388073921204 3.341568648815155\n","7: 1.5301939845085144 1.5722010135650635 3.102394998073578\n","8: 1.4352792799472809 1.457691490650177 2.892970770597458\n","9: 1.3022285401821136 1.335344284772873 2.6375728249549866\n","10: 1.1333229541778564 1.2197620570659637 2.35308501124382\n","11: 1.0509473979473114 1.1221634149551392 2.1731108129024506\n","12: 0.9134229719638824 0.9877965748310089 1.9012195467948914\n","13: 0.8419913947582245 0.9090761542320251 1.7510675489902496\n","14: 0.7338142096996307 0.8116479814052582 1.545462191104889\n","15: 0.626844584941864 0.7427390515804291 1.369583636522293\n","16: 0.53156478703022 0.6693125516176224 1.2008773386478424\n","17: 0.5018057227134705 0.6115425825119019 1.1133483052253723\n","18: 0.5153099447488785 0.5666453987360001 1.0819553434848785\n","19: 0.47099150717258453 0.5308511257171631 1.0018426328897476\n","20: 0.405668243765831 0.5166841298341751 0.9223523736000061\n","21: 0.39907023310661316 0.4695068448781967 0.8685770779848099\n","22: 0.2651689052581787 0.4564076215028763 0.721576526761055\n","23: 0.30299708992242813 0.4429602548480034 0.7459573447704315\n","24: 0.26748280227184296 0.41524022817611694 0.6827230304479599\n","25: 0.26020749285817146 0.4061874449253082 0.6663949377834797\n","26: 0.25864633172750473 0.3668708950281143 0.625517226755619\n","27: 0.18030206859111786 0.36450105905532837 0.5448031276464462\n","28: 0.34571340680122375 0.4075373262166977 0.7532507330179214\n","29: 0.26572246104478836 0.34528930485248566 0.611011765897274\n","30: 0.1964902188628912 0.3313523083925247 0.5278425272554159\n","31: 0.20880598574876785 0.3308185786008835 0.5396245643496513\n","32: 0.11034305393695831 0.34282998740673065 0.45317304134368896\n","33: 0.23758304119110107 0.3250560387969017 0.5626390799880028\n","34: 0.15304849296808243 0.30193938314914703 0.45498787611722946\n","35: 0.1622498743236065 0.3045157641172409 0.4667656384408474\n","36: 0.23169372230768204 0.31760162115097046 0.5492953434586525\n","37: 0.2329235076904297 0.306673526763916 0.5395970344543457\n","38: 0.21416185051202774 0.29659993946552277 0.5107617899775505\n","39: 0.15073608979582787 0.2728998735547066 0.42363596335053444\n","40: 0.19523727148771286 0.269219733774662 0.4644570052623749\n","41: 0.16124545969069004 0.2847610414028168 0.4460065010935068\n","42: 0.11157734505832195 0.2815930172801018 0.39317036233842373\n","43: 0.18243534490466118 0.28904397040605545 0.47147931531071663\n","44: 0.1491193138062954 0.29116493463516235 0.44028424844145775\n","45: 0.17915348708629608 0.25666989386081696 0.43582338094711304\n","46: 0.12447373569011688 0.2708735540509224 0.3953472897410393\n","47: 0.15957977809011936 0.27420034259557724 0.4337801206856966\n","48: 0.15915251895785332 0.25916463136672974 0.41831715032458305\n","49: 0.12779435515403748 0.2527128867805004 0.3805072419345379\n","50: 0.09509154036641121 0.2569481432437897 0.3520396836102009\n","51: 0.10661640949547291 0.27214956283569336 0.37876597233116627\n","52: 0.17622922733426094 0.2575850337743759 0.43381426110863686\n","53: 0.14244301710277796 0.2581637427210808 0.40060675982385874\n","54: 0.16588128544390202 0.2841005250811577 0.4499818105250597\n","55: 0.17879783362150192 0.25444917753338814 0.43324701115489006\n","56: 0.16886712238192558 0.24972319602966309 0.41859031841158867\n","57: 0.15101608261466026 0.2699541300535202 0.42097021266818047\n","58: 0.13973824493587017 0.2664960138499737 0.40623425878584385\n","59: 0.13058225437998772 0.25181183218955994 0.38239408656954765\n","60: 0.13112431205809116 0.25428108870983124 0.3854054007679224\n","61: 0.06767140701413155 0.2513371706008911 0.31900857761502266\n","62: 0.08684295043349266 0.24971459060907364 0.3365575410425663\n","63: 0.11908964812755585 0.2391659915447235 0.35825563967227936\n","64: 0.13794166687875986 0.2304128147661686 0.36835448164492846\n","65: 0.12065493687987328 0.23408836498856544 0.3547433018684387\n","66: 0.11720102839171886 0.25032661855220795 0.3675276469439268\n","67: 0.15060617960989475 0.22961046546697617 0.3802166450768709\n","68: 0.09236461576074362 0.2421693429350853 0.3345339586958289\n","69: 0.09002290293574333 0.26574769243597984 0.3557705953717232\n","70: 0.12441453337669373 0.24931037425994873 0.37372490763664246\n","71: 0.0939056221395731 0.22905805334448814 0.32296367548406124\n","72: 0.09564643539488316 0.22950764745473862 0.3251540828496218\n","73: 0.04740971885621548 0.22751658782362938 0.27492630667984486\n","74: 0.05958306044340134 0.23691485822200775 0.2964979186654091\n","75: 0.07056050561368465 0.25033950060606003 0.3209000062197447\n","76: 0.09676514379680157 0.24365581944584846 0.34042096324265003\n","77: 0.09458433464169502 0.22761435061693192 0.32219868525862694\n","78: 0.18991727009415627 0.23490943759679794 0.4248267076909542\n","79: 0.08993507921695709 0.22394833713769913 0.3138834163546562\n","80: 0.13648121990263462 0.2450137361884117 0.38149495609104633\n","81: 0.049482339061796665 0.21988069266080856 0.26936303172260523\n","82: 0.05872211325913668 0.2195889800786972 0.2783110933378339\n","83: 0.07980521954596043 0.23180574551224709 0.3116109650582075\n","84: 0.10571495536714792 0.2265770137310028 0.33229196909815073\n","85: 0.08866723068058491 0.22122693806886673 0.30989416874945164\n","86: 0.07404576428234577 0.22337041795253754 0.2974161822348833\n","87: 0.11605673283338547 0.2377283275127411 0.35378506034612656\n","88: 0.11768195405602455 0.2234756164252758 0.34115757048130035\n","89: 0.10710760578513145 0.2152145765721798 0.32232218235731125\n","90: 0.0681636817753315 0.22709715738892555 0.29526083916425705\n","91: 0.12651832401752472 0.2729761637747288 0.3994944877922535\n","92: 0.10905920993536711 0.22481396421790123 0.33387317415326834\n","93: 0.14701683819293976 0.21377983689308167 0.3607966750860214\n","94: 0.0878252424299717 0.2298930659890175 0.3177183084189892\n","95: 0.12731114774942398 0.21350282803177834 0.3408139757812023\n","96: 0.1333885658532381 0.20717474818229675 0.34056331403553486\n","97: 0.07203601161018014 0.23136872798204422 0.30340473959222436\n","98: 0.04470688384026289 0.20735848136246204 0.25206536520272493\n","99: 0.10988674219697714 0.2322833277285099 0.34217006992548704\n","100: 0.09308191295713186 0.2058608327060938 0.29894274566322565\n","101: 0.08057535625994205 0.2272966280579567 0.30787198431789875\n","102: 0.04098341288045049 0.21730231121182442 0.2582857240922749\n","103: 0.07466960232704878 0.2281147614121437 0.3027843637391925\n","104: 0.11912860162556171 0.2129996307194233 0.332128232344985\n","105: 0.08160438761115074 0.21596914157271385 0.2975735291838646\n","106: 0.07750487420707941 0.23776063323020935 0.31526550743728876\n","107: 0.06147263012826443 0.2263253852725029 0.2877980154007673\n","108: 0.12042701244354248 0.21938041970133781 0.3398074321448803\n","109: 0.06191733479499817 0.21776337176561356 0.2796807065606117\n","110: 0.06439072778448462 0.230439230799675 0.2948299585841596\n","111: 0.0994983147829771 0.2270416095852852 0.3265399243682623\n","112: 0.06894790660589933 0.21071475371718407 0.2796626603230834\n","113: 0.10237867338582873 0.19952131528407335 0.3018999886699021\n","114: 0.10788388550281525 0.21107697486877441 0.31896086037158966\n","115: 0.13847156893461943 0.23411457240581512 0.37258614134043455\n","116: 0.03559694718569517 0.220678873360157 0.2562758205458522\n","117: 0.1697542406618595 0.24321066960692406 0.41296491026878357\n","118: 0.0705593079328537 0.20428921841084957 0.27484852634370327\n","119: 0.13648325810208917 0.226316150277853 0.3627994083799422\n","120: 0.17265342734754086 0.23677769675850868 0.40943112410604954\n","121: 0.1074465848505497 0.21760542690753937 0.32505201175808907\n","122: 0.1195746585726738 0.22480639070272446 0.34438104927539825\n","123: 0.059343219734728336 0.20884548872709274 0.2681887084618211\n","124: 0.11376478057354689 0.2030538935214281 0.316818674094975\n","125: 0.09820864163339138 0.20882363244891167 0.30703227408230305\n","126: 0.17090602964162827 0.23695530369877815 0.4078613333404064\n","127: 0.12926540337502956 0.19691129587590694 0.3261766992509365\n","128: 0.07329911086708307 0.2146294116973877 0.28792852256447077\n","129: 0.12178129982203245 0.20588820427656174 0.3276695040985942\n","130: 0.10518508777022362 0.21659723669290543 0.32178232446312904\n","131: 0.09640626981854439 0.22575633972883224 0.32216260954737663\n","132: 0.14092391729354858 0.21622222661972046 0.35714614391326904\n","133: 0.10479328222572803 0.2209773063659668 0.32577058859169483\n","134: 0.05478664580732584 0.212157741189003 0.26694438699632883\n","135: 0.07725599408149719 0.20039938762784004 0.27765538170933723\n","136: 0.09234156273305416 0.20437155291438103 0.2967131156474352\n","137: 0.1644729133695364 0.22326822206377983 0.38774113543331623\n","138: 0.06718062423169613 0.2253311313688755 0.29251175560057163\n","139: 0.09897642210125923 0.22283853963017464 0.32181496173143387\n","140: 0.09223002381622791 0.22471633553504944 0.31694635935127735\n","141: 0.0698070339858532 0.20015915483236313 0.2699661888182163\n","142: 0.09049293771386147 0.19850999303162098 0.28900293074548244\n","143: 0.03421507216989994 0.21983712539076805 0.254052197560668\n","144: 0.03345604566857219 0.21581949293613434 0.24927553860470653\n","145: 0.08888812735676765 0.2023630142211914 0.29125114157795906\n","146: 0.054352061823010445 0.20532247051596642 0.25967453233897686\n","147: 0.10413372237235308 0.22592277079820633 0.3300564931705594\n","148: 0.01899470668286085 0.20544440299272537 0.22443910967558622\n","149: 0.03562581865116954 0.19314363598823547 0.228769454639405\n","150: 0.13317546993494034 0.21960294991731644 0.3527784198522568\n","151: 0.09099559672176838 0.21514872461557388 0.30614432133734226\n","152: 0.11962058208882809 0.2147793099284172 0.3343998920172453\n","153: 0.1387297553010285 0.19843753427267075 0.33716728957369924\n","154: 0.04281806945800781 0.2070686984807253 0.2498867679387331\n","155: 0.0404265820980072 0.19586758129298687 0.23629416339099407\n","156: 0.15521676000207663 0.2344856560230255 0.38970241602510214\n","157: 0.1379078347235918 0.2081155888736248 0.3460234235972166\n","158: 0.12506289593875408 0.203875120729208 0.3289380166679621\n","159: 0.0850729108788073 0.23582419753074646 0.32089710840955377\n","160: 0.03613090701401234 0.20215336605906487 0.2382842730730772\n","161: 0.12181773409247398 0.2149863764643669 0.3368041105568409\n","162: 0.10874697053804994 0.207452192902565 0.31619916344061494\n","163: 0.08809275273233652 0.21364127844572067 0.3017340311780572\n","164: 0.08436720259487629 0.2161334492266178 0.3005006518214941\n","165: 0.07536953501403332 0.2096911445260048 0.2850606795400381\n","166: 0.0855970368720591 0.2074009422212839 0.292997979093343\n","167: 0.056972029618918896 0.19770969450473785 0.25468172412365675\n","168: 0.09083186788484454 0.23246335238218307 0.3232952202670276\n","169: 0.056634748354554176 0.21217156946659088 0.26880631782114506\n","170: 0.08125897869467735 0.2020181491971016 0.28327712789177895\n","171: 0.1178810321725905 0.19217361509799957 0.31005464727059007\n","172: 0.15985041297972202 0.19931594282388687 0.3591663558036089\n","173: 0.12016442604362965 0.2107161544263363 0.33088058046996593\n","174: 0.10460339859127998 0.22325216978788376 0.32785556837916374\n","175: 0.08108959998935461 0.2295117937028408 0.3106013936921954\n","176: 0.10074324999004602 0.22058451548218727 0.3213277654722333\n","177: 0.06656191404908895 0.20600662380456924 0.2725685378536582\n","178: 0.05660799704492092 0.22256630659103394 0.27917430363595486\n","179: 0.05782239558175206 0.2080908976495266 0.26591329323127866\n","180: 0.029413148295134306 0.19473574310541153 0.22414889140054584\n","181: 0.07483646646142006 0.19422335736453533 0.2690598238259554\n","182: 0.14075002819299698 0.20631378144025803 0.347063809633255\n","183: 0.1098790392279625 0.20453183725476265 0.31441087648272514\n","184: 0.09071945026516914 0.21064846217632294 0.3013679124414921\n","185: 0.09261988243088126 0.2035585530102253 0.29617843544110656\n","186: 0.04575801361352205 0.21034790948033333 0.2561059230938554\n","187: 0.05178013350814581 0.24482513777911663 0.29660527128726244\n","188: 0.062338341027498245 0.20748984068632126 0.2698281817138195\n","189: 0.12849052250385284 0.20289169624447823 0.33138221874833107\n","190: 0.06884373538196087 0.20182355493307114 0.270667290315032\n","191: 0.1169280419126153 0.2068830095231533 0.3238110514357686\n","192: 0.10114582628011703 0.19311538711190224 0.29426121339201927\n","193: 0.030968913808465004 0.2206602543592453 0.2516291681677103\n","194: 0.09522256627678871 0.19073001109063625 0.28595257736742496\n","195: 0.12162639573216438 0.22243529185652733 0.3440616875886917\n","196: 0.06196891888976097 0.21034545451402664 0.2723143734037876\n","197: 0.09350416576489806 0.20850349217653275 0.3020076579414308\n","198: 0.08872790262103081 0.20548910275101662 0.2942170053720474\n","199: 0.07557656615972519 0.20142189413309097 0.27699846029281616\n","200: 0.10749223362654448 0.20855094492435455 0.31604317855089903\n","201: 0.0919675761833787 0.20343567803502083 0.2954032542183995\n","202: 0.13387734815478325 0.23820925876498222 0.3720866069197655\n","203: 0.06839756481349468 0.2096150889992714 0.2780126538127661\n","204: 0.03183809295296669 0.22076549381017685 0.25260358676314354\n","205: 0.049599426332861185 0.193468214944005 0.2430676412768662\n","206: 0.07169356197118759 0.2154124267399311 0.2871059887111187\n","207: 0.12259428249672055 0.2212388813495636 0.34383316384628415\n","208: 0.12602009624242783 0.2163906041532755 0.3424107003957033\n","209: 0.07788476906716824 0.22655609995126724 0.3044408690184355\n","210: 0.05604124767705798 0.2286912500858307 0.28473249776288867\n","211: 0.044898711144924164 0.21402709186077118 0.25892580300569534\n","212: 0.1286747120320797 0.21674514934420586 0.34541986137628555\n","213: 0.06700430251657963 0.19625558331608772 0.26325988583266735\n","214: 0.09937851130962372 0.19391032308340073 0.29328883439302444\n","215: 0.12703296914696693 0.20260803028941154 0.3296409994363785\n","216: 0.07863800320774317 0.23221291229128838 0.31085091549903154\n","217: 0.06391984224319458 0.20775726810097694 0.2716771103441715\n","218: 0.03908120468258858 0.19024082832038403 0.2293220330029726\n","219: 0.0837156418710947 0.20990361645817757 0.29361925832927227\n","220: 0.06711777113378048 0.20012785866856575 0.26724562980234623\n","221: 0.10658292565494776 0.2325635738670826 0.33914649952203035\n","222: 0.10312983207404613 0.21891892701387405 0.3220487590879202\n","223: 0.06475228536874056 0.2280864268541336 0.29283871222287416\n","224: 0.09513034112751484 0.2310083620250225 0.32613870315253735\n","225: 0.13409286923706532 0.19754061847925186 0.3316334877163172\n","226: 0.06364860944449902 0.22029512003064156 0.28394372947514057\n","227: 0.04090238641947508 0.22462976351380348 0.26553214993327856\n","228: 0.08483062777668238 0.21455801278352737 0.29938864056020975\n","229: 0.04241127148270607 0.2150904666632414 0.25750173814594746\n","230: 0.16492810007184744 0.2271029595285654 0.39203105960041285\n","231: 0.06354764103889465 0.21542524546384811 0.27897288650274277\n","232: 0.03045230731368065 0.19382581114768982 0.22427811846137047\n","233: 0.05397443100810051 0.19839199259877205 0.25236642360687256\n","234: 0.09081830037757754 0.2210744507610798 0.31189275113865733\n","235: 0.08400474116206169 0.23835623636841774 0.32236097753047943\n","236: 0.06415014062076807 0.21053583920001984 0.2746859798207879\n","237: 0.08238475723192096 0.22232653200626373 0.3047112892381847\n","238: 0.05873136222362518 0.219244085252285 0.2779754474759102\n","239: 0.11710531637072563 0.20802904292941093 0.32513435930013657\n","240: 0.05233882600441575 0.1951629649847746 0.24750179098919034\n","241: 0.038800136651843786 0.21005863323807716 0.24885876988992095\n","242: 0.11583724804222584 0.2087247408926487 0.32456198893487453\n","243: 0.07077585253864527 0.20932184904813766 0.28009770158678293\n","244: 0.0830171499401331 0.2076478973031044 0.2906650472432375\n","245: 0.04956185445189476 0.19574628211557865 0.2453081365674734\n","246: 0.0961581151932478 0.2349465675652027 0.3311046827584505\n","247: 0.10045338096097112 0.21983724273741245 0.32029062369838357\n","248: 0.05961120314896107 0.20953025668859482 0.2691414598375559\n","249: 0.0485338750295341 0.1973208636045456 0.2458547386340797\n","250: 0.13315563462674618 0.22332658618688583 0.356482220813632\n","251: 0.05208971165120602 0.21370036527514458 0.2657900769263506\n","252: 0.07266815798357129 0.21559184975922108 0.28826000774279237\n","253: 0.13190685771405697 0.1978352703154087 0.3297421280294657\n","254: 0.11530361976474524 0.2298293672502041 0.3451329870149493\n","255: 0.04459942039102316 0.20611117966473103 0.2507106000557542\n","256: 0.060929483734071255 0.19750767573714256 0.2584371594712138\n","257: 0.08819339144974947 0.19913960993289948 0.28733300138264894\n","258: 0.09459090605378151 0.2036132514476776 0.2982041575014591\n","259: 0.041448150761425495 0.21211039647459984 0.25355854723602533\n","260: 0.10144799761474133 0.21248193830251694 0.31392993591725826\n","261: 0.04111444391310215 0.21785756573081017 0.2589720096439123\n","262: 0.04580249637365341 0.2159782275557518 0.2617807239294052\n","263: 0.08434236142784357 0.19590993970632553 0.2802523011341691\n","264: 0.14465527795255184 0.1968890205025673 0.34154429845511913\n","265: 0.12093465868383646 0.1908656358718872 0.31180029455572367\n","266: 0.09908140543848276 0.20462436974048615 0.3037057751789689\n","267: 0.01787506928667426 0.20098968967795372 0.21886475896462798\n","268: 0.1260976903140545 0.2181391939520836 0.3442368842661381\n","269: 0.16110332403331995 0.24161164835095406 0.402714972384274\n","270: 0.05518339900299907 0.1930941604077816 0.24827755941078067\n","271: 0.12916016578674316 0.20552623644471169 0.33468640223145485\n","272: 0.056568343192338943 0.20762865245342255 0.2641969956457615\n","273: 0.04197897110134363 0.19694282487034798 0.2389217959716916\n","274: 0.09743817243725061 0.199693001806736 0.2971311742439866\n","275: 0.06434185430407524 0.2346668429672718 0.29900869727134705\n","276: 0.07900695316493511 0.19918543845415115 0.27819239161908627\n","277: 0.17951827496290207 0.209979847073555 0.38949812203645706\n","278: 0.07956963963806629 0.21559125185012817 0.29516089148819447\n","279: 0.03461046935990453 0.24468152225017548 0.27929199161008\n","280: 0.1115465397015214 0.2072080746293068 0.3187546143308282\n","281: 0.07359622279182076 0.24918065778911114 0.3227768805809319\n","282: 0.06152955675497651 0.19322290644049644 0.25475246319547296\n","283: 0.08104139566421509 0.21213696151971817 0.29317835718393326\n","284: 0.11499391589313745 0.2046443447470665 0.31963826064020395\n","285: 0.04960331181064248 0.20448726043105125 0.25409057224169374\n","286: 0.03826839104294777 0.19613360799849033 0.2344019990414381\n","287: 0.1262533012777567 0.22903301566839218 0.3552863169461489\n","288: 0.12704821210354567 0.18860825430601835 0.315656466409564\n","289: 0.09220693306997418 0.19614479690790176 0.28835172997787595\n","290: 0.05912017077207565 0.2125719115138054 0.27169208228588104\n","291: 0.028048774227499962 0.21905406191945076 0.24710283614695072\n","292: 0.044348909985274076 0.22230904176831245 0.26665795175358653\n","293: 0.04702979000285268 0.22316882014274597 0.27019861014559865\n","294: 0.0867243530228734 0.19258618354797363 0.27931053657084703\n","295: 0.058456940576434135 0.21954410523176193 0.27800104580819607\n","296: 0.050371546763926744 0.1991560384631157 0.24952758522704244\n","297: 0.07973737642168999 0.20712590217590332 0.2868632785975933\n","298: 0.12228925991803408 0.19223480485379696 0.31452406477183104\n","299: 0.0877433056011796 0.21954158321022987 0.3072848888114095\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CL1h58tJNu56"},"source":["# Save model"],"id":"CL1h58tJNu56"},{"cell_type":"code","metadata":{"id":"brief-details","executionInfo":{"status":"ok","timestamp":1625148822081,"user_tz":-60,"elapsed":414,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":["# torch.save(model.state_dict(), './mixup_model_pytorch_breast')\n","# model = fc_model()\n","# model.load_state_dict(torch.load('./mixup_model_pytorch_breast'))"],"id":"brief-details","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DydYzZprNxjA"},"source":["# Test on Test Data"],"id":"DydYzZprNxjA"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"latin-interest","executionInfo":{"status":"ok","timestamp":1625148822082,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"34212504-41c7-40d5-9420-1abc9e9dcad7"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"latin-interest","execution_count":10,"outputs":[{"output_type":"stream","text":["0.9824561403508771\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YLVUq0LdNzfq"},"source":["# Test on Train Data"],"id":"YLVUq0LdNzfq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"resident-overall","executionInfo":{"status":"ok","timestamp":1625148822082,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}},"outputId":"9900a2f5-037b-4c0a-ebe1-9b2b45aeadb7"},"source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for data in train_loader:\n","        inputs, labels = data\n","        inputs = inputs.to('cuda')\n","        labels = labels.type(torch.FloatTensor).reshape(-1, 1).to('cuda')\n","        outputs = model(inputs)\n","        predicts = (torch.sign(outputs) + 1) / 2\n","        total += labels.size(0)\n","        correct += (predicts == labels).sum().item()\n","print(correct / total)"],"id":"resident-overall","execution_count":11,"outputs":[{"output_type":"stream","text":["0.9765395894428153\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"preceding-galaxy","executionInfo":{"status":"ok","timestamp":1625148822083,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniel H. Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXCFzebYz1uGv2ihM6hqeswMrHPmHtZOloQpE_0A=s64","userId":"09200240480627929504"}}},"source":[""],"id":"preceding-galaxy","execution_count":11,"outputs":[]}]}